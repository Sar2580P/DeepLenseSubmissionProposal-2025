/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.
  _future_warning(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.
  _future_warning(
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: sporwal1818 (shri_krishna) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in ./wandb/run-20250329_160721-o6zd5m3i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Task-4B_SuperRes_finetuned_super_res
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLense_FoundationModels_Analysis
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLense_FoundationModels_Analysis/runs/o6zd5m3i
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
76 4 76 4
Successfully loaded the encoder weights for Super Resolution
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                             â”ƒ Type                        â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model                            â”‚ SuperResolutionAE           â”‚  8.8 M â”‚
â”‚ 1  â”‚ model.encoder                    â”‚ ViT                         â”‚  8.0 M â”‚
â”‚ 2  â”‚ model.encoder.to_patch_embedding â”‚ Sequential                  â”‚  4.9 K â”‚
â”‚ 3  â”‚ model.encoder.transformer        â”‚ Transformer                 â”‚  7.9 M â”‚
â”‚ 4  â”‚ model.encoder.to_latent          â”‚ Identity                    â”‚      0 â”‚
â”‚ 5  â”‚ model.encoder.mlp_head           â”‚ Linear                      â”‚    771 â”‚
â”‚ 6  â”‚ model.encoder.dropout            â”‚ Dropout                     â”‚      0 â”‚
â”‚ 7  â”‚ model.patch_to_emb               â”‚ Sequential                  â”‚  4.9 K â”‚
â”‚ 8  â”‚ model.enc_to_dec                 â”‚ Linear                      â”‚ 32.9 K â”‚
â”‚ 9  â”‚ model.decoder                    â”‚ Transformer                 â”‚  692 K â”‚
â”‚ 10 â”‚ model.decoder.norm               â”‚ LayerNorm                   â”‚    256 â”‚
â”‚ 11 â”‚ model.decoder.layers             â”‚ ModuleList                  â”‚  692 K â”‚
â”‚ 12 â”‚ model.decoder_pos_emb            â”‚ Embedding                   â”‚ 46.3 K â”‚
â”‚ 13 â”‚ model.to_pixels                  â”‚ Linear                      â”‚  2.1 K â”‚
â”‚ 14 â”‚ model.upsample                   â”‚ Sequential                  â”‚  1.2 K â”‚
â”‚ 15 â”‚ model.upsample.0                 â”‚ Rearrange                   â”‚      0 â”‚
â”‚ 16 â”‚ model.upsample.1                 â”‚ Upsample                    â”‚      0 â”‚
â”‚ 17 â”‚ model.upsample.2                 â”‚ Conv2d                      â”‚    640 â”‚
â”‚ 18 â”‚ model.upsample.3                 â”‚ PReLU                       â”‚      1 â”‚
â”‚ 19 â”‚ model.upsample.4                 â”‚ Conv2d                      â”‚    577 â”‚
â”‚ 20 â”‚ model.upsample.5                 â”‚ Sigmoid                     â”‚      0 â”‚
â”‚ 21 â”‚ tr_ssim                          â”‚ StructuralSimilarityIndexMâ€¦ â”‚      0 â”‚
â”‚ 22 â”‚ val_ssim                         â”‚ StructuralSimilarityIndexMâ€¦ â”‚      0 â”‚
â”‚ 23 â”‚ tst_ssim                         â”‚ StructuralSimilarityIndexMâ€¦ â”‚      0 â”‚
â”‚ 24 â”‚ mse                              â”‚ MSELoss                     â”‚      0 â”‚
â”‚ 25 â”‚ tr_psnr                          â”‚ _PeakSignalNoiseRatio       â”‚      0 â”‚
â”‚ 26 â”‚ val_psnr                         â”‚ _PeakSignalNoiseRatio       â”‚      0 â”‚
â”‚ 27 â”‚ tst_psnr                         â”‚ _PeakSignalNoiseRatio       â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 8.8 M                                                         
Non-trainable params: 0                                                         
Total params: 8.8 M                                                             
Total estimated model params size (MB): 35                                      
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
76 4 76 4
Successfully loaded the encoder weights for Super Resolution
[rank: 1] Metric val_MSE_loss improved. New best score: 0.134
[rank: 0] Metric val_MSE_loss improved. New best score: 0.134
Epoch 0, global step 22: 'val_MSE_loss' reached 0.13418 (best 0.13418), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=0 | val_MSE_loss=0.134.ckpt' as top 1
[rank: 0] Metric val_MSE_loss improved by 0.099 >= min_delta = 5e-05. New best score: 0.036
[rank: 1] Metric val_MSE_loss improved by 0.099 >= min_delta = 5e-05. New best score: 0.036
Epoch 1, global step 44: 'val_MSE_loss' reached 0.03550 (best 0.03550), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=1 | val_MSE_loss=0.036.ckpt' as top 1
[rank: 0] Metric val_MSE_loss improved by 0.018 >= min_delta = 5e-05. New best score: 0.017
[rank: 1] Metric val_MSE_loss improved by 0.018 >= min_delta = 5e-05. New best score: 0.017
Epoch 2, global step 66: 'val_MSE_loss' reached 0.01737 (best 0.01737), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=2 | val_MSE_loss=0.017.ckpt' as top 1
[rank: 0] Metric val_MSE_loss improved by 0.002 >= min_delta = 5e-05. New best score: 0.015
[rank: 1] Metric val_MSE_loss improved by 0.002 >= min_delta = 5e-05. New best score: 0.015
Epoch 3, global step 88: 'val_MSE_loss' reached 0.01513 (best 0.01513), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=3 | val_MSE_loss=0.015.ckpt' as top 1
[rank: 0] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.015
[rank: 1] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.015
Epoch 4, global step 110: 'val_MSE_loss' reached 0.01468 (best 0.01468), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=4 | val_MSE_loss=0.015.ckpt' as top 1
[rank: 0] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.015
[rank: 1] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.015
Epoch 5, global step 132: 'val_MSE_loss' reached 0.01454 (best 0.01454), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=5 | val_MSE_loss=0.015.ckpt' as top 1
[rank: 0] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
[rank: 1] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
Epoch 6, global step 154: 'val_MSE_loss' reached 0.01447 (best 0.01447), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=6 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 7, global step 176: 'val_MSE_loss' reached 0.01444 (best 0.01444), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=7 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 8, global step 198: 'val_MSE_loss' reached 0.01443 (best 0.01443), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=8 | val_MSE_loss=0.014.ckpt' as top 1
[rank: 0] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
[rank: 1] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
Epoch 9, global step 220: 'val_MSE_loss' reached 0.01441 (best 0.01441), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=9 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 10, global step 242: 'val_MSE_loss' reached 0.01440 (best 0.01440), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=10 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 11, global step 264: 'val_MSE_loss' reached 0.01439 (best 0.01439), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=11 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 12, global step 286: 'val_MSE_loss' reached 0.01438 (best 0.01438), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=12 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 13, global step 308: 'val_MSE_loss' reached 0.01438 (best 0.01438), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=13 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 14, global step 330: 'val_MSE_loss' reached 0.01437 (best 0.01437), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=14 | val_MSE_loss=0.014.ckpt' as top 1
[rank: 0] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
[rank: 1] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
Epoch 15, global step 352: 'val_MSE_loss' reached 0.01436 (best 0.01436), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=15 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 16, global step 374: 'val_MSE_loss' reached 0.01435 (best 0.01435), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=16 | val_MSE_loss=0.014-v1.ckpt' as top 1
Epoch 17, global step 396: 'val_MSE_loss' reached 0.01434 (best 0.01434), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=17 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 18, global step 418: 'val_MSE_loss' reached 0.01433 (best 0.01433), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=18 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 19, global step 440: 'val_MSE_loss' reached 0.01432 (best 0.01432), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=19 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 20, global step 462: 'val_MSE_loss' reached 0.01431 (best 0.01431), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=20 | val_MSE_loss=0.014.ckpt' as top 1
[rank: 0] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
[rank: 1] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
Epoch 21, global step 484: 'val_MSE_loss' reached 0.01431 (best 0.01431), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=21 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 22, global step 506: 'val_MSE_loss' reached 0.01430 (best 0.01430), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=22 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 23, global step 528: 'val_MSE_loss' reached 0.01429 (best 0.01429), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=23 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 24, global step 550: 'val_MSE_loss' reached 0.01428 (best 0.01428), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=24 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 25, global step 572: 'val_MSE_loss' reached 0.01427 (best 0.01427), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=25 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 26, global step 594: 'val_MSE_loss' reached 0.01427 (best 0.01427), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=26 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 27, global step 616: 'val_MSE_loss' reached 0.01426 (best 0.01426), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=27 | val_MSE_loss=0.014.ckpt' as top 1
[rank: 0] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
[rank: 1] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
Epoch 28, global step 638: 'val_MSE_loss' reached 0.01425 (best 0.01425), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=28 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 29, global step 660: 'val_MSE_loss' reached 0.01424 (best 0.01424), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=29 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 30, global step 682: 'val_MSE_loss' reached 0.01423 (best 0.01423), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=30 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 31, global step 704: 'val_MSE_loss' reached 0.01423 (best 0.01423), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=31 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 32, global step 726: 'val_MSE_loss' reached 0.01422 (best 0.01422), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=32 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 33, global step 748: 'val_MSE_loss' reached 0.01421 (best 0.01421), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=33 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 34, global step 770: 'val_MSE_loss' reached 0.01421 (best 0.01421), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=34 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 35, global step 792: 'val_MSE_loss' reached 0.01420 (best 0.01420), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=35 | val_MSE_loss=0.014.ckpt' as top 1
[rank: 0] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
[rank: 1] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
Epoch 36, global step 814: 'val_MSE_loss' reached 0.01419 (best 0.01419), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=36 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 37, global step 836: 'val_MSE_loss' reached 0.01418 (best 0.01418), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=37 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 38, global step 858: 'val_MSE_loss' reached 0.01418 (best 0.01418), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=38 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 39, global step 880: 'val_MSE_loss' reached 0.01417 (best 0.01417), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=39 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 40, global step 902: 'val_MSE_loss' reached 0.01416 (best 0.01416), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=40 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 41, global step 924: 'val_MSE_loss' reached 0.01416 (best 0.01416), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=41 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 42, global step 946: 'val_MSE_loss' reached 0.01415 (best 0.01415), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=42 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 43, global step 968: 'val_MSE_loss' reached 0.01414 (best 0.01414), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=43 | val_MSE_loss=0.014.ckpt' as top 1
[rank: 0] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
[rank: 1] Metric val_MSE_loss improved by 0.000 >= min_delta = 5e-05. New best score: 0.014
Epoch 44, global step 990: 'val_MSE_loss' reached 0.01414 (best 0.01414), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=44 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 45, global step 1012: 'val_MSE_loss' reached 0.01413 (best 0.01413), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=45 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 46, global step 1034: 'val_MSE_loss' reached 0.01412 (best 0.01412), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=46 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 47, global step 1056: 'val_MSE_loss' reached 0.01412 (best 0.01412), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=47 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 48, global step 1078: 'val_MSE_loss' reached 0.01411 (best 0.01411), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=48 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 49, global step 1100: 'val_MSE_loss' reached 0.01411 (best 0.01411), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=49 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 50, global step 1122: 'val_MSE_loss' reached 0.01410 (best 0.01410), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=50 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 51, global step 1144: 'val_MSE_loss' reached 0.01409 (best 0.01409), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=51 | val_MSE_loss=0.014.ckpt' as top 1
[rank: 0] Monitored metric val_MSE_loss did not improve in the last 8 records. Best score: 0.014. Signaling Trainer to stop.
[rank: 1] Monitored metric val_MSE_loss did not improve in the last 8 records. Best score: 0.014. Signaling Trainer to stop.
Epoch 52, global step 1166: 'val_MSE_loss' reached 0.01409 (best 0.01409), saving model to 'results/MAE/Task-4B_SuperRes_finetuned_super_res/ckpts/epoch=52 | val_MSE_loss=0.014.ckpt' as top 1
Epoch 52/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 64/64 0:00:10 â€¢ 0:00:00 6.97it/s v_num: 3i_4      
                                                               val_MSE_loss:    
                                                               0.014 val_PSNR:  
                                                               18.514 val_SSIM: 
                                                               0.456            
                                                               train_MSE_loss:  
                                                               0.014 train_PSNR:
                                                               18.529           
                                                               train_SSIM: 0.449
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:232: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚       test_MSE_loss       â”‚   0.014052624814212322    â”‚
â”‚         test_PSNR         â”‚     18.52480125427246     â”‚
â”‚         test_SSIM         â”‚    0.4557136297225952     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0:00:00 â€¢ 0:00:00 35.98it/s 
[1;34mwandb[0m: 
[1;34mwandb[0m: ğŸš€ View run [33mTask-4B_SuperRes_finetuned_super_res[0m at: [34mhttps://wandb.ai/shri_krishna/DeepLense_FoundationModels_Analysis/runs/o6zd5m3i[0m
