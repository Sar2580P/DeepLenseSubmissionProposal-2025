Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: sporwal1818 (shri_krishna) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in ./wandb/run-20250401_003040-njhuwsg1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Task-4A_ViT_finetuned_classification
wandb: ‚≠êÔ∏è View project at https://wandb.ai/shri_krishna/DeepLense_FoundationModels_Analysis
wandb: üöÄ View run at https://wandb.ai/shri_krishna/DeepLense_FoundationModels_Analysis/runs/njhuwsg1
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory results/MAE/Task-4A_ViT_finetuned_classification/ckpts exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
76 4 76 4
Successfully loaded the model weights for classification
All model parameters have gradients enabled.
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ    ‚îÉ Name                       ‚îÉ Type                 ‚îÉ Params ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ 0  ‚îÇ model                      ‚îÇ ViT                  ‚îÇ  8.0 M ‚îÇ
‚îÇ 1  ‚îÇ model.to_patch_embedding   ‚îÇ Sequential           ‚îÇ  4.9 K ‚îÇ
‚îÇ 2  ‚îÇ model.to_patch_embedding.0 ‚îÇ Rearrange            ‚îÇ      0 ‚îÇ
‚îÇ 3  ‚îÇ model.to_patch_embedding.1 ‚îÇ LayerNorm            ‚îÇ     32 ‚îÇ
‚îÇ 4  ‚îÇ model.to_patch_embedding.2 ‚îÇ Linear               ‚îÇ  4.4 K ‚îÇ
‚îÇ 5  ‚îÇ model.to_patch_embedding.3 ‚îÇ LayerNorm            ‚îÇ    512 ‚îÇ
‚îÇ 6  ‚îÇ model.transformer          ‚îÇ Transformer          ‚îÇ  7.9 M ‚îÇ
‚îÇ 7  ‚îÇ model.transformer.norm     ‚îÇ LayerNorm            ‚îÇ    512 ‚îÇ
‚îÇ 8  ‚îÇ model.transformer.layers   ‚îÇ ModuleList           ‚îÇ  7.9 M ‚îÇ
‚îÇ 9  ‚îÇ model.to_latent            ‚îÇ Identity             ‚îÇ      0 ‚îÇ
‚îÇ 10 ‚îÇ model.mlp_head             ‚îÇ Linear               ‚îÇ    771 ‚îÇ
‚îÇ 11 ‚îÇ model.dropout              ‚îÇ Dropout              ‚îÇ      0 ‚îÇ
‚îÇ 12 ‚îÇ tr_kappa                   ‚îÇ MulticlassCohenKappa ‚îÇ      0 ‚îÇ
‚îÇ 13 ‚îÇ val_kappa                  ‚îÇ MulticlassCohenKappa ‚îÇ      0 ‚îÇ
‚îÇ 14 ‚îÇ tst_kappa                  ‚îÇ MulticlassCohenKappa ‚îÇ      0 ‚îÇ
‚îÇ 15 ‚îÇ tr_accuracy                ‚îÇ MulticlassAccuracy   ‚îÇ      0 ‚îÇ
‚îÇ 16 ‚îÇ val_accuracy               ‚îÇ MulticlassAccuracy   ‚îÇ      0 ‚îÇ
‚îÇ 17 ‚îÇ tst_accuracy               ‚îÇ MulticlassAccuracy   ‚îÇ      0 ‚îÇ
‚îÇ 18 ‚îÇ criterion                  ‚îÇ CrossEntropyLoss     ‚îÇ      0 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Trainable params: 8.0 M                                                         
Non-trainable params: 0                                                         
Total params: 8.0 M                                                             
Total estimated model params size (MB): 31                                      
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
76 4 76 4
Successfully loaded the model weights for classification
All model parameters have gradients enabled.
[rank: 0] Metric val_ce_loss improved. New best score: 1.099
[rank: 1] Metric val_ce_loss improved. New best score: 1.099
Epoch 0, global step 188: 'val_ce_loss' reached 1.09893 (best 1.09893), saving model to 'results/MAE/Task-4A_ViT_finetuned_classification/ckpts/epoch=0 | val_ce_loss=1.099.ckpt' as top 1
Epoch 1, global step 376: 'val_ce_loss' was not in top 1
Epoch 2, global step 564: 'val_ce_loss' was not in top 1
Epoch 3, global step 752: 'val_ce_loss' was not in top 1
Epoch 4, global step 940: 'val_ce_loss' was not in top 1
Epoch 5, global step 1128: 'val_ce_loss' was not in top 1
[rank: 0] Metric val_ce_loss improved by 0.000 >= min_delta = 5e-05. New best score: 1.099
[rank: 1] Metric val_ce_loss improved by 0.000 >= min_delta = 5e-05. New best score: 1.099
Epoch 6, global step 1316: 'val_ce_loss' reached 1.09879 (best 1.09879), saving model to 'results/MAE/Task-4A_ViT_finetuned_classification/ckpts/epoch=6 | val_ce_loss=1.099.ckpt' as top 1
[rank: 0] Metric val_ce_loss improved by 0.000 >= min_delta = 5e-05. New best score: 1.099
[rank: 1] Metric val_ce_loss improved by 0.000 >= min_delta = 5e-05. New best score: 1.099
Epoch 7, global step 1504: 'val_ce_loss' reached 1.09872 (best 1.09872), saving model to 'results/MAE/Task-4A_ViT_finetuned_classification/ckpts/epoch=7 | val_ce_loss=1.099-v1.ckpt' as top 1
Epoch 8, global step 1692: 'val_ce_loss' reached 1.09868 (best 1.09868), saving model to 'results/MAE/Task-4A_ViT_finetuned_classification/ckpts/epoch=8 | val_ce_loss=1.099-v1.ckpt' as top 1
Epoch 9, global step 1880: 'val_ce_loss' was not in top 1
Epoch 10, global step 2068: 'val_ce_loss' was not in top 1
Epoch 11, global step 2256: 'val_ce_loss' was not in top 1
Epoch 12, global step 2444: 'val_ce_loss' was not in top 1
[rank: 0] Metric val_ce_loss improved by 0.000 >= min_delta = 5e-05. New best score: 1.099
[rank: 1] Metric val_ce_loss improved by 0.000 >= min_delta = 5e-05. New best score: 1.099
Epoch 13, global step 2632: 'val_ce_loss' reached 1.09858 (best 1.09858), saving model to 'results/MAE/Task-4A_ViT_finetuned_classification/ckpts/epoch=13 | val_ce_loss=1.099.ckpt' as top 1
Epoch 14, global step 2820: 'val_ce_loss' reached 1.09854 (best 1.09854), saving model to 'results/MAE/Task-4A_ViT_finetuned_classification/ckpts/epoch=14 | val_ce_loss=1.099.ckpt' as top 1
Epoch 15, global step 3008: 'val_ce_loss' was not in top 1
Epoch 16, global step 3196: 'val_ce_loss' was not in top 1
Epoch 17, global step 3384: 'val_ce_loss' was not in top 1
Epoch 18, global step 3572: 'val_ce_loss' was not in top 1
Epoch 19, global step 3760: 'val_ce_loss' was not in top 1
Epoch 20, global step 3948: 'val_ce_loss' was not in top 1
Epoch 21, global step 4136: 'val_ce_loss' was not in top 1
Epoch 22, global step 4324: 'val_ce_loss' was not in top 1
[rank: 0] Metric val_ce_loss improved by 0.000 >= min_delta = 5e-05. New best score: 1.098
[rank: 1] Metric val_ce_loss improved by 0.000 >= min_delta = 5e-05. New best score: 1.098
Epoch 23, global step 4512: 'val_ce_loss' reached 1.09849 (best 1.09849), saving model to 'results/MAE/Task-4A_ViT_finetuned_classification/ckpts/epoch=23 | val_ce_loss=1.098.ckpt' as top 1
Epoch 24, global step 4700: 'val_ce_loss' was not in top 1
Epoch 25, global step 4888: 'val_ce_loss' was not in top 1
Epoch 26, global step 5076: 'val_ce_loss' was not in top 1
Epoch 27, global step 5264: 'val_ce_loss' was not in top 1
Epoch 28, global step 5452: 'val_ce_loss' was not in top 1
Epoch 29, global step 5640: 'val_ce_loss' was not in top 1
Epoch 30, global step 5828: 'val_ce_loss' was not in top 1
Epoch 31, global step 6016: 'val_ce_loss' was not in top 1
Epoch 32, global step 6204: 'val_ce_loss' was not in top 1
[rank: 0] Metric val_ce_loss improved by 0.000 >= min_delta = 5e-05. New best score: 1.098
[rank: 1] Metric val_ce_loss improved by 0.000 >= min_delta = 5e-05. New best score: 1.098
Epoch 33, global step 6392: 'val_ce_loss' reached 1.09843 (best 1.09843), saving model to 'results/MAE/Task-4A_ViT_finetuned_classification/ckpts/epoch=33 | val_ce_loss=1.098.ckpt' as top 1
Epoch 34, global step 6580: 'val_ce_loss' was not in top 1
Epoch 35, global step 6768: 'val_ce_loss' was not in top 1
Epoch 36, global step 6956: 'val_ce_loss' was not in top 1
Epoch 37, global step 7144: 'val_ce_loss' was not in top 1
Epoch 38, global step 7332: 'val_ce_loss' was not in top 1
Epoch 39, global step 7520: 'val_ce_loss' was not in top 1
Epoch 40, global step 7708: 'val_ce_loss' was not in top 1
Epoch 41, global step 7896: 'val_ce_loss' reached 1.09842 (best 1.09842), saving model to 'results/MAE/Task-4A_ViT_finetuned_classification/ckpts/epoch=41 | val_ce_loss=1.098.ckpt' as top 1
Epoch 42, global step 8084: 'val_ce_loss' was not in top 1
[rank: 0] Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.098. Signaling Trainer to stop.
[rank: 1] Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.098. Signaling Trainer to stop.
Epoch 43, global step 8272: 'val_ce_loss' was not in top 1
Epoch 43/199 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 564/564 0:01:46 ‚Ä¢       5.29it/s v_num: 1_13      
                                      0:00:00                  val_ce_loss:     
                                                               1.099 val_kappa: 
                                                               0.000 val_acc:   
                                                               0.336            
                                                               train_ce_loss:   
                                                               1.099            
                                                               train_kappa:     
                                                               0.009 train_acc: 
                                                               0.334            
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:232: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ        Test metric        ‚îÉ       DataLoader 0        ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ      test_acc_epoch       ‚îÇ    0.3355026841163635     ‚îÇ
‚îÇ    test_ce_loss_epoch     ‚îÇ    1.0987244844436646     ‚îÇ
‚îÇ     test_kappa_epoch      ‚îÇ            0.0            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Testing ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 70/70 0:00:07 ‚Ä¢ 0:00:00 9.50it/s 
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mTask-4A_ViT_finetuned_classification[0m at: [34mhttps://wandb.ai/shri_krishna/DeepLense_FoundationModels_Analysis/runs/njhuwsg1[0m
