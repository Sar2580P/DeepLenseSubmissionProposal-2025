Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: sporwal1818 (shri_krishna) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in ./wandb/run-20250331_002058-ishmwe4l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Task-4A_ViT_finetuned_classification
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLense_FoundationModels_Analysis
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLense_FoundationModels_Analysis/runs/ishmwe4l
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory results/MAE/Task-4A_ViT_finetuned_classification/ckpts exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
76 4 76 4
Successfully loaded the model weights for classification
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type                 â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model                      â”‚ ViT                  â”‚  8.0 M â”‚
â”‚ 1  â”‚ model.to_patch_embedding   â”‚ Sequential           â”‚  4.9 K â”‚
â”‚ 2  â”‚ model.to_patch_embedding.0 â”‚ Rearrange            â”‚      0 â”‚
â”‚ 3  â”‚ model.to_patch_embedding.1 â”‚ LayerNorm            â”‚     32 â”‚
â”‚ 4  â”‚ model.to_patch_embedding.2 â”‚ Linear               â”‚  4.4 K â”‚
â”‚ 5  â”‚ model.to_patch_embedding.3 â”‚ LayerNorm            â”‚    512 â”‚
â”‚ 6  â”‚ model.transformer          â”‚ Transformer          â”‚  7.9 M â”‚
â”‚ 7  â”‚ model.transformer.norm     â”‚ LayerNorm            â”‚    512 â”‚
â”‚ 8  â”‚ model.transformer.layers   â”‚ ModuleList           â”‚  7.9 M â”‚
â”‚ 9  â”‚ model.to_latent            â”‚ Identity             â”‚      0 â”‚
â”‚ 10 â”‚ model.mlp_head             â”‚ Linear               â”‚    771 â”‚
â”‚ 11 â”‚ model.dropout              â”‚ Dropout              â”‚      0 â”‚
â”‚ 12 â”‚ tr_kappa                   â”‚ MulticlassCohenKappa â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa                  â”‚ MulticlassCohenKappa â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa                  â”‚ MulticlassCohenKappa â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy                â”‚ MulticlassAccuracy   â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy               â”‚ MulticlassAccuracy   â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy               â”‚ MulticlassAccuracy   â”‚      0 â”‚
â”‚ 18 â”‚ criterion                  â”‚ CrossEntropyLoss     â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 8.0 M                                                         
Non-trainable params: 0                                                         
Total params: 8.0 M                                                             
Total estimated model params size (MB): 31                                      
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
76 4 76 4
Successfully loaded the model weights for classification
[rank: 1] Metric val_ce_loss improved. New best score: 1.098
[rank: 0] Metric val_ce_loss improved. New best score: 1.098
Epoch 0, global step 188: 'val_ce_loss' reached 1.09847 (best 1.09847), saving model to 'results/MAE/Task-4A_ViT_finetuned_classification/ckpts/epoch=0 | val_ce_loss=1.098.ckpt' as top 1
Epoch 1, global step 376: 'val_ce_loss' was not in top 1
Epoch 2, global step 564: 'val_ce_loss' was not in top 1
Epoch 3, global step 752: 'val_ce_loss' was not in top 1
Epoch 4, global step 940: 'val_ce_loss' was not in top 1
Epoch 5, global step 1128: 'val_ce_loss' was not in top 1
Epoch 6, global step 1316: 'val_ce_loss' was not in top 1
Epoch 7, global step 1504: 'val_ce_loss' was not in top 1
Epoch 8, global step 1692: 'val_ce_loss' was not in top 1
Epoch 9, global step 1880: 'val_ce_loss' was not in top 1
[rank: 0] Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.098. Signaling Trainer to stop.
[rank: 1] Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.098. Signaling Trainer to stop.
Epoch 10, global step 2068: 'val_ce_loss' was not in top 1
Epoch 10/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 564/564 0:01:51 â€¢       5.22it/s v_num: 4l_6      
                                      0:00:00                  val_ce_loss:     
                                                               1.099 val_kappa: 
                                                               0.000 val_acc:   
                                                               0.336            
                                                               train_ce_loss:   
                                                               1.101            
                                                               train_kappa:     
                                                               -0.004 train_acc:
                                                               0.331            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:232: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.3355026841163635     â”‚
â”‚    test_ce_loss_epoch     â”‚    1.0986640453338623     â”‚
â”‚     test_kappa_epoch      â”‚            0.0            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 70/70 0:00:08 â€¢ 0:00:00 7.84it/s 
[1;34mwandb[0m: 
[1;34mwandb[0m: ğŸš€ View run [33mTask-4A_ViT_finetuned_classification[0m at: [34mhttps://wandb.ai/shri_krishna/DeepLense_FoundationModels_Analysis/runs/ishmwe4l[0m
