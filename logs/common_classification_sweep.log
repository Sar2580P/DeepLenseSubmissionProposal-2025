wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: ae55otgx with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 0.00014331042727981684
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 4.8580109226538715e-05
wandb: Currently logged in as: sporwal1818 (shri_krishna) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_192520-ae55otgx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-1
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ae55otgx
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Create sweep with ID: 5e1jb5oe
Sweep URL: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.116
Metric val_ce_loss improved by 0.006 >= min_delta = 0.0001. New best score: 1.110
Metric val_ce_loss improved by 0.008 >= min_delta = 0.0001. New best score: 1.102
Metric val_ce_loss improved by 0.059 >= min_delta = 0.0001. New best score: 1.043
Metric val_ce_loss improved by 0.057 >= min_delta = 0.0001. New best score: 0.986
Metric val_ce_loss improved by 0.215 >= min_delta = 0.0001. New best score: 0.771
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 0.771. Signaling Trainer to stop.
Epoch 22/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:25 •       15.44it/s v_num: otgx      
                                     0:00:00                   val_ce_loss:     
                                                               5.688 val_kappa: 
                                                               0.079 val_acc:   
                                                               0.369            
                                                               train_ce_loss:   
                                                               0.172            
                                                               train_kappa:     
                                                               0.932 train_acc: 
                                                               0.935            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.36399999260902405    │
│    test_ce_loss_epoch     │     5.676748752593994     │
│     test_kappa_epoch      │    0.07352942228317261    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:04 • 0:00:00 10.28it/s 
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██
wandb: exponential_decay_lr_scheduler ████▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▅▅▅▇▇▃▂▄█▆▆▄▅▆▃▆▃▆▃▅▄▃▄▆█▅▅▅▄▄▄▄▅▅▃▇▁▄▅▆
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▅▄▃▂▂▅█▅▁▄▂▅▆▃▅▃▇▄▇▅▆█▅▂▂▄▄▄▆▅▅▄▃▃▆▁▆▄▃▅
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▁▅▄▆▄▅▃▄▅▅▅▄▇▄▆▆▄▆▆▅▄▅▅▄█▇▇▅▆▄▆▄▆▄▅▆▅▂▆▇
wandb:                      train_acc ▁▁▁▁▂▂▃▃▄▄▅▅▅▆▆▆▇▇▇▇███
wandb:                  train_ce_loss ██████▇▇▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                    train_kappa ▁▁▁▁▂▂▃▄▅▅▅▆▆▆▇▇▇▇▇████
wandb:            trainer/global_step ▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇██▁▁▁▁▁▁▁
wandb:                        val_acc ▁▁▁▁▁▁▃▃▄▁▁▆▇▁▇▆▄▁▇▁▄█▂
wandb:                    val_ce_loss ▁▁▁▂▂▃▁▂▁█▄▁▁▆▁▂▄▇▂▇▃▂█
wandb:                      val_kappa ▁▁▁▁▁▁▄▄▅▁▂▆█▁▇▇▄▂█▂▅█▂
wandb: 
wandb: Run summary:
wandb:                          epoch 23
wandb: exponential_decay_lr_scheduler 0.00011
wandb:                 test_acc_epoch 0.364
wandb:                  test_acc_step 0.41071
wandb:             test_ce_loss_epoch 5.67675
wandb:              test_ce_loss_step 5.98594
wandb:               test_kappa_epoch 0.07353
wandb:                test_kappa_step 0.14917
wandb:                      train_acc 0.93498
wandb:                  train_ce_loss 0.17161
wandb:                    train_kappa 0.93156
wandb:            trainer/global_step 4370
wandb:                        val_acc 0.36889
wandb:                    val_ce_loss 5.6882
wandb:                      val_kappa 0.07903
wandb: 
wandb: 🚀 View run cool-sweep-1 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ae55otgx
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_192520-ae55otgx/logs
wandb: Agent Starting Run: j3uo2ije with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.2747862192849845e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.9711157517657677e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_193714-j3uo2ije
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-2
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/j3uo2ije
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.100
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.100. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:24 •       16.05it/s v_num: 2ije      
                                     0:00:00                   val_ce_loss:     
                                                               2.121 val_kappa: 
                                                               0.140 val_acc:   
                                                               0.402            
                                                               train_ce_loss:   
                                                               0.048            
                                                               train_kappa:     
                                                               0.990 train_acc: 
                                                               0.991            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.38466668128967285    │
│    test_ce_loss_epoch     │    2.2249033451080322     │
│     test_kappa_epoch      │    0.12424951791763306    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:02 • 0:00:00 16.88it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ████▇▇▇▇▇▇▆▆▆▆▆▅▅▄▄▄▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▅▃▄▆▅▆▃▂▃▇▁▅▅▄▂▃▆▃▄▅▃▃▅▄▆▆▄█▆▅▆▂▅▅▅▅▄▆▇▅
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▃▇▅▃▃▄▆█▆▄▆▇▆▅▃▅▆▄▅▅▃▇▄▆▇▄▇▁▄▃▄▄▅▄▄▅▇▃▃▂
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▄▄▃▅▃▄▂▃▄█▁▃▄▁▅▃▅▃▆▅▂▁▅▆▇▆▃▆▆▆▇▄▄▅▄▄▆▅▅▇
wandb:                      train_acc ▁▂▂▃▃▄▅▇▇██
wandb:                  train_ce_loss ███▇▇▆▅▃▂▁▁
wandb:                    train_kappa ▁▂▂▃▄▅▆▇▇██
wandb:            trainer/global_step ▁▁▁▁▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▁▁▁▁▁▁█
wandb:                        val_acc ▁▁▄▃▃▂▅▇▄▆█
wandb:                    val_ce_loss ▁▁▁▁▂▄▃▅█▇▇
wandb:                      val_kappa ▁▂▅▄▃▃▆█▆▇█
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.38467
wandb:                  test_acc_step 0.41071
wandb:             test_ce_loss_epoch 2.2249
wandb:              test_ce_loss_step 1.82317
wandb:               test_kappa_epoch 0.12425
wandb:                test_kappa_step 0.31148
wandb:                      train_acc 0.9914
wandb:                  train_ce_loss 0.04843
wandb:                    train_kappa 0.99012
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.40185
wandb:                    val_ce_loss 2.12113
wandb:                      val_kappa 0.14032
wandb: 
wandb: 🚀 View run feasible-sweep-2 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/j3uo2ije
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_193714-j3uo2ije/logs
wandb: Agent Starting Run: 70sq9jhe with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 7.442877189973635e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.027616377755173e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_194243-70sq9jhe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-3
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/70sq9jhe
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.127
Metric val_ce_loss improved by 0.022 >= min_delta = 0.0001. New best score: 1.105
Metric val_ce_loss improved by 0.003 >= min_delta = 0.0001. New best score: 1.102
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.102. Signaling Trainer to stop.
Epoch 12/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:24 •       16.23it/s v_num: 9jhe      
                                     0:00:00                   val_ce_loss:     
                                                               2.390 val_kappa: 
                                                               0.166 val_acc:   
                                                               0.398            
                                                               train_ce_loss:   
                                                               0.042            
                                                               train_kappa:     
                                                               0.986 train_acc: 
                                                               0.989            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.41100001335144043    │
│    test_ce_loss_epoch     │     2.293755292892456     │
│     test_kappa_epoch      │    0.18101978302001953    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:02 • 0:00:00 17.10it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler █▇▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▄▅▄▁▅▅▆▄▄▄▅▄▅▄▅▂▅▅▅▅▃▄▄▄▄▅▄▅▄▅▄▅▃▄▆▆▇█▅▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▂▃▄▇▄▁▁▄▄▃▄▄▃▅▂█▄▂▃▄▃▃▅▄▃▃▄▁▄▆▃▃▃▃▃▃▁▁▂▃
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▃▄▃▁▄▇▆▅▆▅▄▄▃▄▅▁▅▆▅▅▂▄▂▄▅▅▅▅▂▆▂█▂▃▅▆█▇▃▄
wandb:                      train_acc ▁▂▂▂▃▄▅▆▇▇███
wandb:                  train_ce_loss ████▇▇▆▅▃▂▁▁▁
wandb:                    train_kappa ▁▂▂▃▃▄▅▆▇▇███
wandb:            trainer/global_step ▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▆▆▆▆▇▇▇▇████▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▃▆▇▆▂▁▃▅█▅▇█
wandb:                    val_ce_loss ▁▁▁▁▁▃▇▇█▅▇█▆
wandb:                      val_kappa ▁▃▅▅▅▂▂▃▃▇▅▇█
wandb: 
wandb: Run summary:
wandb:                          epoch 13
wandb: exponential_decay_lr_scheduler 7e-05
wandb:                 test_acc_epoch 0.411
wandb:                  test_acc_step 0.35714
wandb:             test_ce_loss_epoch 2.29376
wandb:              test_ce_loss_step 2.26136
wandb:               test_kappa_epoch 0.18102
wandb:                test_kappa_step 0.125
wandb:                      train_acc 0.98909
wandb:                  train_ce_loss 0.04228
wandb:                    train_kappa 0.98618
wandb:            trainer/global_step 1235
wandb:                        val_acc 0.39778
wandb:                    val_ce_loss 2.38966
wandb:                      val_kappa 0.16612
wandb: 
wandb: 🚀 View run sage-sweep-3 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/70sq9jhe
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_194243-70sq9jhe/logs
wandb: Agent Starting Run: 5mx5oqjq with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.047572612715447e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 1.3509558483830627e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_194927-5mx5oqjq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-4
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/5mx5oqjq
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.114
Metric val_ce_loss improved by 0.013 >= min_delta = 0.0001. New best score: 1.101
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.101. Signaling Trainer to stop.
Epoch 11/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:27 •       13.89it/s v_num: oqjq      
                                     0:00:00                   val_ce_loss:     
                                                               2.313 val_kappa: 
                                                               0.136 val_acc:   
                                                               0.395            
                                                               train_ce_loss:   
                                                               0.016            
                                                               train_kappa:     
                                                               0.999 train_acc: 
                                                               0.999            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.3733333349227905     │
│    test_ce_loss_epoch     │    2.4106335639953613     │
│     test_kappa_epoch      │    0.0864419937133789     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 12.76it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▄▆▅▃▄▄▂▃▄▄▅▁▃▂▃█▆▆▁▄▅▃▅▄▇▅▅▄▆▆▅▂▄▅▃▅▂▅▃▅
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▂▁▇▇▂▅▄▅▄▅▁█▃▆▅▄▂▁▅▄▃▅▄▆▃▅▂▄▄▅▅▇▄▄▃▂▃▄▄▄
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▅▆▄▂▇▆▄▄▆█▅▁▄▂▃▇▄▇▃▄▅▅▅▄█▅▅▄▇▆▆▄▄▅▅▄▂▄▅▄
wandb:                      train_acc ▁▂▂▃▄▅▆▇████
wandb:                  train_ce_loss ███▇▇▆▄▃▂▁▁▁
wandb:                    train_kappa ▁▂▂▃▄▅▆▇████
wandb:            trainer/global_step ▁▂▂▂▂▃▃▃▃▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▂▁▅▅▇▅█▇██▇
wandb:                    val_ce_loss ▁▁▁▁▂▃▅▆█▇▇█
wandb:                      val_kappa ▁▃▂▄▆▇▆▇▇██▇
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.37333
wandb:                  test_acc_step 0.39286
wandb:             test_ce_loss_epoch 2.41063
wandb:              test_ce_loss_step 2.40201
wandb:               test_kappa_epoch 0.08644
wandb:                test_kappa_step 0.02999
wandb:                      train_acc 0.99909
wandb:                  train_ce_loss 0.01595
wandb:                    train_kappa 0.99858
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.39519
wandb:                    val_ce_loss 2.31287
wandb:                      val_kappa 0.13643
wandb: 
wandb: 🚀 View run true-sweep-4 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/5mx5oqjq
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_194927-5mx5oqjq/logs
wandb: Agent Starting Run: c4dx5zth with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.319205826168334e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 1.3529947964598624e-05
wandb: creating run
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_195529-c4dx5zth
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-5
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/c4dx5zth
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.109
Metric val_ce_loss improved by 0.002 >= min_delta = 0.0001. New best score: 1.108
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.108. Signaling Trainer to stop.
Epoch 11/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:23 •       16.87it/s v_num: 5zth      
                                     0:00:00                   val_ce_loss:     
                                                               2.301 val_kappa: 
                                                               0.170 val_acc:   
                                                               0.403            
                                                               train_ce_loss:   
                                                               0.020            
                                                               train_kappa:     
                                                               0.998 train_acc: 
                                                               0.998            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.42533332109451294    │
│    test_ce_loss_epoch     │    2.1679484844207764     │
│     test_kappa_epoch      │    0.19584137201309204    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:02 • 0:00:00 16.84it/s 
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler █▇▇▇▇▇▇▆▆▆▅▅▅▅▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▆▅▃▂▆▄▅▆▄▄▁▃▅▅▄▄▃▆█▅▄▃▄▅▆▆▄▅▄▅▅▃▃▆▇▅▄█▃▇
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▂▃▄▇▄▄▂▄▅▄█▆▇▄▄▆▅▂▁▃▄▅▃▅▂▂▆▁▄▄▃▆▄▃▃▅▅▁▄▄
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▆▃▃▃▃▃▃▅▃▅▂▂▂▃▃▂▁▃▇▅▁▄▄▃▆▆▁▂▄▄▅▃▁▄▄▅▃█▂▅
wandb:                      train_acc ▁▂▂▃▄▅▆▇▇███
wandb:                  train_ce_loss ███▇▇▆▄▃▂▁▁▁
wandb:                    train_kappa ▁▂▂▃▄▅▆▇████
wandb:            trainer/global_step ▁▂▂▂▂▂▃▃▃▃▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇█▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▃▂▃▅▁▃▂█▆▆█
wandb:                    val_ce_loss ▁▁▁▂▂▅▄█▆▆▆▆
wandb:                      val_kappa ▁▃▂▃▄▂▄▃█▇▇█
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.42533
wandb:                  test_acc_step 0.51786
wandb:             test_ce_loss_epoch 2.16795
wandb:              test_ce_loss_step 2.1803
wandb:               test_kappa_epoch 0.19584
wandb:                test_kappa_step 0.28686
wandb:                      train_acc 0.99827
wandb:                  train_ce_loss 0.01971
wandb:                    train_kappa 0.99796
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.40333
wandb:                    val_ce_loss 2.30087
wandb:                      val_kappa 0.16952
wandb: 
wandb: 🚀 View run distinctive-sweep-5 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/c4dx5zth
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_195529-c4dx5zth/logs
wandb: Agent Starting Run: uiaqddlh with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.448311020378875e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.149038690238364e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_200126-uiaqddlh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-6
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/uiaqddlh
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.107
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.107. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:29 •       13.34it/s v_num: ddlh      
                                     0:00:00                   val_ce_loss:     
                                                               2.176 val_kappa: 
                                                               0.165 val_acc:   
                                                               0.400            
                                                               train_ce_loss:   
                                                               0.058            
                                                               train_kappa:     
                                                               0.983 train_acc: 
                                                               0.987            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.40533334016799927    │
│    test_ce_loss_epoch     │    2.2087297439575195     │
│     test_kappa_epoch      │    0.1738918423652649     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 13.83it/s 
wandb: uploading wandb-summary.json; uploading config.yaml; uploading history steps 100-149, summary, console lines 41-61
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ██▇▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▅▆▂▄▅▂▃▄▄▅▁▃▁▅▁▅▄▃▄▅▂▅▄▄▅▆▅▅▃▆▅▅▁▅▆▆▄█▄▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▃▁▆▇▃▅▇▅▃▃▅▆▇▄█▅▅▃▃▅▆▃▄▄▃▄▆▃▅▄▃▅█▃▅▂▄▃▅▅
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▇▇▅▅▆▅▃▅▇█▃▄▃▂▂▄▄▁▅▆▃▅▂▄▇█▅▅▃▆▇▄▄▂▅▅▄▅▄▅
wandb:                      train_acc ▁▁▂▂▃▄▅▆▇██
wandb:                  train_ce_loss ███▇▇▆▅▃▂▁▁
wandb:                    train_kappa ▁▂▂▃▄▅▆▇███
wandb:            trainer/global_step ▁▁▂▂▂▃▄▄▄▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇█████▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▁▃▃▄▅▄▆▅█▇
wandb:                    val_ce_loss ▁▁▁▁▁▃▅▄▇▇█
wandb:                      val_kappa ▂▁▃▃▄▅▄▆▆█▇
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.40533
wandb:                  test_acc_step 0.39286
wandb:             test_ce_loss_epoch 2.20873
wandb:              test_ce_loss_step 2.27457
wandb:               test_kappa_epoch 0.17389
wandb:                test_kappa_step 0.20426
wandb:                      train_acc 0.98675
wandb:                  train_ce_loss 0.05772
wandb:                    train_kappa 0.98274
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.4
wandb:                    val_ce_loss 2.17554
wandb:                      val_kappa 0.16505
wandb: 
wandb: 🚀 View run flowing-sweep-6 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/uiaqddlh
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_200126-uiaqddlh/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 3gi43ovb with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 6.031799624671424e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 6.9783799959705e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_200656-3gi43ovb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-7
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/3gi43ovb
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.101
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.101. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:24 •       16.33it/s v_num: 3ovb      
                                     0:00:00                   val_ce_loss:     
                                                               2.317 val_kappa: 
                                                               0.150 val_acc:   
                                                               0.406            
                                                               train_ce_loss:   
                                                               0.096            
                                                               train_kappa:     
                                                               0.968 train_acc: 
                                                               0.973            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.40166667103767395    │
│    test_ce_loss_epoch     │     2.276824951171875     │
│     test_kappa_epoch      │    0.1615772843360901     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 14.68it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ██▇▇▇▇▇▇▇▇▇▇▆▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▄▄▁▃▂▆▃▅▂▆▃▂▄▄▃▁▃█▇▄▁▄▇▅▆▇▁▆▁▄▄▆▃▅▆▄▄█▄▃
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▃▅▆▄▄▃▄▃▅▂▆▆▇▄███▂▂▄▆▅▁▄▄▂▆▂▆▄▃▄▄▅▄▅▆▃▄▅
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▆▆▂▄▁▅▄▅▅█▅▄▅▃▅▄▂██▆▂▅▇▅▇▇▃█▄▇▆▇▃▅▆▇▆▇▄▆
wandb:                      train_acc ▁▂▂▃▃▄▅▆▇██
wandb:                  train_ce_loss ███▇▇▆▅▄▂▁▁
wandb:                    train_kappa ▁▂▂▃▄▄▅▆▇██
wandb:            trainer/global_step ▁▁▁▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▇▇▇▇▇██▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:                        val_acc ▂▁▂▃▅▆▅████
wandb:                    val_ce_loss ▁▁▁▁▂▂▅▄▆▆█
wandb:                      val_kappa ▂▁▂▃▃▆▄▆██▇
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.40167
wandb:                  test_acc_step 0.375
wandb:             test_ce_loss_epoch 2.27682
wandb:              test_ce_loss_step 2.49784
wandb:               test_kappa_epoch 0.16158
wandb:                test_kappa_step 0.21519
wandb:                      train_acc 0.97309
wandb:                  train_ce_loss 0.09646
wandb:                    train_kappa 0.96761
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.40556
wandb:                    val_ce_loss 2.3168
wandb:                      val_kappa 0.15043
wandb: 
wandb: 🚀 View run royal-sweep-7 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/3gi43ovb
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_200656-3gi43ovb/logs
wandb: Agent Starting Run: 0ix2qz54 with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 5.519312553060601e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 1.015682881002732e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_201221-0ix2qz54
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-8
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/0ix2qz54
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.100
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.100. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:25 •       14.95it/s v_num: qz54      
                                     0:00:00                   val_ce_loss:     
                                                               2.744 val_kappa: 
                                                               0.107 val_acc:   
                                                               0.372            
                                                               train_ce_loss:   
                                                               0.341            
                                                               train_kappa:     
                                                               0.848 train_acc: 
                                                               0.869            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.3646666705608368     │
│    test_ce_loss_epoch     │     2.743196964263916     │
│     test_kappa_epoch      │    0.0871044397354126     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 13.83it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▇▅▅▅▃▄▃▁▇▄▃▂▁▄▃▇▅▆▃▄▃▂▄▇█▅▃▆▃▄▁▄▄▅▂█▂▅▆▆
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▅▄▃▄▅▅▇▄▂▅▆▇▆▅▅▂▅▅▇▂▅█▇▂▂▅▄▄▆▄▆▄▅▃█▁▅▃▇▄
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step █▄▆▄▃▅▄▂▃▆▃▄▁▃█▄█▅▆▅▅▄▄▃▅▆▃▅▃▅▃▄▅▄▄▆▇▅▅▆
wandb:                      train_acc ▁▁▂▂▃▃▄▅▆▇█
wandb:                  train_ce_loss ████▇▇▆▅▄▂▁
wandb:                    train_kappa ▁▁▂▂▃▄▄▅▆▇█
wandb:            trainer/global_step ▂▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▂▂▁▁▂▅▁▄▄█▃
wandb:                    val_ce_loss ▁▁▁▁▃▁█▂▄▂▆
wandb:                      val_kappa ▂▁▂▁▁▄▁▃▅█▄
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.36467
wandb:                  test_acc_step 0.41071
wandb:             test_ce_loss_epoch 2.7432
wandb:              test_ce_loss_step 2.67122
wandb:               test_kappa_epoch 0.0871
wandb:                test_kappa_step 0.17647
wandb:                      train_acc 0.86905
wandb:                  train_ce_loss 0.34056
wandb:                    train_kappa 0.84788
wandb:            trainer/global_step 2090
wandb:                        val_acc 0.37222
wandb:                    val_ce_loss 2.74416
wandb:                      val_kappa 0.10743
wandb: 
wandb: 🚀 View run giddy-sweep-8 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/0ix2qz54
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_201221-0ix2qz54/logs
wandb: Agent Starting Run: 9839w8ke with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.594935753085661e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.7507905661419135e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_201756-9839w8ke
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-9
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/9839w8ke
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.105
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.105. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:23 •       16.57it/s v_num: w8ke      
                                     0:00:00                   val_ce_loss:     
                                                               2.284 val_kappa: 
                                                               0.174 val_acc:   
                                                               0.411            
                                                               train_ce_loss:   
                                                               0.080            
                                                               train_kappa:     
                                                               0.977 train_acc: 
                                                               0.979            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.4046666622161865     │
│    test_ce_loss_epoch     │    2.2972652912139893     │
│     test_kappa_epoch      │    0.1691380739212036     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 12.84it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ██▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▅▃▃▄▅▅▇▃▄▆▅▅▂▃▄▁▂▆▄▄▆▄▅▃▄▇▆▆▄▂▄▃▂▅▅▅▅█▇▅
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▃▅▃▇▄▄▃▄▅▄▃▄█▇▃▇█▃▃▄▄▄▂▃▃▂▄▃▃▆▄▇█▄▂▅▃▂▁▅
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▅▅▅▄▃▅█▄▅▇▅▄▄▂▄▂▂▆▅▆▇▄▃▄▅▆▅▆▆▁▄▄▃▆▅▆▇█▆▆
wandb:                      train_acc ▁▂▂▃▃▄▅▆▇██
wandb:                  train_ce_loss ███▇▇▆▅▄▂▁▁
wandb:                    train_kappa ▁▂▂▃▄▅▆▇▇██
wandb:            trainer/global_step ▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇▇████▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▂▂▄▂▅▅▅█▅▇
wandb:                    val_ce_loss ▁▁▁▁▃▂▃▆▅█▇
wandb:                      val_kappa ▁▂▂▄▂▅▅▅█▅▇
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.40467
wandb:                  test_acc_step 0.44643
wandb:             test_ce_loss_epoch 2.29727
wandb:              test_ce_loss_step 2.46435
wandb:               test_kappa_epoch 0.16914
wandb:                test_kappa_step 0.2797
wandb:                      train_acc 0.97922
wandb:                  train_ce_loss 0.08017
wandb:                    train_kappa 0.97698
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.41074
wandb:                    val_ce_loss 2.28393
wandb:                      val_kappa 0.17373
wandb: 
wandb: 🚀 View run cool-sweep-9 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/9839w8ke
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_201756-9839w8ke/logs
wandb: Agent Starting Run: 07m4oktc with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 5.2491659758535025e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 1.4933964902180926e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_202326-07m4oktc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-sweep-10
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/07m4oktc
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.105
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.105. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:24 •       15.72it/s v_num: oktc      
                                     0:00:00                   val_ce_loss:     
                                                               4.154 val_kappa: 
                                                               0.067 val_acc:   
                                                               0.364            
                                                               train_ce_loss:   
                                                               0.210            
                                                               train_kappa:     
                                                               0.914 train_acc: 
                                                               0.926            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.35066667199134827    │
│    test_ce_loss_epoch     │     4.268784999847412     │
│     test_kappa_epoch      │    0.06504064798355103    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 13.17it/s 
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ██▇▇▇▇▇▇▆▅▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▇▅▅▅▄▃▁▅▅▅▄▇▃▃▃▆▄▅▃▃▁▃▁▃▇▃▅▇▃▅▃▆▇█▁▃▁▆▃▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▅▃▁▅▃▄▇▄▂▄▄▅▄▄▆▁▄▃▅▃▅█▇▂▂▅▄▃▄▄▆▄▄▃▆▃▅▅▆▅
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step █▄▄▃▁▆▂▂▂▄▄▅▅▄▆▃▆▃▅▄▂▄▃▁▇▃▄▇▄█▃▆▆▇▃▄▃▇▅▃
wandb:                      train_acc ▁▁▂▂▃▃▄▅▆▇█
wandb:                  train_ce_loss ████▇▇▆▅▄▂▁
wandb:                    train_kappa ▁▂▂▂▃▄▅▆▇▇█
wandb:            trainer/global_step ▁▁▁▂▂▂▃▃▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇███▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▂▁▂▁▁▂█▃▇▇▃
wandb:                    val_ce_loss ▁▁▁▁▂▂▁▃▂▃█
wandb:                      val_kappa ▂▁▂▁▁▃█▃▇▆▃
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.35067
wandb:                  test_acc_step 0.33929
wandb:             test_ce_loss_epoch 4.26878
wandb:              test_ce_loss_step 4.44619
wandb:               test_kappa_epoch 0.06504
wandb:                test_kappa_step -0.03016
wandb:                      train_acc 0.92617
wandb:                  train_ce_loss 0.20985
wandb:                    train_kappa 0.91386
wandb:            trainer/global_step 2090
wandb:                        val_acc 0.3637
wandb:                    val_ce_loss 4.15372
wandb:                      val_kappa 0.06733
wandb: 
wandb: 🚀 View run bright-sweep-10 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/07m4oktc
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_202326-07m4oktc/logs
wandb: Agent Starting Run: svaeqm5v with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 6.951872902027624e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.951918703072749e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_202857-svaeqm5v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-11
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/svaeqm5v
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.102
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.102. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:24 •       16.00it/s v_num: qm5v      
                                     0:00:00                   val_ce_loss:     
                                                               4.358 val_kappa: 
                                                               0.063 val_acc:   
                                                               0.361            
                                                               train_ce_loss:   
                                                               0.189            
                                                               train_kappa:     
                                                               0.924 train_acc: 
                                                               0.935            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.3610000014305115     │
│    test_ce_loss_epoch     │     4.337547302246094     │
│     test_kappa_epoch      │    0.06591182947158813    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 15.13it/s 
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▇▄▂▅▄▆▅▄▂▇▅▅▃▄▅▁▂▄█▄▅▇▇▄▄▆▃▅▄▄▇▄▃▄▆▄▆▇▆▅
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▃▄▅▃▃▃▃▃▇▃▂▄█▄▄▅▆▄▁▅▂▂▃▆▄▂▄▃▃▅▁▄▅▄▃▄▃▃▃▃
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step █▃▄▆▂▃▂▃▃▆▄▂▁▂▃▂▂▃▆▁▂▃▃▂▄▅▁▄▂▃▃▃▁▁▂▄▃▄▂▅
wandb:                      train_acc ▁▁▂▂▃▃▄▅▆▇█
wandb:                  train_ce_loss ████▇▇▆▅▃▂▁
wandb:                    train_kappa ▁▂▂▂▃▄▅▆▇██
wandb:            trainer/global_step ▁▁▂▂▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▁▁▂▂▁█▂▅▆▂
wandb:                    val_ce_loss ▁▁▁▁▁▁▁▅▂▃█
wandb:                      val_kappa ▂▁▂▂▁▁█▂▇█▂
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.361
wandb:                  test_acc_step 0.35714
wandb:             test_ce_loss_epoch 4.33755
wandb:              test_ce_loss_step 3.99636
wandb:               test_kappa_epoch 0.06591
wandb:                test_kappa_step 0.13895
wandb:                      train_acc 0.93457
wandb:                  train_ce_loss 0.18855
wandb:                    train_kappa 0.92362
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.36074
wandb:                    val_ce_loss 4.35759
wandb:                      val_kappa 0.0635
wandb: 
wandb: 🚀 View run astral-sweep-11 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/svaeqm5v
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_202857-svaeqm5v/logs
wandb: Agent Starting Run: kt5sqvtq with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.00012258599315823844
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.550883524753298e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_203426-kt5sqvtq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-12
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/kt5sqvtq
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.107
Metric val_ce_loss improved by 0.004 >= min_delta = 0.0001. New best score: 1.103
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.103. Signaling Trainer to stop.
Epoch 11/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:25 •       15.05it/s v_num: qvtq      
                                     0:00:00                   val_ce_loss:     
                                                               1.281 val_kappa: 
                                                               0.264 val_acc:   
                                                               0.442            
                                                               train_ce_loss:   
                                                               0.652            
                                                               train_kappa:     
                                                               0.668 train_acc: 
                                                               0.715            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.45366665720939636    │
│    test_ce_loss_epoch     │     1.254696249961853     │
│     test_kappa_epoch      │    0.2751889228820801     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 14.18it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ██▇▇▇▇▇▇▆▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▆▅▄▄▇▂▁▂▅▅▅▄▂▅▅▆▅▄▃▃▃▅▃▃▆▄▃▄▁▂▁▆▁▇▅▄▃█▅▅
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▂▄▅▄▄▅▆▇▄▃▆▆▅▄▄▃▃▅▅▅█▆▄▆▃▃▆▃▇▇█▄▆▂▅▃▃▁▅▃
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▇▆▂▄▃▅▃▄▁▆▃▄▅▄▇▃▄▅▅▆▁▅▅▁▅▆▃▂▂▃▄▆▃▇▃▆▇█▅█
wandb:                      train_acc ▁▁▂▂▂▃▄▅▅▆▇█
wandb:                  train_ce_loss ████▇▇▆▆▅▄▂▁
wandb:                    train_kappa ▁▁▂▂▃▃▄▅▆▇▇█
wandb:            trainer/global_step ▁▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇██▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▂▂▂▁▂▁▅▁▅▁▂█
wandb:                    val_ce_loss ▁▁▁▁▁▃▁▄▁██▁
wandb:                      val_kappa ▁▂▂▁▂▁▅▁▄▁▂█
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 0.00011
wandb:                 test_acc_epoch 0.45367
wandb:                  test_acc_step 0.5
wandb:             test_ce_loss_epoch 1.2547
wandb:              test_ce_loss_step 1.07876
wandb:               test_kappa_epoch 0.27519
wandb:                test_kappa_step 0.46617
wandb:                      train_acc 0.71494
wandb:                  train_ce_loss 0.65232
wandb:                    train_kappa 0.66785
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.44222
wandb:                    val_ce_loss 1.28132
wandb:                      val_kappa 0.26407
wandb: 
wandb: 🚀 View run bumbling-sweep-12 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/kt5sqvtq
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_203426-kt5sqvtq/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: lxcdeqzi with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 6.840510722374592e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.2768165372926652e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_204026-lxcdeqzi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-13
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/lxcdeqzi
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.114
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.114. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:21 •       17.73it/s v_num: eqzi      
                                     0:00:00                   val_ce_loss:     
                                                               2.343 val_kappa: 
                                                               0.136 val_acc:   
                                                               0.395            
                                                               train_ce_loss:   
                                                               0.151            
                                                               train_kappa:     
                                                               0.945 train_acc: 
                                                               0.952            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.39100000262260437    │
│    test_ce_loss_epoch     │     2.290358543395996     │
│     test_kappa_epoch      │    0.1344902515411377     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:02 • 0:00:00 18.61it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ████▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step █▂▄▂▅▆▃▄▂▄▃▃▃▂▅▂▅▄▇▄▅▂▅▃▅▇▂▆▁▃▄▄▃▄▄▄▄▅▄▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▂▃▅▄▄▃▆▃▄▃▆▄▆▅▅▆▅▃▁▄▃█▅▆▅▂▄▂█▄▅▄▅▃▃▅▃▂▂▄
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step █▅▅▃▆▆▄▆▃▅▂▅▅▂▆▄▇▅▇▇▅▃▃▃▆█▃▄▁▄▃▇▃▃▇▇█▅▃▅
wandb:                      train_acc ▁▁▂▂▃▃▄▅▇▇█
wandb:                  train_ce_loss ████▇▇▆▅▃▂▁
wandb:                    train_kappa ▁▁▂▂▃▄▅▆▇▇█
wandb:            trainer/global_step ▁▂▂▂▃▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▂▂▃▃▇▁▄▃▇█
wandb:                    val_ce_loss ▁▁▁▁▁▂▇▄█▅▆
wandb:                      val_kappa ▁▁▂▃▃█▂▄▄██
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.391
wandb:                  test_acc_step 0.41071
wandb:             test_ce_loss_epoch 2.29036
wandb:              test_ce_loss_step 2.21826
wandb:               test_kappa_epoch 0.13449
wandb:                test_kappa_step 0.15663
wandb:                      train_acc 0.95173
wandb:                  train_ce_loss 0.15053
wandb:                    train_kappa 0.94479
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.39481
wandb:                    val_ce_loss 2.34257
wandb:                      val_kappa 0.1364
wandb: 
wandb: 🚀 View run charmed-sweep-13 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/lxcdeqzi
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_204026-lxcdeqzi/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ml8h6hho with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.00013492993012448017
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.477192938128564e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_204553-ml8h6hho
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-14
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ml8h6hho
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.098
Metric val_ce_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.097
Metric val_ce_loss improved by 0.119 >= min_delta = 0.0001. New best score: 0.977
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 0.977. Signaling Trainer to stop.
Epoch 19/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:24 •       15.59it/s v_num: 6hho      
                                     0:00:00                   val_ce_loss:     
                                                               3.494 val_kappa: 
                                                               0.170 val_acc:   
                                                               0.412            
                                                               train_ce_loss:   
                                                               0.115            
                                                               train_kappa:     
                                                               0.952 train_acc: 
                                                               0.957            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │     0.398333340883255     │
│    test_ce_loss_epoch     │    3.4774017333984375     │
│     test_kappa_epoch      │    0.12695038318634033    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 14.50it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb: exponential_decay_lr_scheduler ██▇▇▇▇▇▆▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step █▅▃▆▄▄▄▁▆▅▅▃▅▇▂█▃▅▄▄▂▁▂▇█▆▅▆▃▄▃▂▄▄▃▇▂▅▅▆
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▁▄▆▄▅▅▇▆▂▄▆▇▅▄▅▃▅▅▅▆▇█▇▄▂▅▅▃▅▆▅▄▅▄▅▃▆▂▄▃
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▇▃▄▅▁▆▆▁▂▅▂▅▅▃▅▃▄▆▇▄▃▄▂▁██▇▃▅▆▂▁▅▃▂▆▄▄▇▆
wandb:                      train_acc ▁▁▁▂▂▂▃▃▄▄▅▅▆▆▇▇████
wandb:                  train_ce_loss █████▇▇▇▆▆▅▅▄▃▃▂▂▁▁▁
wandb:                    train_kappa ▁▁▁▂▂▃▄▄▅▅▆▆▆▇▇▇████
wandb:            trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇█▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▁▁▂▂▁▄▄▁▇▂▂▂▃█▃▄▃▂▃
wandb:                    val_ce_loss ▁▁▁▁▁▂▁▁█▁▅▃▄▃▁▃▃▇▇▄
wandb:                      val_kappa ▁▁▁▁▂▁▄▄▁▇▂▂▂▂█▃▄▃▂▃
wandb: 
wandb: Run summary:
wandb:                          epoch 20
wandb: exponential_decay_lr_scheduler 0.00011
wandb:                 test_acc_epoch 0.39833
wandb:                  test_acc_step 0.46429
wandb:             test_ce_loss_epoch 3.4774
wandb:              test_ce_loss_step 2.93847
wandb:               test_kappa_epoch 0.12695
wandb:                test_kappa_step 0.23438
wandb:                      train_acc 0.95671
wandb:                  train_ce_loss 0.11549
wandb:                    train_kappa 0.95222
wandb:            trainer/global_step 1900
wandb:                        val_acc 0.41222
wandb:                    val_ce_loss 3.49411
wandb:                      val_kappa 0.1695
wandb: 
wandb: 🚀 View run whole-sweep-14 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ml8h6hho
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_204553-ml8h6hho/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: e6etpsra with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.512168846921846e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 6.281122366182823e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_205559-e6etpsra
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-15
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/e6etpsra
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.114
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.114. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:21 •       18.62it/s v_num: psra      
                                     0:00:00                   val_ce_loss:     
                                                               2.136 val_kappa: 
                                                               0.190 val_acc:   
                                                               0.410            
                                                               train_ce_loss:   
                                                               0.083            
                                                               train_kappa:     
                                                               0.974 train_acc: 
                                                               0.978            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.3973333239555359     │
│    test_ce_loss_epoch     │    2.1523447036743164     │
│     test_kappa_epoch      │    0.16555404663085938    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 15.14it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ███▇▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▂▂▂▂▂▂▂▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▆▅▅▅▅▆▅▁▂█▄▃▁▆▅▃▄█▆▅▃▃▆▆▅▆▆▄▆▅▇▃▄▅▆▄█▅▁▃
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▄▇▄▆▅▇▆▇▅▃▆▇▇▇▄▄▇▁▁▃▇▇▂▄▅▃█▃▄▅▅▇▅▆▅▆▃▃▆█
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▆▄▃▄▃▇▄▁▃▇▄▃▃▅▄▃▅█▇▆▄▃▅▆▄▅▆▅▃▅▇▄▄▄▆▅▇▅▂▄
wandb:                      train_acc ▁▂▂▃▃▄▅▆▇██
wandb:                  train_ce_loss ███▇▇▆▅▄▂▂▁
wandb:                    train_kappa ▁▂▂▃▄▅▆▆▇██
wandb:            trainer/global_step ▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▆▇▇▇▇███▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▃▁▃▄▅▅▇██▆█
wandb:                    val_ce_loss ▁▁▁▁▁▂▂▃▅█▅
wandb:                      val_kappa ▂▁▃▃▅▅▆▇█▅█
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.39733
wandb:                  test_acc_step 0.32143
wandb:             test_ce_loss_epoch 2.15234
wandb:              test_ce_loss_step 2.65158
wandb:               test_kappa_epoch 0.16555
wandb:                test_kappa_step 0.11066
wandb:                      train_acc 0.97802
wandb:                  train_ce_loss 0.08316
wandb:                    train_kappa 0.9739
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.40963
wandb:                    val_ce_loss 2.13601
wandb:                      val_kappa 0.18959
wandb: 
wandb: 🚀 View run worldly-sweep-15 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/e6etpsra
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_205559-e6etpsra/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: kitmgk1o with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 6.247947040549693e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.4684561490620074e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_210135-kitmgk1o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-16
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/kitmgk1o
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.104
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.104. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:27 •       13.97it/s v_num: gk1o      
                                     0:00:00                   val_ce_loss:     
                                                               2.203 val_kappa: 
                                                               0.181 val_acc:   
                                                               0.403            
                                                               train_ce_loss:   
                                                               0.082            
                                                               train_kappa:     
                                                               0.971 train_acc: 
                                                               0.978            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.39800000190734863    │
│    test_ce_loss_epoch     │    2.2290618419647217     │
│     test_kappa_epoch      │    0.16580551862716675    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 13.89it/s 
wandb: uploading history steps 102-102, summary; updating run config
wandb: uploading output.log; uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ██▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▁▄▅▅▄▃▇▁▂▄▃▄▃▃▁█▄▆█▇▆▄▃▃▆▇▃▅█▄▆▅▆▂▅▆▆▃▂▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▅▅▃▆▆▆▄▆▆▆▅▆▅██▅▄▃▁▃▅▆▄▅▂▃█▄▁▅▇▃▄▄▅▃▄▄█▅
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▄▆▆▇▅▅▆▁▆▅▆▅▃▂▂▆▄▅▇▆▅▄▄▄▇▇▂▇██▆█▆▁▆█▄▄▃▅
wandb:                      train_acc ▁▁▂▂▃▄▅▆▇██
wandb:                  train_ce_loss ███▇▇▆▅▄▂▂▁
wandb:                    train_kappa ▁▂▂▃▃▄▅▆▇██
wandb:            trainer/global_step ▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▆▆▇▇▇▇▇███▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▂▁▂▃▅▄▅▁█▆
wandb:                    val_ce_loss ▁▁▁▁▁▂▂▄█▃▄
wandb:                      val_kappa ▁▁▁▂▄▃▅▄▁█▆
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.398
wandb:                  test_acc_step 0.41071
wandb:             test_ce_loss_epoch 2.22906
wandb:              test_ce_loss_step 2.25371
wandb:               test_kappa_epoch 0.16581
wandb:                test_kappa_step 0.17027
wandb:                      train_acc 0.9777
wandb:                  train_ce_loss 0.08204
wandb:                    train_kappa 0.97135
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.40259
wandb:                    val_ce_loss 2.20253
wandb:                      val_kappa 0.18085
wandb: 
wandb: 🚀 View run gentle-sweep-16 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/kitmgk1o
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_210135-kitmgk1o/logs
wandb: Agent Starting Run: 8ekmhdek with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 0.0001042018583892548
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.722811200356592e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_210802-8ekmhdek
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-17
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/8ekmhdek
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.103
Metric val_ce_loss improved by 0.066 >= min_delta = 0.0001. New best score: 1.037
Metric val_ce_loss improved by 0.064 >= min_delta = 0.0001. New best score: 0.973
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 0.973. Signaling Trainer to stop.
Epoch 21/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:24 •       15.89it/s v_num: hdek      
                                     0:00:00                   val_ce_loss:     
                                                               2.041 val_kappa: 
                                                               0.595 val_acc:   
                                                               0.619            
                                                               train_ce_loss:   
                                                               0.074            
                                                               train_kappa:     
                                                               0.971 train_acc: 
                                                               0.973            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │     0.609333336353302     │
│    test_ce_loss_epoch     │    1.9919058084487915     │
│     test_kappa_epoch      │    0.5691580772399902     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:04 • 0:00:00 10.45it/s 
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██
wandb: exponential_decay_lr_scheduler █████▇▇▇▇▇▇▆▆▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▇▅▅▅▅▅█▃▃▅▃▃▅▂▇▁▆▅▆▇▆▇▇▃▄▅▃▄▂▂▅▅▅▆▇▃▇▇▅█
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▂▆▂▄▄▂▂▆▆▆▆▅▃▆▃█▃▄▂▁▂▂▂▅▅▂▆▄▅▅▃▆▅▃▃▇▂▂▃▂
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step █▄▃▆▅▄▇▃▆▇▅▄▅▃▆▂▄▅▆▇▆█▃▃▅▆▃▆▃▁▅▅▅▆▅▄▆▆▅█
wandb:                      train_acc ▁▁▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇▇████
wandb:                  train_ce_loss █████▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁
wandb:                    train_kappa ▁▁▁▂▂▃▄▄▅▅▆▆▆▇▇▇██████
wandb:            trainer/global_step ▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇█████▁▁▁▁▁
wandb:                        val_acc ▁▁▁▂▂▁▁▄▃▅▁▇▂▃▃▆█▅█▂▆█
wandb:                    val_ce_loss ▁▁▁▁▁▂▃▁▂▁█▁▂▂▄▂▁▂▁▇▂▂
wandb:                      val_kappa ▁▁▁▂▂▁▁▄▂▆▁▇▂▃▃▇█▅█▂▆█
wandb: 
wandb: Run summary:
wandb:                          epoch 22
wandb: exponential_decay_lr_scheduler 8e-05
wandb:                 test_acc_epoch 0.60933
wandb:                  test_acc_step 0.69643
wandb:             test_ce_loss_epoch 1.99191
wandb:              test_ce_loss_step 1.64027
wandb:               test_kappa_epoch 0.56916
wandb:                test_kappa_step 0.7615
wandb:                      train_acc 0.97329
wandb:                  train_ce_loss 0.07374
wandb:                    train_kappa 0.9711
wandb:            trainer/global_step 4180
wandb:                        val_acc 0.61852
wandb:                    val_ce_loss 2.04077
wandb:                      val_kappa 0.59515
wandb: 
wandb: 🚀 View run royal-sweep-17 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/8ekmhdek
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_210802-8ekmhdek/logs
wandb: Agent Starting Run: oo8yf2w3 with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 6.970853779207268e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.6212850414965646e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_211901-oo8yf2w3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-18
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/oo8yf2w3
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.110
Metric val_ce_loss improved by 0.010 >= min_delta = 0.0001. New best score: 1.100
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.100. Signaling Trainer to stop.
Epoch 11/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:23 •       16.40it/s v_num: f2w3      
                                     0:00:00                   val_ce_loss:     
                                                               2.235 val_kappa: 
                                                               0.166 val_acc:   
                                                               0.411            
                                                               train_ce_loss:   
                                                               0.100            
                                                               train_kappa:     
                                                               0.965 train_acc: 
                                                               0.970            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.4090000092983246     │
│    test_ce_loss_epoch     │    2.2609431743621826     │
│     test_kappa_epoch      │    0.16526609659194946    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 15.38it/s 
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ███▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▅▅▅▅▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▅▇▅▅▅▆▅▅▅▆▂▆▄▆▃▃▅▆▆▅█▃▇▃▆▅▃▆▆▅▅▃▁▆▄▃▅██▅
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▃▂▅▁▄▂▂▄▂▄▅▂▆▃▅█▄▂▃▃▂▄▁▄▂▂▅▃▃▄▄▆▅▄▃▅▄▂▂▃
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step █▆▆▇▇▇▅▅▅▆▁▅▅▆▂▅▄▅█▇▇▂█▃▅▅▃▅▆▃▅▃▂▅▁▆▅▆▇▆
wandb:                      train_acc ▁▁▂▂▃▃▄▅▆▇██
wandb:                  train_ce_loss ████▇▇▆▅▄▂▁▁
wandb:                    train_kappa ▁▁▂▂▃▄▅▅▆▇██
wandb:            trainer/global_step ▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▇▇▇▇▇▇▇██▁▁▁▁▁▁▁▁▁█
wandb:                        val_acc ▁▁▃▃▂▅▂▆▅▆▆█
wandb:                    val_ce_loss ▁▁▁▁▂▁▂▄▄▆█▆
wandb:                      val_kappa ▁▂▂▃▂▅▃▇▆▅▆█
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.409
wandb:                  test_acc_step 0.42857
wandb:             test_ce_loss_epoch 2.26094
wandb:              test_ce_loss_step 2.18406
wandb:               test_kappa_epoch 0.16527
wandb:                test_kappa_step 0.24904
wandb:                      train_acc 0.97041
wandb:                  train_ce_loss 0.10007
wandb:                    train_kappa 0.96497
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.41111
wandb:                    val_ce_loss 2.23526
wandb:                      val_kappa 0.16636
wandb: 
wandb: 🚀 View run misunderstood-sweep-18 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/oo8yf2w3
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_211901-oo8yf2w3/logs
wandb: Agent Starting Run: 4o3hucgw with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.631844029815053e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.714973528107466e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_212453-4o3hucgw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-19
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/4o3hucgw
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.113
Metric val_ce_loss improved by 0.011 >= min_delta = 0.0001. New best score: 1.102
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.102. Signaling Trainer to stop.
Epoch 11/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:23 •       16.56it/s v_num: ucgw      
                                     0:00:00                   val_ce_loss:     
                                                               2.334 val_kappa: 
                                                               0.189 val_acc:   
                                                               0.401            
                                                               train_ce_loss:   
                                                               0.028            
                                                               train_kappa:     
                                                               0.996 train_acc: 
                                                               0.996            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.41200000047683716    │
│    test_ce_loss_epoch     │    2.2844250202178955     │
│     test_kappa_epoch      │    0.18939930200576782    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 13.54it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler █████▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▇▇▅▄▄▆▄▅▁▃▃▄▄▂▅▁▃▂█▄▄▃▃▃▇▇▃▇▄▃▅▃▁▂▅▆▂▇▄▅
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▃▂▄▄▄▂▇▆▆▆▆▆▆▇▅██▆▁▆▅▆▅▇▅▄▇▃▅▇▃▆▆▇▆▅▅▃▅▃
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▇▇▆▄▃▇▃▆▃▆▆▃▃▁▆▃▅▃▇▂▅▄▁▂█▆▂▆▆▂▅▄▁▁▆▄▁▇▄▆
wandb:                      train_acc ▁▂▂▃▃▄▅▇▇███
wandb:                  train_ce_loss ███▇▇▆▅▃▂▂▁▁
wandb:                    train_kappa ▁▂▂▃▄▅▆▇▇███
wandb:            trainer/global_step ▂▂▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▇▇▇█▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:                        val_acc ▁▃▃▃▇▆▆▃▆▇▄█
wandb:                    val_ce_loss ▁▁▁▂▁▃▃█▅██▆
wandb:                      val_kappa ▁▂▃▃▆▅▇▄▇▆▆█
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.412
wandb:                  test_acc_step 0.44643
wandb:             test_ce_loss_epoch 2.28443
wandb:              test_ce_loss_step 1.73704
wandb:               test_kappa_epoch 0.1894
wandb:                test_kappa_step 0.29032
wandb:                      train_acc 0.99572
wandb:                  train_ce_loss 0.02773
wandb:                    train_kappa 0.99559
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.40148
wandb:                    val_ce_loss 2.33386
wandb:                      val_kappa 0.18909
wandb: 
wandb: 🚀 View run absurd-sweep-19 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/4o3hucgw
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_212453-4o3hucgw/logs
wandb: Agent Starting Run: he52a58p with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 7.284453261477594e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.938446311272622e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_213047-he52a58p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-20
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/he52a58p
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.129
Metric val_ce_loss improved by 0.030 >= min_delta = 0.0001. New best score: 1.098
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.098. Signaling Trainer to stop.
Epoch 11/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:22 •       17.10it/s v_num: a58p      
                                     0:00:00                   val_ce_loss:     
                                                               2.395 val_kappa: 
                                                               0.076 val_acc:   
                                                               0.392            
                                                               train_ce_loss:   
                                                               0.454            
                                                               train_kappa:     
                                                               0.793 train_acc: 
                                                               0.814            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.3889999985694885     │
│    test_ce_loss_epoch     │    2.5066771507263184     │
│     test_kappa_epoch      │    0.08062863349914551    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:02 • 0:00:00 17.85it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ████▇▇▇▇▇▇▇▇▆▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▆▆▅▇▆▄▁▂▆▄▃▂▃█▆▆▅▆▄▄▁▂▃▆█▁▃▅▄▂▂▆▃█▃▇▁▂▃▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▄▂▃▃▃▅█▅▃▅▅▅▆▅▅▂▄▃▆▄▆▇▅▃▁▄▆▃▆▆▆▂▄▃▆▂▆▄▆▅
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▆▅▅▆▂▇▃▅▄▅▅▄▂▇█▆▇█▇▇▄▂▃▃█▄▁▂▆▇▂▄▃▇▆▆▆▂▄▃
wandb:                      train_acc ▁▁▁▂▃▃▄▅▆▆▇█
wandb:                  train_ce_loss ████▇▇▆▅▄▃▂▁
wandb:                    train_kappa ▁▁▂▂▃▄▅▅▆▇▇█
wandb:            trainer/global_step ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▆▆▆▇▇▇▇▇▇███▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▂▂▁▁▁▂▂█▆▅▃
wandb:                    val_ce_loss ▁▁▁▂▄█▃▅▁▃▃▆
wandb:                      val_kappa ▁▂▂▁▁▂▃▂█▅▆▃
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 7e-05
wandb:                 test_acc_epoch 0.389
wandb:                  test_acc_step 0.375
wandb:             test_ce_loss_epoch 2.50668
wandb:              test_ce_loss_step 2.63223
wandb:               test_kappa_epoch 0.08063
wandb:                test_kappa_step -0.06233
wandb:                      train_acc 0.81436
wandb:                  train_ce_loss 0.45398
wandb:                    train_kappa 0.79305
wandb:            trainer/global_step 2280
wandb:                        val_acc 0.39185
wandb:                    val_ce_loss 2.39454
wandb:                      val_kappa 0.076
wandb: 
wandb: 🚀 View run earnest-sweep-20 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/he52a58p
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_213047-he52a58p/logs
wandb: Agent Starting Run: ay1a8cgp with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 8.381574010550002e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.7094304355853094e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_213641-ay1a8cgp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-21
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ay1a8cgp
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.112
Metric val_ce_loss improved by 0.005 >= min_delta = 0.0001. New best score: 1.107
Metric val_ce_loss improved by 0.005 >= min_delta = 0.0001. New best score: 1.102
Metric val_ce_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.101
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.101. Signaling Trainer to stop.
Epoch 13/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:24 •       15.58it/s v_num: 8cgp      
                                     0:00:00                   val_ce_loss:     
                                                               1.830 val_kappa: 
                                                               0.259 val_acc:   
                                                               0.456            
                                                               train_ce_loss:   
                                                               0.303            
                                                               train_kappa:     
                                                               0.873 train_acc: 
                                                               0.882            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.45133334398269653    │
│    test_ce_loss_epoch     │    1.8351728916168213     │
│     test_kappa_epoch      │    0.23670828342437744    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 13.25it/s 
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇█
wandb: exponential_decay_lr_scheduler █████▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▅▆▅▅▁▅▅▃▃▃▃▄▅▃█▄▃▅▇▄▃▅▅▄▅▆▅▅▅▇▄▅▄▄▆▄▄▇▃▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▃▂▂▃▇▃▁▄▄▆▅▅▃▅▂▃▄▃▁▄▅▆▅▃▃▁▄▂▅▄▆▃▄▃▂▂▄▂█▄
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▄▅▅▄▁▆▅▃▅▆▅▅▅▃█▅▂▅▇▅▃▄▄▄▇█▃▆▃▇▅▅▃▄▅▆▅▇▄▅
wandb:                      train_acc ▁▁▁▂▂▃▃▄▄▅▆▇▇█
wandb:                  train_ce_loss █████▇▇▆▆▅▄▃▂▁
wandb:                    train_kappa ▁▁▁▂▂▃▄▄▅▆▆▇██
wandb:            trainer/global_step ▁▁▁▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▇▇▇▇▇▇██▁▁▁▁▁▁▁▁▁▁▁█
wandb:                        val_acc ▂▁▂▃▁▁▂▁▅▆█▄▃▆
wandb:                    val_ce_loss ▁▁▁▁▂▄▆█▂▁▂▆▆▃
wandb:                      val_kappa ▂▂▂▃▁▁▁▁▄▇█▄▃▇
wandb: 
wandb: Run summary:
wandb:                          epoch 14
wandb: exponential_decay_lr_scheduler 7e-05
wandb:                 test_acc_epoch 0.45133
wandb:                  test_acc_step 0.42857
wandb:             test_ce_loss_epoch 1.83517
wandb:              test_ce_loss_step 1.94321
wandb:               test_kappa_epoch 0.23671
wandb:                test_kappa_step 0.24841
wandb:                      train_acc 0.88152
wandb:                  train_ce_loss 0.30322
wandb:                    train_kappa 0.87305
wandb:            trainer/global_step 2660
wandb:                        val_acc 0.45593
wandb:                    val_ce_loss 1.83033
wandb:                      val_kappa 0.25942
wandb: 
wandb: 🚀 View run flowing-sweep-21 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ay1a8cgp
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_213641-ay1a8cgp/logs
wandb: Agent Starting Run: iydwkqt5 with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 0.00019803304466861824
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 4.611888825225099e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_214349-iydwkqt5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-22
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/iydwkqt5
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.104
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.104. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:25 •       15.40it/s v_num: kqt5      
                                     0:00:00                   val_ce_loss:     
                                                               1.150 val_kappa: 
                                                               0.373 val_acc:   
                                                               0.511            
                                                               train_ce_loss:   
                                                               0.735            
                                                               train_kappa:     
                                                               0.593 train_acc: 
                                                               0.662            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.5120000243186951     │
│    test_ce_loss_epoch     │     1.129611611366272     │
│     test_kappa_epoch      │    0.37357258796691895    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 13.83it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ███▇▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▆▂▄▇▄▅█▄▂▅▄▅▃▅▆▂▁▅▇▇▅▇▅▃▄▇▄▃▅▅█▅▄▅▆▃█▅▆▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▂▆▅▅▅▄▂▅▇▃▃▄▆▆▂▅█▅▂▃▄▂▆▅▄▁▅▅▅▅▂▄▅▄▄▅▁▃▂▃
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▆▃▃█▅▄▆▃▄▄▄▄▂▅▅▃▁▅▅▆▄▅▃▄▅▆▄▄▄▄▆▅▄▄▄▅▆▄▆▄
wandb:                      train_acc ▁▁▁▂▂▄▅▆▇▇█
wandb:                  train_ce_loss ████▇▆▅▄▃▂▁
wandb:                    train_kappa ▁▁▁▂▃▄▅▆▇██
wandb:            trainer/global_step ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇█▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▂▁▁▂▁▄▃▂▃█
wandb:                    val_ce_loss ▁▁▁▁▁▆▃▄█▅▁
wandb:                      val_kappa ▁▂▂▁▂▁▂▃▂▂█
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 0.00018
wandb:                 test_acc_epoch 0.512
wandb:                  test_acc_step 0.48214
wandb:             test_ce_loss_epoch 1.12961
wandb:              test_ce_loss_step 1.0229
wandb:               test_kappa_epoch 0.37357
wandb:                test_kappa_step 0.35012
wandb:                      train_acc 0.66243
wandb:                  train_ce_loss 0.73475
wandb:                    train_kappa 0.59284
wandb:            trainer/global_step 2090
wandb:                        val_acc 0.51148
wandb:                    val_ce_loss 1.15026
wandb:                      val_kappa 0.37262
wandb: 
wandb: 🚀 View run golden-sweep-22 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/iydwkqt5
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_214349-iydwkqt5/logs
wandb: Agent Starting Run: h6dwz4j2 with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.221695281640076e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.530389989451133e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_214920-h6dwz4j2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-23
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/h6dwz4j2
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.117
Metric val_ce_loss improved by 0.014 >= min_delta = 0.0001. New best score: 1.102
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.102. Signaling Trainer to stop.
Epoch 11/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:24 •       16.02it/s v_num: z4j2      
                                     0:00:00                   val_ce_loss:     
                                                               2.361 val_kappa: 
                                                               0.132 val_acc:   
                                                               0.385            
                                                               train_ce_loss:   
                                                               0.013            
                                                               train_kappa:     
                                                               0.999 train_acc: 
                                                               1.000            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.39633333683013916    │
│    test_ce_loss_epoch     │     2.310173511505127     │
│     test_kappa_epoch      │    0.15408724546432495    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 12.95it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ██▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▆▄▃▅▅▆▁▆▃█▇▂▃▇▂▄▄▇▃▅▄▂▆▅▄▆▅▅▃▃▄▄▂▄▅▄▅█▅▅
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▃▅▆▄▂▃█▃▆▁▃▇▆▅▅▆▆▁▄▄▅▇▃▅▄▂▅▂█▆▅▃█▆▄▇▄▂▄▅
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▇▄▅▅▃▅▁▆▄█▅▃▂▇▄▄▃▅▄▆▂▄▅▅▅▅▅▇▄▂▆▄▂▃▅▃▆▅▆▄
wandb:                      train_acc ▁▂▂▃▃▄▆▇▇███
wandb:                  train_ce_loss ███▇▇▆▄▃▂▁▁▁
wandb:                    train_kappa ▁▂▂▃▄▅▆▇████
wandb:            trainer/global_step ▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇█▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▃▂▅▆█▄▅██▆▇
wandb:                    val_ce_loss ▁▁▁▁▂▂▆▆█▇▇▇
wandb:                      val_kappa ▁▃▁▄▅▇▄▅▇█▆▇
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.39633
wandb:                  test_acc_step 0.42857
wandb:             test_ce_loss_epoch 2.31017
wandb:              test_ce_loss_step 2.38791
wandb:               test_kappa_epoch 0.15409
wandb:                test_kappa_step 0.13869
wandb:                      train_acc 0.99963
wandb:                  train_ce_loss 0.0135
wandb:                    train_kappa 0.99944
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.38481
wandb:                    val_ce_loss 2.36123
wandb:                      val_kappa 0.13204
wandb: 
wandb: 🚀 View run comic-sweep-23 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/h6dwz4j2
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_214920-h6dwz4j2/logs
wandb: Agent Starting Run: u8kc1hjg with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 6.77754411691963e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.5269983339321772e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_215526-u8kc1hjg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-24
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/u8kc1hjg
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.112
Metric val_ce_loss improved by 0.007 >= min_delta = 0.0001. New best score: 1.105
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.105. Signaling Trainer to stop.
Epoch 11/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:26 •       14.49it/s v_num: 1hjg      
                                     0:00:00                   val_ce_loss:     
                                                               2.239 val_kappa: 
                                                               0.180 val_acc:   
                                                               0.424            
                                                               train_ce_loss:   
                                                               0.088            
                                                               train_kappa:     
                                                               0.969 train_acc: 
                                                               0.973            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.4216666519641876     │
│    test_ce_loss_epoch     │    2.2660534381866455     │
│     test_kappa_epoch      │    0.20585542917251587    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:02 • 0:00:00 18.45it/s 
wandb: uploading config.yaml
wandb: uploading history steps 110-159, summary, console lines 42-62
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ██▇▇▇▇▇▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▃▄▅▃▃▇▅▄▄▅▂▃▅▄▄▆▄▁▃▂▂▂▃▂▄▄▂▄▂▃▂▅▄▁▅▃█▇▄▂
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▆▂▅▆▅▁▂▄▄▄█▅▃▄▆▄▆▄▃▆█▇▄▆▃▄▄▄▇▇▅▃▇▆▄▅▁▂▂▅
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▃▅▄▄▄▆▅▅▅▄▄▅▄▄▅▅▄▂▄▃▃▃▂▃▅▅▄▅▂▄▃▄▄▁▃▄█▆▃▂
wandb:                      train_acc ▁▂▂▂▃▃▄▅▆▇██
wandb:                  train_ce_loss ████▇▇▆▅▃▂▁▁
wandb:                    train_kappa ▁▂▂▂▃▄▅▆▇▇██
wandb:            trainer/global_step ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇███▁▁▁▁▁▁▁
wandb:                        val_acc ▁▁▁▂▂▂▃▂▅▄▇█
wandb:                    val_ce_loss ▁▁▁▁▁▄▄▇▃█▅▆
wandb:                      val_kappa ▁▁▂▂▃▂▂▂▆▅█▇
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.42167
wandb:                  test_acc_step 0.375
wandb:             test_ce_loss_epoch 2.26605
wandb:              test_ce_loss_step 2.44606
wandb:               test_kappa_epoch 0.20586
wandb:                test_kappa_step 0.01145
wandb:                      train_acc 0.97276
wandb:                  train_ce_loss 0.08789
wandb:                    train_kappa 0.96882
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.42407
wandb:                    val_ce_loss 2.23886
wandb:                      val_kappa 0.17956
wandb: 
wandb: 🚀 View run easy-sweep-24 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/u8kc1hjg
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_215526-u8kc1hjg/logs
wandb: Agent Starting Run: cq60h2w5 with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 7.012535728784849e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.623693491129549e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_220133-cq60h2w5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-25
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/cq60h2w5
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.103
Metric val_ce_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.102
Metric val_ce_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.101
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.101. Signaling Trainer to stop.
Epoch 18/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:25 •       15.09it/s v_num: h2w5      
                                     0:00:00                   val_ce_loss:     
                                                               4.361 val_kappa: 
                                                               0.152 val_acc:   
                                                               0.417            
                                                               train_ce_loss:   
                                                               0.047            
                                                               train_kappa:     
                                                               0.984 train_acc: 
                                                               0.985            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.42233332991600037    │
│    test_ce_loss_epoch     │     4.306259632110596     │
│     test_kappa_epoch      │    0.15314191579818726    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:02 • 0:00:00 15.68it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███
wandb: exponential_decay_lr_scheduler █████▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▄▄▄▄▄▄▄▃▃▃▃▂▂▂▁▁▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▆▄▃▅▆▆▇▅▃▇▄▅▄▃▅▁▂▆█▅▅█▅▁▇▇▄▅▄▅█▆▅▆█▄▇▆▆▅
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▃▆▅▄▄▂▄▂▆▄▄▅▆▄▃█▇▃▂▄▃▂▅█▅▃▄▃▃▅▂▅▅▄▁▅▂▄▂▂
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▅▄▄█▆▆▆▄▄▇▄▄▄▁▄▂▂▇▅▆▄▇▁▁█▇▅▄▃▄█▆▄▄▄▅▇▆▄▇
wandb:                      train_acc ▁▁▁▂▂▂▃▃▄▅▅▆▇▇▇████
wandb:                  train_ce_loss ██████▇▇▆▆▅▄▃▂▂▁▁▁▁
wandb:                    train_kappa ▁▁▂▂▂▃▃▄▅▅▆▇▇▇█████
wandb:            trainer/global_step ▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▁▁▁▁▁▁█
wandb:                        val_acc ▁▂▁▂▁▁▄▁▇▆▂▂▄█▅█▂▆▅
wandb:                    val_ce_loss ▁▁▁▁▁▂▁▂▁▂▅▄▄▂▃▂█▃▇
wandb:                      val_kappa ▁▂▂▂▂▁▃▁▇▅▂▂▄▇▅█▃▆▄
wandb: 
wandb: Run summary:
wandb:                          epoch 19
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.42233
wandb:                  test_acc_step 0.42857
wandb:             test_ce_loss_epoch 4.30626
wandb:              test_ce_loss_step 3.48945
wandb:               test_kappa_epoch 0.15314
wandb:                test_kappa_step 0.24684
wandb:                      train_acc 0.98535
wandb:                  train_ce_loss 0.04722
wandb:                    train_kappa 0.98364
wandb:            trainer/global_step 3610
wandb:                        val_acc 0.41704
wandb:                    val_ce_loss 4.36127
wandb:                      val_kappa 0.1525
wandb: 
wandb: 🚀 View run decent-sweep-25 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/cq60h2w5
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_220133-cq60h2w5/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: dkedlgey with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.0001432595983100201
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 4.094795798177227e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_221123-dkedlgey
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-26
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/dkedlgey
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.136
Metric val_ce_loss improved by 0.035 >= min_delta = 0.0001. New best score: 1.101
Metric val_ce_loss improved by 0.003 >= min_delta = 0.0001. New best score: 1.099
Metric val_ce_loss improved by 0.058 >= min_delta = 0.0001. New best score: 1.041
Metric val_ce_loss improved by 0.070 >= min_delta = 0.0001. New best score: 0.971
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 0.971. Signaling Trainer to stop.
Epoch 21/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:23 •       16.88it/s v_num: lgey      
                                     0:00:00                   val_ce_loss:     
                                                               1.756 val_kappa: 
                                                               0.525 val_acc:   
                                                               0.603            
                                                               train_ce_loss:   
                                                               0.079            
                                                               train_kappa:     
                                                               0.970 train_acc: 
                                                               0.972            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.5996666550636292     │
│    test_ce_loss_epoch     │     1.757861614227295     │
│     test_kappa_epoch      │    0.5197272300720215     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 14.03it/s 
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██
wandb: exponential_decay_lr_scheduler █████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▅▂▂▆▅▅▂▁▄▄▄▂▂▂▄▁▁▄█▅▄▅▃▅▅▃▂▄▄▄▄▅▃▅▄▄▄▄▅▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▄█▆▂▅▄▇▇▄▅▄▇▆▇▄▇█▄▁▃▄▃▆▄▅▅▆▅▅▅▅▄▆▃▄▃▅▄▃▃
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▅▁▃▇▄▄▃▃▅▆▅▄▃▁▅▁▂▄█▇▃▇▁▃▅▄▃▆▄▄▄▅▂▅▅▄▄▄▅▅
wandb:                      train_acc ▁▁▁▂▂▂▂▃▃▄▄▅▅▆▇▇▇▇████
wandb:                  train_ce_loss ███████▇▇▆▆▅▄▄▃▃▂▂▁▁▁▁
wandb:                    train_kappa ▁▁▁▂▂▂▃▃▄▅▅▆▆▇▇▇██████
wandb:            trainer/global_step ▁▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇███▁▁▁▁▁▁
wandb:                        val_acc ▁▁▁▂▂▁▂▅▃▁▅▇▃▃▇▇▄▄▃▃██
wandb:                    val_ce_loss ▁▁▁▁▁▂▁▁▁█▂▁▃▃▂▂▅▄▅▇▃▃
wandb:                      val_kappa ▁▁▁▂▂▁▂▅▄▁▆▇▃▄█▇▄▄▃▂██
wandb: 
wandb: Run summary:
wandb:                          epoch 22
wandb: exponential_decay_lr_scheduler 0.00012
wandb:                 test_acc_epoch 0.59967
wandb:                  test_acc_step 0.60714
wandb:             test_ce_loss_epoch 1.75786
wandb:              test_ce_loss_step 1.26203
wandb:               test_kappa_epoch 0.51973
wandb:                test_kappa_step 0.59018
wandb:                      train_acc 0.97235
wandb:                  train_ce_loss 0.079
wandb:                    train_kappa 0.97049
wandb:            trainer/global_step 2090
wandb:                        val_acc 0.60259
wandb:                    val_ce_loss 1.75577
wandb:                      val_kappa 0.52517
wandb: 
wandb: 🚀 View run stellar-sweep-26 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/dkedlgey
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_221123-dkedlgey/logs
wandb: Agent Starting Run: l68b17hi with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.0001295646275903409
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 6.753310321054957e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_222206-l68b17hi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-27
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/l68b17hi
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.112
Metric val_ce_loss improved by 0.010 >= min_delta = 0.0001. New best score: 1.102
Metric val_ce_loss improved by 0.008 >= min_delta = 0.0001. New best score: 1.094
Metric val_ce_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.093
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.093. Signaling Trainer to stop.
Epoch 20/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:23 •       16.45it/s v_num: 17hi      
                                     0:00:00                   val_ce_loss:     
                                                               2.711 val_kappa: 
                                                               0.407 val_acc:   
                                                               0.529            
                                                               train_ce_loss:   
                                                               0.046            
                                                               train_kappa:     
                                                               0.982 train_acc: 
                                                               0.985            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.5323333144187927     │
│    test_ce_loss_epoch     │    2.6432738304138184     │
│     test_kappa_epoch      │    0.41815441846847534    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 13.29it/s 
wandb: uploading output.log; uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb: exponential_decay_lr_scheduler █████▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▆▄▆▄▅▆█▆▂▅▆▅▅▁▆▅▃▃█▆▇▆▅▃▆▆▃▅▃▅█▅▆▆▇▃▇█▄▅
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▄▅▄▅▆▃▃▄█▄▄▃▅▆▃▄▆▅▂▃▅▄▄▇▅▁▆▃▆▃▄▅▄▃▂▆▁▃▄▄
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▇▅▆▇▆▇▇▇▅▇▇▆▅▁▆▆▅▅█▇▅▇▄▅▇▇▅█▄▅█▇█▇▇▅▇█▄█
wandb:                      train_acc ▁▁▁▂▂▂▂▃▄▄▅▅▆▇▇▇▇████
wandb:                  train_ce_loss ██████▇▇▆▆▅▅▄▃▂▂▂▁▁▁▁
wandb:                    train_kappa ▁▁▁▂▂▃▃▄▅▅▆▆▇▇▇▇█████
wandb:            trainer/global_step ▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████▁▁▁▁▁
wandb:                        val_acc ▁▁▂▁▂▂▂▁▃▃▆▂▃▄▃▇▇▅█▄▇
wandb:                    val_ce_loss ▁▁▁▁▁▁▂█▂▂▁▅▃▅▆▄▃▇▃▆▆
wandb:                      val_kappa ▁▂▂▁▂▁▂▁▃▄▆▂▃▄▃▆▇▆█▄▇
wandb: 
wandb: Run summary:
wandb:                          epoch 21
wandb: exponential_decay_lr_scheduler 0.00011
wandb:                 test_acc_epoch 0.53233
wandb:                  test_acc_step 0.53571
wandb:             test_ce_loss_epoch 2.64327
wandb:              test_ce_loss_step 2.51616
wandb:               test_kappa_epoch 0.41815
wandb:                test_kappa_step 0.55068
wandb:                      train_acc 0.98461
wandb:                  train_ce_loss 0.04593
wandb:                    train_kappa 0.98226
wandb:            trainer/global_step 1995
wandb:                        val_acc 0.52852
wandb:                    val_ce_loss 2.71132
wandb:                      val_kappa 0.40708
wandb: 
wandb: 🚀 View run usual-sweep-27 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/l68b17hi
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_222206-l68b17hi/logs
wandb: Agent Starting Run: v0lb0ezs with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 9.014620927120994e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 1.2112530011813313e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_223213-v0lb0ezs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-28
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/v0lb0ezs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.109
Metric val_ce_loss improved by 0.104 >= min_delta = 0.0001. New best score: 1.005
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.005. Signaling Trainer to stop.
Epoch 16/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:26 •       14.38it/s v_num: 0ezs      
                                     0:00:00                   val_ce_loss:     
                                                               7.600 val_kappa: 
                                                               0.022 val_acc:   
                                                               0.344            
                                                               train_ce_loss:   
                                                               0.134            
                                                               train_kappa:     
                                                               0.948 train_acc: 
                                                               0.950            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.3466666638851166     │
│    test_ce_loss_epoch     │     7.626334190368652     │
│     test_kappa_epoch      │   0.028235316276550293    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:02 • 0:00:00 16.25it/s 
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███
wandb: exponential_decay_lr_scheduler █████▇▇▇▇▇▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▁▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▆▅▅▇▇▃▃▅█▅▆▄▄▆▃▆▄▅▃▅▄▃▄▇▆▄▅▅▄▄▃▅▅▅▄▇▁▄▄▅
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▄▄▂▁▃▆▇▄▂▅▂▅▆▂▆▁▅▃▆▆▆█▆▂▁▄▅▃▆▅▇▃▄▃█▁█▄▅▃
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▆▅▆█▃▇▄▄▄▅▆▇▃▆▅▃▇▅█▆█▃▇▄▆▅▄▃▅▅▆▇█▃▆▆▆▁▂▆
wandb:                      train_acc ▁▁▁▂▂▃▃▄▄▅▆▆▇▇███
wandb:                  train_ce_loss █████▇▇▆▆▅▄▄▃▂▂▁▁
wandb:                    train_kappa ▁▁▁▂▃▃▄▄▅▅▆▆▇▇███
wandb:            trainer/global_step ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇███▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▂▁▁▂▃▁▆▂▂▂▅█▃▂█▄▁
wandb:                    val_ce_loss ▁▁▁▁▁▃▁▂▃▃▁▁▃▄▂▄█
wandb:                      val_kappa ▂▁▂▂▂▁▆▂▂▃▆█▃▃▇▅▂
wandb: 
wandb: Run summary:
wandb:                          epoch 17
wandb: exponential_decay_lr_scheduler 8e-05
wandb:                 test_acc_epoch 0.34667
wandb:                  test_acc_step 0.35714
wandb:             test_ce_loss_epoch 7.62633
wandb:              test_ce_loss_step 6.90854
wandb:               test_kappa_epoch 0.02824
wandb:                test_kappa_step 0.04906
wandb:                      train_acc 0.95045
wandb:                  train_ce_loss 0.13437
wandb:                    train_kappa 0.94843
wandb:            trainer/global_step 3230
wandb:                        val_acc 0.3437
wandb:                    val_ce_loss 7.60019
wandb:                      val_kappa 0.02185
wandb: 
wandb: 🚀 View run vibrant-sweep-28 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/v0lb0ezs
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_223213-v0lb0ezs/logs
wandb: Agent Starting Run: tdy1z6po with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.00012455695319139057
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.5111629887345276e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_224034-tdy1z6po
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-29
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/tdy1z6po
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.106
Metric val_ce_loss improved by 0.020 >= min_delta = 0.0001. New best score: 1.086
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.086. Signaling Trainer to stop.
Epoch 18/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:28 •       13.53it/s v_num: z6po      
                                     0:00:00                   val_ce_loss:     
                                                               1.953 val_kappa: 
                                                               0.369 val_acc:   
                                                               0.499            
                                                               train_ce_loss:   
                                                               0.111            
                                                               train_kappa:     
                                                               0.957 train_acc: 
                                                               0.961            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │     0.515333354473114     │
│    test_ce_loss_epoch     │     1.848555326461792     │
│     test_kappa_epoch      │    0.3839162588119507     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 13.80it/s 
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml; uploading history steps 176-229, summary, console lines 42-62
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███
wandb: exponential_decay_lr_scheduler █████▇▇▇▇▇▆▆▆▆▆▆▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▇▃▅▅▃▄▅▁▅▃▃▃▄▃▃▄▃▇▅▃▃▄▄▄▅▅▃▅▄▁▂▅▁█▃▅▃▆▅▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▁▄▃▂▆▄▄▆▁▆▆█▃▇▄▄▅▁▅▄▇▅▄▆▃▃▆▂▆██▂▇▁▅▂▃▃▃▅
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step █▆▆▆▆▅▆▁▆▄▃▆▇▃▅▅▄▇▇▄▅▆▄▅▇▇▅▆▄▂▄▇▃▇▅▆▆▇▆▆
wandb:                      train_acc ▁▁▁▂▂▂▃▃▄▄▅▅▆▆▇▇███
wandb:                  train_ce_loss ██████▇▇▆▆▅▅▄▄▃▂▂▂▁
wandb:                    train_kappa ▁▁▁▂▂▂▃▄▄▅▅▆▆▇▇▇███
wandb:            trainer/global_step ▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇████▁▁▁▁▁▁▁
wandb:                        val_acc ▂▁▂▂▂▁▃▃▇▃▂▃▆▁▄▂▆▄█
wandb:                    val_ce_loss ▁▁▁▁▁▁▁▁▁▂▃▄▁█▃▆▂▅▂
wandb:                      val_kappa ▂▁▂▁▁▁▄▃▇▃▂▂▇▁▄▂▇▃█
wandb: 
wandb: Run summary:
wandb:                          epoch 19
wandb: exponential_decay_lr_scheduler 0.0001
wandb:                 test_acc_epoch 0.51533
wandb:                  test_acc_step 0.53571
wandb:             test_ce_loss_epoch 1.84856
wandb:              test_ce_loss_step 1.90097
wandb:               test_kappa_epoch 0.38392
wandb:                test_kappa_step 0.45495
wandb:                      train_acc 0.96062
wandb:                  train_ce_loss 0.1108
wandb:                    train_kappa 0.95744
wandb:            trainer/global_step 1805
wandb:                        val_acc 0.49889
wandb:                    val_ce_loss 1.95294
wandb:                      val_kappa 0.3688
wandb: 
wandb: 🚀 View run revived-sweep-29 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/tdy1z6po
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_224034-tdy1z6po/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: omgu68q9 with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 9.371429241367074e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.651165348950609e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_225004-omgu68q9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-30
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/omgu68q9
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.111
Metric val_ce_loss improved by 0.008 >= min_delta = 0.0001. New best score: 1.103
Metric val_ce_loss improved by 0.002 >= min_delta = 0.0001. New best score: 1.101
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.101. Signaling Trainer to stop.
Epoch 13/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:26 •       14.71it/s v_num: 68q9      
                                     0:00:00                   val_ce_loss:     
                                                               2.165 val_kappa: 
                                                               0.289 val_acc:   
                                                               0.474            
                                                               train_ce_loss:   
                                                               0.108            
                                                               train_kappa:     
                                                               0.960 train_acc: 
                                                               0.964            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.46566668152809143    │
│    test_ce_loss_epoch     │     2.214128017425537     │
│     test_kappa_epoch      │    0.27945882081985474    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:02 • 0:00:00 16.78it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇█
wandb: exponential_decay_lr_scheduler ██▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▇▃▁▃▅▇▆▂▅█▂▅▂▅▁▂▄▃▆▅▅▅█▃▅▇▂▅▅▃▇▃▃▅▆▄▂▇▆▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▁▆█▄▆▂▄▇▄▃▅▆█▅█▆▆▃▃▆▄▄▃▇▃▂▆▄▄▅▄▆▆▃▄▄▇▁▃▅
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▇▅▁▁▅▇▆▅▃█▄▄▄▄▂▃▃▄▅▅▄▆▇▄▅▅▅▆▄▄▇▅▄▄▅▄▅▆▅▅
wandb:                      train_acc ▁▁▂▂▂▃▃▄▅▆▆▇██
wandb:                  train_ce_loss █████▇▇▆▅▄▃▂▁▁
wandb:                    train_kappa ▁▁▂▂▃▃▄▅▅▆▇▇██
wandb:            trainer/global_step ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▆▆▆▆▆▆▇███▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▁▂▄▄▂▂▂▇█▃▇▇█
wandb:                    val_ce_loss ▁▁▁▁▁▂▃▅▂▂█▄▅▅
wandb:                      val_kappa ▁▁▂▄▅▁▂▂▇█▂▇██
wandb: 
wandb: Run summary:
wandb:                          epoch 14
wandb: exponential_decay_lr_scheduler 8e-05
wandb:                 test_acc_epoch 0.46567
wandb:                  test_acc_step 0.42857
wandb:             test_ce_loss_epoch 2.21413
wandb:              test_ce_loss_step 2.32682
wandb:               test_kappa_epoch 0.27946
wandb:                test_kappa_step 0.27407
wandb:                      train_acc 0.96383
wandb:                  train_ce_loss 0.10832
wandb:                    train_kappa 0.95998
wandb:            trainer/global_step 1330
wandb:                        val_acc 0.47444
wandb:                    val_ce_loss 2.16507
wandb:                      val_kappa 0.28863
wandb: 
wandb: 🚀 View run curious-sweep-30 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/omgu68q9
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_225004-omgu68q9/logs
wandb: Agent Starting Run: ancajt8k with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 0.00017261862351525368
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 6.692975125166259e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_225700-ancajt8k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-31
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ancajt8k
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.102
Metric val_ce_loss improved by 0.004 >= min_delta = 0.0001. New best score: 1.099
Metric val_ce_loss improved by 0.026 >= min_delta = 0.0001. New best score: 1.073
Metric val_ce_loss improved by 0.209 >= min_delta = 0.0001. New best score: 0.864
Metric val_ce_loss improved by 0.095 >= min_delta = 0.0001. New best score: 0.768
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 0.768. Signaling Trainer to stop.
Epoch 27/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:25 •       14.97it/s v_num: jt8k      
                                     0:00:00                   val_ce_loss:     
                                                               3.705 val_kappa: 
                                                               0.358 val_acc:   
                                                               0.546            
                                                               train_ce_loss:   
                                                               0.120            
                                                               train_kappa:     
                                                               0.953 train_acc: 
                                                               0.954            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.5556666851043701     │
│    test_ce_loss_epoch     │    3.5261361598968506     │
│     test_kappa_epoch      │    0.3704336881637573     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 14.81it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██
wandb: exponential_decay_lr_scheduler █████▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▇▂▂▇▄▇▇▃▂▆▅▅▁▅▅▃▃▅█▅▆▇▃▆▇▅▃▇▃▅▇▅▆▅█▄█▆▇▇
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▂█▃▅▃▄▂▅▅▃▄▅▇▄▅▆▅▄▁▂▃▂▅▃▃▂▆▂▅▄▂▄▂▃▁▅▂▃▁▂
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▆▃▅▇▅▆▅▃▅▆▅▄▂▃▄▄▃▆▄▅▄▆▁▅█▅▃▇▄▅▅▅▅▅▅▄▅▄▅█
wandb:                      train_acc ▁▁▁▁▁▂▂▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇█████
wandb:                  train_ce_loss ██████▇▇▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁▁▁
wandb:                    train_kappa ▁▁▁▁▂▂▃▃▄▅▅▅▆▆▆▆▇▇▇▇▇███████
wandb:            trainer/global_step ▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇████▁▁▁▁▁
wandb:                        val_acc ▁▁▁▁▁▂▁▁▁▁▁▆▆▁▅▃▂▇▅▅▆▅▅▅▄█▆▅
wandb:                    val_ce_loss ▁▁▁▂▁▁▂█▂▄▆▁▁▆▂▂▅▁▂▂▂▃▂▃▅▂▂▅
wandb:                      val_kappa ▁▁▁▁▁▃▁▁▁▁▁▆▇▁▅▃▂▇▅▅▇▅▆▄▄█▅▅
wandb: 
wandb: Run summary:
wandb:                          epoch 28
wandb: exponential_decay_lr_scheduler 0.00013
wandb:                 test_acc_epoch 0.55567
wandb:                  test_acc_step 0.625
wandb:             test_ce_loss_epoch 3.52614
wandb:              test_ce_loss_step 2.95937
wandb:               test_kappa_epoch 0.37043
wandb:                test_kappa_step 0.61787
wandb:                      train_acc 0.9542
wandb:                  train_ce_loss 0.11977
wandb:                    train_kappa 0.95347
wandb:            trainer/global_step 5320
wandb:                        val_acc 0.5463
wandb:                    val_ce_loss 3.70516
wandb:                      val_kappa 0.35792
wandb: 
wandb: 🚀 View run dainty-sweep-31 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ancajt8k
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_225700-ancajt8k/logs
wandb: Agent Starting Run: 8jo3udht with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 8.138570702092895e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 4.007040775365062e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_231046-8jo3udht
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-32
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/8jo3udht
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.120
Metric val_ce_loss improved by 0.010 >= min_delta = 0.0001. New best score: 1.110
Metric val_ce_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.109
Metric val_ce_loss improved by 0.004 >= min_delta = 0.0001. New best score: 1.105
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.105. Signaling Trainer to stop.
Epoch 13/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:28 •       13.85it/s v_num: udht      
                                     0:00:00                   val_ce_loss:     
                                                               2.050 val_kappa: 
                                                               0.369 val_acc:   
                                                               0.490            
                                                               train_ce_loss:   
                                                               0.369            
                                                               train_kappa:     
                                                               0.840 train_acc: 
                                                               0.853            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.5016666650772095     │
│    test_ce_loss_epoch     │    1.9842861890792847     │
│     test_kappa_epoch      │    0.3948584198951721     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 12.87it/s 
wandb: uploading output.log; uploading config.yaml
wandb: uploading history steps 132-181, summary, console lines 44-64
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇█
wandb: exponential_decay_lr_scheduler ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▇▅▂▄▂▅▇▃▂▆▁▇▅▁▇▃▄▃▆▅▆▆▄▂▅█▂▃▃▅▆▅▃▃▇▁▇▃▂▇
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▄▅▅▅▇▄▃▇█▅▇▃▄▇▃▇▅▅▂▅▅▃▄▆▇▄▆▆▄▄▄▅▅▅▃█▁▅▅▃
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▇▅▁▅▄▄▅▄▅▇▃▆▄▂▆▄▃▄▄▅▅▄▂▄▆█▂▅▂▄▅▅▃▃▆▃▄▂▂▇
wandb:                      train_acc ▁▁▁▂▂▂▃▃▄▅▆▆▇█
wandb:                  train_ce_loss ██████▇▇▆▅▄▃▂▁
wandb:                    train_kappa ▁▁▁▂▂▃▃▄▅▅▆▇▇█
wandb:            trainer/global_step ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇███▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▂▂▁▂▁▁▃▃▄▂▃▃█
wandb:                    val_ce_loss ▁▁▁▁▁▂▄▁▄▃▅█▆▃
wandb:                      val_kappa ▁▂▁▁▂▁▂▃▃▃▁▃▃█
wandb: 
wandb: Run summary:
wandb:                          epoch 14
wandb: exponential_decay_lr_scheduler 7e-05
wandb:                 test_acc_epoch 0.50167
wandb:                  test_acc_step 0.58929
wandb:             test_ce_loss_epoch 1.98429
wandb:              test_ce_loss_step 1.67481
wandb:               test_kappa_epoch 0.39486
wandb:                test_kappa_step 0.61326
wandb:                      train_acc 0.85342
wandb:                  train_ce_loss 0.36887
wandb:                    train_kappa 0.83976
wandb:            trainer/global_step 2660
wandb:                        val_acc 0.49037
wandb:                    val_ce_loss 2.05014
wandb:                      val_kappa 0.36928
wandb: 
wandb: 🚀 View run decent-sweep-32 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/8jo3udht
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_231046-8jo3udht/logs
wandb: Agent Starting Run: ynigfmpd with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.00017186391081894798
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.941812724686396e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_231750-ynigfmpd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-33
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ynigfmpd
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.110
Metric val_ce_loss improved by 0.002 >= min_delta = 0.0001. New best score: 1.108
Metric val_ce_loss improved by 0.045 >= min_delta = 0.0001. New best score: 1.063
Metric val_ce_loss improved by 0.012 >= min_delta = 0.0001. New best score: 1.051
Metric val_ce_loss improved by 0.177 >= min_delta = 0.0001. New best score: 0.874
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 0.874. Signaling Trainer to stop.
Epoch 22/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:23 •       16.84it/s v_num: fmpd      
                                     0:00:00                   val_ce_loss:     
                                                               2.188 val_kappa: 
                                                               0.459 val_acc:   
                                                               0.561            
                                                               train_ce_loss:   
                                                               0.162            
                                                               train_kappa:     
                                                               0.931 train_acc: 
                                                               0.938            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.5596666932106018     │
│    test_ce_loss_epoch     │    2.1479318141937256     │
│     test_kappa_epoch      │    0.4551404118537903     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 15.35it/s 
wandb: uploading output.log; uploading config.yaml
wandb: uploading output.log
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██
wandb: exponential_decay_lr_scheduler █████▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▆▂▃▅▁▇▇▃▃▅▄▅▂▂▅▁▂▅▄▄▅▇▂▅▃▅▂▄▄▄▅▄▃▇▅▂█▆▄▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▃▆▄▃█▁▃█▆▄▄▅▇█▃▇█▆▂▅▄▂█▅▅▂▆▄▆▆▄▃▅▁▅▆▂▃▄▃
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▇▃▃▅▃▇▆▄▅▆▅▅▁▂▅▃▁▅▄▄▄█▁▄▅▅▂▅▄▄▄▅▃▅▄▅▆▅▂▅
wandb:                      train_acc ▁▁▁▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇███
wandb:                  train_ce_loss █████▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▁▁▁
wandb:                    train_kappa ▁▁▁▂▂▃▄▄▅▅▅▆▆▆▇▇▇▇▇████
wandb:            trainer/global_step ▁▂▂▃▃▃▄▄▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇███▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▁▁▁▂▄▁▄▅▂▃▂██▂▁▂▆▄▅▁▆▇
wandb:                    val_ce_loss ▁▁▁▁▁▁▂▁▁▂▁▃▁▁▄▇▄▁▂▂█▁▂
wandb:                      val_kappa ▁▁▁▁▁▄▁▄▅▂▄▂██▂▁▂▇▄▄▁▆▇
wandb: 
wandb: Run summary:
wandb:                          epoch 23
wandb: exponential_decay_lr_scheduler 0.00014
wandb:                 test_acc_epoch 0.55967
wandb:                  test_acc_step 0.55357
wandb:             test_ce_loss_epoch 2.14793
wandb:              test_ce_loss_step 1.74514
wandb:               test_kappa_epoch 0.45514
wandb:                test_kappa_step 0.49708
wandb:                      train_acc 0.93815
wandb:                  train_ce_loss 0.16229
wandb:                    train_kappa 0.93071
wandb:            trainer/global_step 2185
wandb:                        val_acc 0.56111
wandb:                    val_ce_loss 2.18787
wandb:                      val_kappa 0.4589
wandb: 
wandb: 🚀 View run olive-sweep-33 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ynigfmpd
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_231750-ynigfmpd/logs
wandb: Agent Starting Run: m61m7zjr with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.00013276022787327656
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.551124074386431e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_232849-m61m7zjr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-34
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/m61m7zjr
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.122
Metric val_ce_loss improved by 0.011 >= min_delta = 0.0001. New best score: 1.112
Metric val_ce_loss improved by 0.006 >= min_delta = 0.0001. New best score: 1.105
Metric val_ce_loss improved by 0.009 >= min_delta = 0.0001. New best score: 1.096
Metric val_ce_loss improved by 0.023 >= min_delta = 0.0001. New best score: 1.073
Metric val_ce_loss improved by 0.051 >= min_delta = 0.0001. New best score: 1.022
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.022. Signaling Trainer to stop.
Epoch 21/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:24 •       16.22it/s v_num: 7zjr      
                                     0:00:00                   val_ce_loss:     
                                                               4.503 val_kappa: 
                                                               0.138 val_acc:   
                                                               0.390            
                                                               train_ce_loss:   
                                                               0.088            
                                                               train_kappa:     
                                                               0.964 train_acc: 
                                                               0.968            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.40166667103767395    │
│    test_ce_loss_epoch     │     4.426739692687988     │
│     test_kappa_epoch      │    0.1524486541748047     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 15.15it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██
wandb: exponential_decay_lr_scheduler █████▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▆▄▅▇▆▃▃▅█▅▆▅▃▅▃▇▅▆▂▆▅▁▆▇▇▄▅▇▄▃▃▅▆▅▃▇▁▅▅▇
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▄▃▂▃▄▄▆▄▁▄▄▄▅▅▄▁▃▂▆▅▅█▅▃▁▄▄▂▃▅▆▄▃▂▅▂█▂▄▂
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▅▃▁▇▂▅▄▂▃▅▃▆▄▂▅▇▆▆▄▆▄▃▆▄▅▅▄▆▃▄▄▅▆▅▅▄▅▄▅█
wandb:                      train_acc ▁▁▁▂▂▂▃▃▄▄▅▅▆▆▇▇▇█████
wandb:                  train_ce_loss ██████▇▇▇▆▅▅▄▄▃▂▂▂▁▁▁▁
wandb:                    train_kappa ▁▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇██████
wandb:            trainer/global_step ▁▁▁▁▂▂▂▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▂▂▂▂▃▃▂▃▃▆▇▇▇▃▂█▃█▆▇▃
wandb:                    val_ce_loss ▁▁▁▁▁▁▁▂▂▄▁▁▁▁▄█▂▆▂▄▄▇
wandb:                      val_kappa ▁▁▁▂▂▃▃▂▃▂▆▇▇▇▂▂█▃█▅▆▃
wandb: 
wandb: Run summary:
wandb:                          epoch 22
wandb: exponential_decay_lr_scheduler 0.00011
wandb:                 test_acc_epoch 0.40167
wandb:                  test_acc_step 0.48214
wandb:             test_ce_loss_epoch 4.42674
wandb:              test_ce_loss_step 3.76557
wandb:               test_kappa_epoch 0.15245
wandb:                test_kappa_step 0.37442
wandb:                      train_acc 0.96794
wandb:                  train_ce_loss 0.08841
wandb:                    train_kappa 0.96368
wandb:            trainer/global_step 2090
wandb:                        val_acc 0.39037
wandb:                    val_ce_loss 4.50342
wandb:                      val_kappa 0.13823
wandb: 
wandb: 🚀 View run glowing-sweep-34 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/m61m7zjr
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_232849-m61m7zjr/logs
wandb: Agent Starting Run: o8zm1mbb with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.00014768231449264118
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.909963570042533e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_233944-o8zm1mbb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-35
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/o8zm1mbb
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.105
Metric val_ce_loss improved by 0.002 >= min_delta = 0.0001. New best score: 1.103
Metric val_ce_loss improved by 0.006 >= min_delta = 0.0001. New best score: 1.097
Metric val_ce_loss improved by 0.028 >= min_delta = 0.0001. New best score: 1.069
Metric val_ce_loss improved by 0.050 >= min_delta = 0.0001. New best score: 1.018
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.018. Signaling Trainer to stop.
Epoch 23/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:24 •       16.10it/s v_num: 1mbb      
                                     0:00:00                   val_ce_loss:     
                                                               3.343 val_kappa: 
                                                               0.268 val_acc:   
                                                               0.437            
                                                               train_ce_loss:   
                                                               0.100            
                                                               train_kappa:     
                                                               0.961 train_acc: 
                                                               0.964            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.44066667556762695    │
│    test_ce_loss_epoch     │    3.3470253944396973     │
│     test_kappa_epoch      │    0.25914859771728516    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 15.15it/s 
wandb: uploading history steps 226-275, summary, console lines 45-65; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██
wandb: exponential_decay_lr_scheduler ████▇▆▆▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step █▆▅▆▆▂▃▃▆▄▆▃▃▆▃▆▃▅▄▃▂▃▄▃█▃▄▇▁▄▃▂▂▅▅▆▁▆▆▅
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▂▆▅▂▃▄█▇▂▃▃▆▇▄▄▂▇▆▄▆▆▆▆▆▁▄▇▂▇▆▆▄▅▄▃▄▆▂▃▅
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▆▆▆▅▁▃▆▂▁▅▅▄▄▃▆▅▄▅▅▃▄▅▃▃█▃▆▇▁▅▃▃▃▄▇▅▆▁▅▅
wandb:                      train_acc ▁▁▁▁▂▂▂▃▃▄▄▅▅▅▆▆▆▇▇▇▇███
wandb:                  train_ce_loss ██████▇▇▇▆▆▅▅▅▄▄▃▃▂▂▂▁▁▁
wandb:                    train_kappa ▁▁▁▁▂▂▃▃▄▅▅▅▆▆▆▇▇▇▇█████
wandb:            trainer/global_step ▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇█▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▁▁▁▂▂▃▁▁▁▄▂▃▂▆▁▄▃▂▃▆▆▂█▃
wandb:                    val_ce_loss ▁▁▁▁▁▁▅▂▂▁▃▃▅▁█▂▃█▃▂▂▅▁▃
wandb:                      val_kappa ▁▁▁▂▃▃▁▁▁▄▁▁▂▆▁▄▃▂▄▇▆▂█▄
wandb: 
wandb: Run summary:
wandb:                          epoch 24
wandb: exponential_decay_lr_scheduler 0.00012
wandb:                 test_acc_epoch 0.44067
wandb:                  test_acc_step 0.44643
wandb:             test_ce_loss_epoch 3.34703
wandb:              test_ce_loss_step 3.58185
wandb:               test_kappa_epoch 0.25915
wandb:                test_kappa_step 0.28313
wandb:                      train_acc 0.96416
wandb:                  train_ce_loss 0.09979
wandb:                    train_kappa 0.96122
wandb:            trainer/global_step 2280
wandb:                        val_acc 0.43667
wandb:                    val_ce_loss 3.34322
wandb:                      val_kappa 0.26808
wandb: 
wandb: 🚀 View run flowing-sweep-35 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/o8zm1mbb
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_233944-o8zm1mbb/logs
wandb: Agent Starting Run: 6eqdocvh with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 8.624288825036369e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.4550277851495488e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_235137-6eqdocvh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-36
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/6eqdocvh
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.109
Metric val_ce_loss improved by 0.007 >= min_delta = 0.0001. New best score: 1.102
Metric val_ce_loss improved by 0.004 >= min_delta = 0.0001. New best score: 1.098
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.098. Signaling Trainer to stop.
Epoch 14/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:25 •       15.25it/s v_num: ocvh      
                                     0:00:00                   val_ce_loss:     
                                                               2.260 val_kappa: 
                                                               0.242 val_acc:   
                                                               0.443            
                                                               train_ce_loss:   
                                                               0.040            
                                                               train_kappa:     
                                                               0.987 train_acc: 
                                                               0.990            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.4416666626930237     │
│    test_ce_loss_epoch     │     2.252209186553955     │
│     test_kappa_epoch      │    0.23715978860855103    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 14.37it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: exponential_decay_lr_scheduler █▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▅▄▅▂▃█▇▆▃▄▂▁▂▃▄▄█▆▅▂▅▄▃▅▆▇▁▆▅▂▂▆▅▅▇▂█▅▅▁
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▁▄▅▃▃▁▃▅▄▄▆▆▇▅▆▅▄▂▂▆▂▅▅▇▃▃▇▂▁▅█▃▃▂▄▅▂▂▃█
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▆▅▆▃▃▇▇▄▄▆▂▂▁▄▅▂▆▆▆▃▃▆▄▃█▇▂▅▅▂▄▆▅▃▆▄▇▆▄▄
wandb:                      train_acc ▁▁▂▂▂▃▄▄▅▆▇▇███
wandb:                  train_ce_loss ████▇▇▆▆▅▄▃▂▂▁▁
wandb:                    train_kappa ▁▁▂▂▃▃▄▅▆▇▇████
wandb:            trainer/global_step ▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▇▇▇███▁▁▁▁▁▁▁▁▁█
wandb:                        val_acc ▂▁▃▃▄▂▂▃▅▂▃▆▂██
wandb:                    val_ce_loss ▁▁▁▁▁▂▂▄▂▃▆▃█▃▃
wandb:                      val_kappa ▁▁▃▃▅▃▃▃▅▄▃▆▃██
wandb: 
wandb: Run summary:
wandb:                          epoch 15
wandb: exponential_decay_lr_scheduler 7e-05
wandb:                 test_acc_epoch 0.44167
wandb:                  test_acc_step 0.33929
wandb:             test_ce_loss_epoch 2.25221
wandb:              test_ce_loss_step 3.06761
wandb:               test_kappa_epoch 0.23716
wandb:                test_kappa_step 0.22814
wandb:                      train_acc 0.98951
wandb:                  train_ce_loss 0.04025
wandb:                    train_kappa 0.98704
wandb:            trainer/global_step 1425
wandb:                        val_acc 0.44259
wandb:                    val_ce_loss 2.26047
wandb:                      val_kappa 0.24168
wandb: 
wandb: 🚀 View run olive-sweep-36 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/6eqdocvh
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_235137-6eqdocvh/logs
wandb: Agent Starting Run: mgj9hvlr with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 7.067110079456035e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 6.464561264195618e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_235904-mgj9hvlr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-37
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/mgj9hvlr
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.109
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.109. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:23 •       16.52it/s v_num: hvlr      
                                     0:00:00                   val_ce_loss:     
                                                               2.305 val_kappa: 
                                                               0.121 val_acc:   
                                                               0.390            
                                                               train_ce_loss:   
                                                               0.589            
                                                               train_kappa:     
                                                               0.705 train_acc: 
                                                               0.750            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.3736666738986969     │
│    test_ce_loss_epoch     │    2.3307645320892334     │
│     test_kappa_epoch      │    0.11992263793945312    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 15.17it/s 
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ███▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▅▄▅▅▅▄▁▃█▇▆▅▂▅▃▆▅▄▄▅▃▁▃█▇▃▄▅▄▄▅▄▅▃▂▆▂▅▃▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▄▃▂▂▅▄█▅▂▄▄▆▃▃▅▁▃▃▄▄▅▇▇▂▂▄▆▃▅▃▅▅▄▄▆▁▆▂▆▄
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▇▃▃▅▃▅▁▃▃▇▅▆▂▅▅▂█▃▇▅▄▄▂▅▇▃▃▆▅▄▇▃▄▃▃▇▇▄▂▆
wandb:                      train_acc ▁▁▁▂▂▃▄▅▆▇█
wandb:                  train_ce_loss █████▇▆▅▄▃▁
wandb:                    train_kappa ▁▁▂▂▂▃▄▅▆▇█
wandb:            trainer/global_step ▁▂▂▂▂▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██▁▁▁▁▁▁▁▁▁▁▁
wandb:                        val_acc ▂▁▁▂▁▅▂▄▁█▄
wandb:                    val_ce_loss ▁▁▁▁▁▁▇▂█▂▆
wandb:                      val_kappa ▂▁▁▁▁▅▂▃▁█▄
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.37367
wandb:                  test_acc_step 0.375
wandb:             test_ce_loss_epoch 2.33076
wandb:              test_ce_loss_step 2.30493
wandb:               test_kappa_epoch 0.11992
wandb:                test_kappa_step 0.17883
wandb:                      train_acc 0.75016
wandb:                  train_ce_loss 0.58923
wandb:                    train_kappa 0.70531
wandb:            trainer/global_step 2090
wandb:                        val_acc 0.39037
wandb:                    val_ce_loss 2.30476
wandb:                      val_kappa 0.1207
wandb: 
wandb: 🚀 View run fine-sweep-37 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/mgj9hvlr
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_235904-mgj9hvlr/logs
wandb: Agent Starting Run: rqygzpaq with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.1851080740208175e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.524819387086987e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250326_000440-rqygzpaq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-38
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/rqygzpaq
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.109
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.109. Signaling Trainer to stop.
Epoch 10/59 ━━━━━━━━━━━━━━━━ 380/380 0:00:26 •       14.53it/s v_num: zpaq      
                                     0:00:00                   val_ce_loss:     
                                                               2.298 val_kappa: 
                                                               0.167 val_acc:   
                                                               0.394            
                                                               train_ce_loss:   
                                                               0.025            
                                                               train_kappa:     
                                                               0.997 train_acc: 
                                                               0.997            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │    0.38199999928474426    │
│    test_ce_loss_epoch     │     2.348148822784424     │
│     test_kappa_epoch      │     0.145008385181427     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 0:00:03 • 0:00:00 14.06it/s 
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█
wandb: exponential_decay_lr_scheduler ███▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁
wandb:                 test_acc_epoch ▁
wandb:                  test_acc_step ▆▃▅▆▆▃▅▅█▅▅▅▃▅▆▆▁▆▆▃▆▄▆▅▄▆▇█▅▄▅▅▅▄▄▆█▅▃▄
wandb:             test_ce_loss_epoch ▁
wandb:              test_ce_loss_step ▆▆▅▃▃▆▃▅▃▃▄▇█▆▃▄█▃▃▇▅█▃▇▄▅▃▂▅▅▅▅▆▇▆▁▄▆▃▄
wandb:               test_kappa_epoch ▁
wandb:                test_kappa_step ▆▄▂▄▃▃▃▃▆▆▆▃▃▅▆▃▁▅▆▅▅▃▃▃▅▄▆▆▅▅▅▆▄▃▂▄█▃▄▄
wandb:                      train_acc ▁▂▂▃▄▅▆▇███
wandb:                  train_ce_loss ███▇▇▅▄▃▂▁▁
wandb:                    train_kappa ▁▂▂▃▄▅▇▇███
wandb:            trainer/global_step ▁▂▂▂▂▂▂▂▂▃▃▄▄▄▄▅▆▆▆▆▇▇▇▇██▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:                        val_acc ▁▃▂▃▅▅▄▇█▇█
wandb:                    val_ce_loss ▁▁▂▁▂▃█▆▆▇▇
wandb:                      val_kappa ▁▃▁▄▄▆▃▆▇▇█
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.382
wandb:                  test_acc_step 0.35714
wandb:             test_ce_loss_epoch 2.34815
wandb:              test_ce_loss_step 2.14118
wandb:               test_kappa_epoch 0.14501
wandb:                test_kappa_step 0.07692
wandb:                      train_acc 0.99716
wandb:                  train_ce_loss 0.02537
wandb:                    train_kappa 0.99685
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.39407
wandb:                    val_ce_loss 2.29794
wandb:                      val_kappa 0.16667
wandb: 
wandb: 🚀 View run effortless-sweep-38 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/rqygzpaq
wandb: ⭐️ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250326_000440-rqygzpaq/logs
wandb: Agent Starting Run: qtqa8m3i with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.0001178793039621521
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 1.3314027877165238e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250326_001014-qtqa8m3i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-39
wandb: ⭐️ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: 🧹 View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: 🚀 View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/qtqa8m3i
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                    ┃ Type                      ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model_obj               │ ImagenetModels            │ 11.2 M │
│ 1  │ model_obj.model         │ ResNet                    │ 11.2 M │
│ 2  │ model_obj.model.conv1   │ Conv2d                    │  9.4 K │
│ 3  │ model_obj.model.bn1     │ BatchNorm2d               │    128 │
│ 4  │ model_obj.model.relu    │ ReLU                      │      0 │
│ 5  │ model_obj.model.maxpool │ MaxPool2d                 │      0 │
│ 6  │ model_obj.model.layer1  │ Sequential                │  147 K │
│ 7  │ model_obj.model.layer2  │ Sequential                │  525 K │
│ 8  │ model_obj.model.layer3  │ Sequential                │  2.1 M │
│ 9  │ model_obj.model.layer4  │ Sequential                │  8.4 M │
│ 10 │ model_obj.model.avgpool │ AdaptiveAvgPool2d         │      0 │
│ 11 │ model_obj.model.fc      │ Linear                    │  1.5 K │
│ 12 │ tr_kappa                │ MulticlassCohenKappa      │      0 │
│ 13 │ val_kappa               │ MulticlassCohenKappa      │      0 │
│ 14 │ tst_kappa               │ MulticlassCohenKappa      │      0 │
│ 15 │ tr_accuracy             │ MulticlassAccuracy        │      0 │
│ 16 │ val_accuracy            │ MulticlassAccuracy        │      0 │
│ 17 │ tst_accuracy            │ MulticlassAccuracy        │      0 │
│ 18 │ val_conf_mat            │ MulticlassConfusionMatrix │      0 │
│ 19 │ criterion               │ CrossEntropyLoss          │      0 │
└────┴─────────────────────────┴───────────────────────────┴────────┘
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.108
Metric val_ce_loss improved by 0.007 >= min_delta = 0.0001. New best score: 1.102
Metric val_ce_loss improved by 0.031 >= min_delta = 0.0001. New best score: 1.071
Metric val_ce_loss improved by 0.028 >= min_delta = 0.0001. New best score: 1.042
