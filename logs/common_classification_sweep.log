wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: ae55otgx with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 0.00014331042727981684
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 4.8580109226538715e-05
wandb: Currently logged in as: sporwal1818 (shri_krishna) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_192520-ae55otgx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-1
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ae55otgx
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Create sweep with ID: 5e1jb5oe
Sweep URL: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.116
Metric val_ce_loss improved by 0.006 >= min_delta = 0.0001. New best score: 1.110
Metric val_ce_loss improved by 0.008 >= min_delta = 0.0001. New best score: 1.102
Metric val_ce_loss improved by 0.059 >= min_delta = 0.0001. New best score: 1.043
Metric val_ce_loss improved by 0.057 >= min_delta = 0.0001. New best score: 0.986
Metric val_ce_loss improved by 0.215 >= min_delta = 0.0001. New best score: 0.771
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 0.771. Signaling Trainer to stop.
Epoch 22/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:25 â€¢       15.44it/s v_num: otgx      
                                     0:00:00                   val_ce_loss:     
                                                               5.688 val_kappa: 
                                                               0.079 val_acc:   
                                                               0.369            
                                                               train_ce_loss:   
                                                               0.172            
                                                               train_kappa:     
                                                               0.932 train_acc: 
                                                               0.935            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.36399999260902405    â”‚
â”‚    test_ce_loss_epoch     â”‚     5.676748752593994     â”‚
â”‚     test_kappa_epoch      â”‚    0.07352942228317261    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:04 â€¢ 0:00:00 10.28it/s 
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–…â–…â–…â–‡â–‡â–ƒâ–‚â–„â–ˆâ–†â–†â–„â–…â–†â–ƒâ–†â–ƒâ–†â–ƒâ–…â–„â–ƒâ–„â–†â–ˆâ–…â–…â–…â–„â–„â–„â–„â–…â–…â–ƒâ–‡â–â–„â–…â–†
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–…â–„â–ƒâ–‚â–‚â–…â–ˆâ–…â–â–„â–‚â–…â–†â–ƒâ–…â–ƒâ–‡â–„â–‡â–…â–†â–ˆâ–…â–‚â–‚â–„â–„â–„â–†â–…â–…â–„â–ƒâ–ƒâ–†â–â–†â–„â–ƒâ–…
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–â–…â–„â–†â–„â–…â–ƒâ–„â–…â–…â–…â–„â–‡â–„â–†â–†â–„â–†â–†â–…â–„â–…â–…â–„â–ˆâ–‡â–‡â–…â–†â–„â–†â–„â–†â–„â–…â–†â–…â–‚â–†â–‡
wandb:                      train_acc â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–
wandb:                    train_kappa â–â–â–â–â–‚â–‚â–ƒâ–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:                        val_acc â–â–â–â–â–â–â–ƒâ–ƒâ–„â–â–â–†â–‡â–â–‡â–†â–„â–â–‡â–â–„â–ˆâ–‚
wandb:                    val_ce_loss â–â–â–â–‚â–‚â–ƒâ–â–‚â–â–ˆâ–„â–â–â–†â–â–‚â–„â–‡â–‚â–‡â–ƒâ–‚â–ˆ
wandb:                      val_kappa â–â–â–â–â–â–â–„â–„â–…â–â–‚â–†â–ˆâ–â–‡â–‡â–„â–‚â–ˆâ–‚â–…â–ˆâ–‚
wandb: 
wandb: Run summary:
wandb:                          epoch 23
wandb: exponential_decay_lr_scheduler 0.00011
wandb:                 test_acc_epoch 0.364
wandb:                  test_acc_step 0.41071
wandb:             test_ce_loss_epoch 5.67675
wandb:              test_ce_loss_step 5.98594
wandb:               test_kappa_epoch 0.07353
wandb:                test_kappa_step 0.14917
wandb:                      train_acc 0.93498
wandb:                  train_ce_loss 0.17161
wandb:                    train_kappa 0.93156
wandb:            trainer/global_step 4370
wandb:                        val_acc 0.36889
wandb:                    val_ce_loss 5.6882
wandb:                      val_kappa 0.07903
wandb: 
wandb: ğŸš€ View run cool-sweep-1 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ae55otgx
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_192520-ae55otgx/logs
wandb: Agent Starting Run: j3uo2ije with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.2747862192849845e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.9711157517657677e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_193714-j3uo2ije
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-2
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/j3uo2ije
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.100
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.100. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:24 â€¢       16.05it/s v_num: 2ije      
                                     0:00:00                   val_ce_loss:     
                                                               2.121 val_kappa: 
                                                               0.140 val_acc:   
                                                               0.402            
                                                               train_ce_loss:   
                                                               0.048            
                                                               train_kappa:     
                                                               0.990 train_acc: 
                                                               0.991            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.38466668128967285    â”‚
â”‚    test_ce_loss_epoch     â”‚    2.2249033451080322     â”‚
â”‚     test_kappa_epoch      â”‚    0.12424951791763306    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:02 â€¢ 0:00:00 16.88it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–…â–ƒâ–„â–†â–…â–†â–ƒâ–‚â–ƒâ–‡â–â–…â–…â–„â–‚â–ƒâ–†â–ƒâ–„â–…â–ƒâ–ƒâ–…â–„â–†â–†â–„â–ˆâ–†â–…â–†â–‚â–…â–…â–…â–…â–„â–†â–‡â–…
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–ƒâ–‡â–…â–ƒâ–ƒâ–„â–†â–ˆâ–†â–„â–†â–‡â–†â–…â–ƒâ–…â–†â–„â–…â–…â–ƒâ–‡â–„â–†â–‡â–„â–‡â–â–„â–ƒâ–„â–„â–…â–„â–„â–…â–‡â–ƒâ–ƒâ–‚
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–„â–„â–ƒâ–…â–ƒâ–„â–‚â–ƒâ–„â–ˆâ–â–ƒâ–„â–â–…â–ƒâ–…â–ƒâ–†â–…â–‚â–â–…â–†â–‡â–†â–ƒâ–†â–†â–†â–‡â–„â–„â–…â–„â–„â–†â–…â–…â–‡
wandb:                      train_acc â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–‡â–‡â–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–ƒâ–‚â–â–
wandb:                    train_kappa â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–‡â–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–ˆ
wandb:                        val_acc â–â–â–„â–ƒâ–ƒâ–‚â–…â–‡â–„â–†â–ˆ
wandb:                    val_ce_loss â–â–â–â–â–‚â–„â–ƒâ–…â–ˆâ–‡â–‡
wandb:                      val_kappa â–â–‚â–…â–„â–ƒâ–ƒâ–†â–ˆâ–†â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.38467
wandb:                  test_acc_step 0.41071
wandb:             test_ce_loss_epoch 2.2249
wandb:              test_ce_loss_step 1.82317
wandb:               test_kappa_epoch 0.12425
wandb:                test_kappa_step 0.31148
wandb:                      train_acc 0.9914
wandb:                  train_ce_loss 0.04843
wandb:                    train_kappa 0.99012
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.40185
wandb:                    val_ce_loss 2.12113
wandb:                      val_kappa 0.14032
wandb: 
wandb: ğŸš€ View run feasible-sweep-2 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/j3uo2ije
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_193714-j3uo2ije/logs
wandb: Agent Starting Run: 70sq9jhe with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 7.442877189973635e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.027616377755173e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_194243-70sq9jhe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-3
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/70sq9jhe
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.127
Metric val_ce_loss improved by 0.022 >= min_delta = 0.0001. New best score: 1.105
Metric val_ce_loss improved by 0.003 >= min_delta = 0.0001. New best score: 1.102
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.102. Signaling Trainer to stop.
Epoch 12/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:24 â€¢       16.23it/s v_num: 9jhe      
                                     0:00:00                   val_ce_loss:     
                                                               2.390 val_kappa: 
                                                               0.166 val_acc:   
                                                               0.398            
                                                               train_ce_loss:   
                                                               0.042            
                                                               train_kappa:     
                                                               0.986 train_acc: 
                                                               0.989            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.41100001335144043    â”‚
â”‚    test_ce_loss_epoch     â”‚     2.293755292892456     â”‚
â”‚     test_kappa_epoch      â”‚    0.18101978302001953    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:02 â€¢ 0:00:00 17.10it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–„â–…â–„â–â–…â–…â–†â–„â–„â–„â–…â–„â–…â–„â–…â–‚â–…â–…â–…â–…â–ƒâ–„â–„â–„â–„â–…â–„â–…â–„â–…â–„â–…â–ƒâ–„â–†â–†â–‡â–ˆâ–…â–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–‚â–ƒâ–„â–‡â–„â–â–â–„â–„â–ƒâ–„â–„â–ƒâ–…â–‚â–ˆâ–„â–‚â–ƒâ–„â–ƒâ–ƒâ–…â–„â–ƒâ–ƒâ–„â–â–„â–†â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–‚â–ƒ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–ƒâ–„â–ƒâ–â–„â–‡â–†â–…â–†â–…â–„â–„â–ƒâ–„â–…â–â–…â–†â–…â–…â–‚â–„â–‚â–„â–…â–…â–…â–…â–‚â–†â–‚â–ˆâ–‚â–ƒâ–…â–†â–ˆâ–‡â–ƒâ–„
wandb:                      train_acc â–â–‚â–‚â–‚â–ƒâ–„â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–ƒâ–‚â–â–â–
wandb:                    train_kappa â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–ƒâ–†â–‡â–†â–‚â–â–ƒâ–…â–ˆâ–…â–‡â–ˆ
wandb:                    val_ce_loss â–â–â–â–â–â–ƒâ–‡â–‡â–ˆâ–…â–‡â–ˆâ–†
wandb:                      val_kappa â–â–ƒâ–…â–…â–…â–‚â–‚â–ƒâ–ƒâ–‡â–…â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 13
wandb: exponential_decay_lr_scheduler 7e-05
wandb:                 test_acc_epoch 0.411
wandb:                  test_acc_step 0.35714
wandb:             test_ce_loss_epoch 2.29376
wandb:              test_ce_loss_step 2.26136
wandb:               test_kappa_epoch 0.18102
wandb:                test_kappa_step 0.125
wandb:                      train_acc 0.98909
wandb:                  train_ce_loss 0.04228
wandb:                    train_kappa 0.98618
wandb:            trainer/global_step 1235
wandb:                        val_acc 0.39778
wandb:                    val_ce_loss 2.38966
wandb:                      val_kappa 0.16612
wandb: 
wandb: ğŸš€ View run sage-sweep-3 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/70sq9jhe
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_194243-70sq9jhe/logs
wandb: Agent Starting Run: 5mx5oqjq with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.047572612715447e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 1.3509558483830627e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_194927-5mx5oqjq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-4
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/5mx5oqjq
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.114
Metric val_ce_loss improved by 0.013 >= min_delta = 0.0001. New best score: 1.101
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.101. Signaling Trainer to stop.
Epoch 11/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:27 â€¢       13.89it/s v_num: oqjq      
                                     0:00:00                   val_ce_loss:     
                                                               2.313 val_kappa: 
                                                               0.136 val_acc:   
                                                               0.395            
                                                               train_ce_loss:   
                                                               0.016            
                                                               train_kappa:     
                                                               0.999 train_acc: 
                                                               0.999            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.3733333349227905     â”‚
â”‚    test_ce_loss_epoch     â”‚    2.4106335639953613     â”‚
â”‚     test_kappa_epoch      â”‚    0.0864419937133789     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 12.76it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–„â–†â–…â–ƒâ–„â–„â–‚â–ƒâ–„â–„â–…â–â–ƒâ–‚â–ƒâ–ˆâ–†â–†â–â–„â–…â–ƒâ–…â–„â–‡â–…â–…â–„â–†â–†â–…â–‚â–„â–…â–ƒâ–…â–‚â–…â–ƒâ–…
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–‚â–â–‡â–‡â–‚â–…â–„â–…â–„â–…â–â–ˆâ–ƒâ–†â–…â–„â–‚â–â–…â–„â–ƒâ–…â–„â–†â–ƒâ–…â–‚â–„â–„â–…â–…â–‡â–„â–„â–ƒâ–‚â–ƒâ–„â–„â–„
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–…â–†â–„â–‚â–‡â–†â–„â–„â–†â–ˆâ–…â–â–„â–‚â–ƒâ–‡â–„â–‡â–ƒâ–„â–…â–…â–…â–„â–ˆâ–…â–…â–„â–‡â–†â–†â–„â–„â–…â–…â–„â–‚â–„â–…â–„
wandb:                      train_acc â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–„â–ƒâ–‚â–â–â–
wandb:                    train_kappa â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–‚â–â–…â–…â–‡â–…â–ˆâ–‡â–ˆâ–ˆâ–‡
wandb:                    val_ce_loss â–â–â–â–â–‚â–ƒâ–…â–†â–ˆâ–‡â–‡â–ˆ
wandb:                      val_kappa â–â–ƒâ–‚â–„â–†â–‡â–†â–‡â–‡â–ˆâ–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.37333
wandb:                  test_acc_step 0.39286
wandb:             test_ce_loss_epoch 2.41063
wandb:              test_ce_loss_step 2.40201
wandb:               test_kappa_epoch 0.08644
wandb:                test_kappa_step 0.02999
wandb:                      train_acc 0.99909
wandb:                  train_ce_loss 0.01595
wandb:                    train_kappa 0.99858
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.39519
wandb:                    val_ce_loss 2.31287
wandb:                      val_kappa 0.13643
wandb: 
wandb: ğŸš€ View run true-sweep-4 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/5mx5oqjq
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_194927-5mx5oqjq/logs
wandb: Agent Starting Run: c4dx5zth with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.319205826168334e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 1.3529947964598624e-05
wandb: creating run
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_195529-c4dx5zth
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-5
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/c4dx5zth
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.109
Metric val_ce_loss improved by 0.002 >= min_delta = 0.0001. New best score: 1.108
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.108. Signaling Trainer to stop.
Epoch 11/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:23 â€¢       16.87it/s v_num: 5zth      
                                     0:00:00                   val_ce_loss:     
                                                               2.301 val_kappa: 
                                                               0.170 val_acc:   
                                                               0.403            
                                                               train_ce_loss:   
                                                               0.020            
                                                               train_kappa:     
                                                               0.998 train_acc: 
                                                               0.998            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.42533332109451294    â”‚
â”‚    test_ce_loss_epoch     â”‚    2.1679484844207764     â”‚
â”‚     test_kappa_epoch      â”‚    0.19584137201309204    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:02 â€¢ 0:00:00 16.84it/s 
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–†â–…â–ƒâ–‚â–†â–„â–…â–†â–„â–„â–â–ƒâ–…â–…â–„â–„â–ƒâ–†â–ˆâ–…â–„â–ƒâ–„â–…â–†â–†â–„â–…â–„â–…â–…â–ƒâ–ƒâ–†â–‡â–…â–„â–ˆâ–ƒâ–‡
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–‚â–ƒâ–„â–‡â–„â–„â–‚â–„â–…â–„â–ˆâ–†â–‡â–„â–„â–†â–…â–‚â–â–ƒâ–„â–…â–ƒâ–…â–‚â–‚â–†â–â–„â–„â–ƒâ–†â–„â–ƒâ–ƒâ–…â–…â–â–„â–„
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–†â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–ƒâ–…â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–â–ƒâ–‡â–…â–â–„â–„â–ƒâ–†â–†â–â–‚â–„â–„â–…â–ƒâ–â–„â–„â–…â–ƒâ–ˆâ–‚â–…
wandb:                      train_acc â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–„â–ƒâ–‚â–â–â–
wandb:                    train_kappa â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–ƒâ–‚â–ƒâ–…â–â–ƒâ–‚â–ˆâ–†â–†â–ˆ
wandb:                    val_ce_loss â–â–â–â–‚â–‚â–…â–„â–ˆâ–†â–†â–†â–†
wandb:                      val_kappa â–â–ƒâ–‚â–ƒâ–„â–‚â–„â–ƒâ–ˆâ–‡â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.42533
wandb:                  test_acc_step 0.51786
wandb:             test_ce_loss_epoch 2.16795
wandb:              test_ce_loss_step 2.1803
wandb:               test_kappa_epoch 0.19584
wandb:                test_kappa_step 0.28686
wandb:                      train_acc 0.99827
wandb:                  train_ce_loss 0.01971
wandb:                    train_kappa 0.99796
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.40333
wandb:                    val_ce_loss 2.30087
wandb:                      val_kappa 0.16952
wandb: 
wandb: ğŸš€ View run distinctive-sweep-5 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/c4dx5zth
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_195529-c4dx5zth/logs
wandb: Agent Starting Run: uiaqddlh with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.448311020378875e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.149038690238364e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_200126-uiaqddlh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-6
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/uiaqddlh
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.107
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.107. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:29 â€¢       13.34it/s v_num: ddlh      
                                     0:00:00                   val_ce_loss:     
                                                               2.176 val_kappa: 
                                                               0.165 val_acc:   
                                                               0.400            
                                                               train_ce_loss:   
                                                               0.058            
                                                               train_kappa:     
                                                               0.983 train_acc: 
                                                               0.987            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.40533334016799927    â”‚
â”‚    test_ce_loss_epoch     â”‚    2.2087297439575195     â”‚
â”‚     test_kappa_epoch      â”‚    0.1738918423652649     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 13.83it/s 
wandb: uploading wandb-summary.json; uploading config.yaml; uploading history steps 100-149, summary, console lines 41-61
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–…â–†â–‚â–„â–…â–‚â–ƒâ–„â–„â–…â–â–ƒâ–â–…â–â–…â–„â–ƒâ–„â–…â–‚â–…â–„â–„â–…â–†â–…â–…â–ƒâ–†â–…â–…â–â–…â–†â–†â–„â–ˆâ–„â–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–ƒâ–â–†â–‡â–ƒâ–…â–‡â–…â–ƒâ–ƒâ–…â–†â–‡â–„â–ˆâ–…â–…â–ƒâ–ƒâ–…â–†â–ƒâ–„â–„â–ƒâ–„â–†â–ƒâ–…â–„â–ƒâ–…â–ˆâ–ƒâ–…â–‚â–„â–ƒâ–…â–…
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–‡â–‡â–…â–…â–†â–…â–ƒâ–…â–‡â–ˆâ–ƒâ–„â–ƒâ–‚â–‚â–„â–„â–â–…â–†â–ƒâ–…â–‚â–„â–‡â–ˆâ–…â–…â–ƒâ–†â–‡â–„â–„â–‚â–…â–…â–„â–…â–„â–…
wandb:                      train_acc â–â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–ƒâ–‚â–â–
wandb:                    train_kappa â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–‚â–‚â–‚â–ƒâ–„â–„â–„â–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–â–ƒâ–ƒâ–„â–…â–„â–†â–…â–ˆâ–‡
wandb:                    val_ce_loss â–â–â–â–â–â–ƒâ–…â–„â–‡â–‡â–ˆ
wandb:                      val_kappa â–‚â–â–ƒâ–ƒâ–„â–…â–„â–†â–†â–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.40533
wandb:                  test_acc_step 0.39286
wandb:             test_ce_loss_epoch 2.20873
wandb:              test_ce_loss_step 2.27457
wandb:               test_kappa_epoch 0.17389
wandb:                test_kappa_step 0.20426
wandb:                      train_acc 0.98675
wandb:                  train_ce_loss 0.05772
wandb:                    train_kappa 0.98274
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.4
wandb:                    val_ce_loss 2.17554
wandb:                      val_kappa 0.16505
wandb: 
wandb: ğŸš€ View run flowing-sweep-6 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/uiaqddlh
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_200126-uiaqddlh/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 3gi43ovb with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 6.031799624671424e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 6.9783799959705e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_200656-3gi43ovb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-7
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/3gi43ovb
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.101
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.101. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:24 â€¢       16.33it/s v_num: 3ovb      
                                     0:00:00                   val_ce_loss:     
                                                               2.317 val_kappa: 
                                                               0.150 val_acc:   
                                                               0.406            
                                                               train_ce_loss:   
                                                               0.096            
                                                               train_kappa:     
                                                               0.968 train_acc: 
                                                               0.973            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.40166667103767395    â”‚
â”‚    test_ce_loss_epoch     â”‚     2.276824951171875     â”‚
â”‚     test_kappa_epoch      â”‚    0.1615772843360901     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 14.68it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–„â–„â–â–ƒâ–‚â–†â–ƒâ–…â–‚â–†â–ƒâ–‚â–„â–„â–ƒâ–â–ƒâ–ˆâ–‡â–„â–â–„â–‡â–…â–†â–‡â–â–†â–â–„â–„â–†â–ƒâ–…â–†â–„â–„â–ˆâ–„â–ƒ
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–ƒâ–…â–†â–„â–„â–ƒâ–„â–ƒâ–…â–‚â–†â–†â–‡â–„â–ˆâ–ˆâ–ˆâ–‚â–‚â–„â–†â–…â–â–„â–„â–‚â–†â–‚â–†â–„â–ƒâ–„â–„â–…â–„â–…â–†â–ƒâ–„â–…
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–†â–†â–‚â–„â–â–…â–„â–…â–…â–ˆâ–…â–„â–…â–ƒâ–…â–„â–‚â–ˆâ–ˆâ–†â–‚â–…â–‡â–…â–‡â–‡â–ƒâ–ˆâ–„â–‡â–†â–‡â–ƒâ–…â–†â–‡â–†â–‡â–„â–†
wandb:                      train_acc â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–‡â–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–‚â–â–
wandb:                    train_kappa â–â–‚â–‚â–ƒâ–„â–„â–…â–†â–‡â–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–ˆ
wandb:                        val_acc â–‚â–â–‚â–ƒâ–…â–†â–…â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    val_ce_loss â–â–â–â–â–‚â–‚â–…â–„â–†â–†â–ˆ
wandb:                      val_kappa â–‚â–â–‚â–ƒâ–ƒâ–†â–„â–†â–ˆâ–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.40167
wandb:                  test_acc_step 0.375
wandb:             test_ce_loss_epoch 2.27682
wandb:              test_ce_loss_step 2.49784
wandb:               test_kappa_epoch 0.16158
wandb:                test_kappa_step 0.21519
wandb:                      train_acc 0.97309
wandb:                  train_ce_loss 0.09646
wandb:                    train_kappa 0.96761
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.40556
wandb:                    val_ce_loss 2.3168
wandb:                      val_kappa 0.15043
wandb: 
wandb: ğŸš€ View run royal-sweep-7 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/3gi43ovb
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_200656-3gi43ovb/logs
wandb: Agent Starting Run: 0ix2qz54 with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 5.519312553060601e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 1.015682881002732e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_201221-0ix2qz54
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-8
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/0ix2qz54
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.100
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.100. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:25 â€¢       14.95it/s v_num: qz54      
                                     0:00:00                   val_ce_loss:     
                                                               2.744 val_kappa: 
                                                               0.107 val_acc:   
                                                               0.372            
                                                               train_ce_loss:   
                                                               0.341            
                                                               train_kappa:     
                                                               0.848 train_acc: 
                                                               0.869            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.3646666705608368     â”‚
â”‚    test_ce_loss_epoch     â”‚     2.743196964263916     â”‚
â”‚     test_kappa_epoch      â”‚    0.0871044397354126     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 13.83it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–‡â–…â–…â–…â–ƒâ–„â–ƒâ–â–‡â–„â–ƒâ–‚â–â–„â–ƒâ–‡â–…â–†â–ƒâ–„â–ƒâ–‚â–„â–‡â–ˆâ–…â–ƒâ–†â–ƒâ–„â–â–„â–„â–…â–‚â–ˆâ–‚â–…â–†â–†
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–…â–„â–ƒâ–„â–…â–…â–‡â–„â–‚â–…â–†â–‡â–†â–…â–…â–‚â–…â–…â–‡â–‚â–…â–ˆâ–‡â–‚â–‚â–…â–„â–„â–†â–„â–†â–„â–…â–ƒâ–ˆâ–â–…â–ƒâ–‡â–„
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–ˆâ–„â–†â–„â–ƒâ–…â–„â–‚â–ƒâ–†â–ƒâ–„â–â–ƒâ–ˆâ–„â–ˆâ–…â–†â–…â–…â–„â–„â–ƒâ–…â–†â–ƒâ–…â–ƒâ–…â–ƒâ–„â–…â–„â–„â–†â–‡â–…â–…â–†
wandb:                      train_acc â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–‡â–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–‚â–
wandb:                    train_kappa â–â–â–‚â–‚â–ƒâ–„â–„â–…â–†â–‡â–ˆ
wandb:            trainer/global_step â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–‚â–‚â–â–â–‚â–…â–â–„â–„â–ˆâ–ƒ
wandb:                    val_ce_loss â–â–â–â–â–ƒâ–â–ˆâ–‚â–„â–‚â–†
wandb:                      val_kappa â–‚â–â–‚â–â–â–„â–â–ƒâ–…â–ˆâ–„
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.36467
wandb:                  test_acc_step 0.41071
wandb:             test_ce_loss_epoch 2.7432
wandb:              test_ce_loss_step 2.67122
wandb:               test_kappa_epoch 0.0871
wandb:                test_kappa_step 0.17647
wandb:                      train_acc 0.86905
wandb:                  train_ce_loss 0.34056
wandb:                    train_kappa 0.84788
wandb:            trainer/global_step 2090
wandb:                        val_acc 0.37222
wandb:                    val_ce_loss 2.74416
wandb:                      val_kappa 0.10743
wandb: 
wandb: ğŸš€ View run giddy-sweep-8 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/0ix2qz54
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_201221-0ix2qz54/logs
wandb: Agent Starting Run: 9839w8ke with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.594935753085661e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.7507905661419135e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_201756-9839w8ke
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-9
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/9839w8ke
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.105
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.105. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:23 â€¢       16.57it/s v_num: w8ke      
                                     0:00:00                   val_ce_loss:     
                                                               2.284 val_kappa: 
                                                               0.174 val_acc:   
                                                               0.411            
                                                               train_ce_loss:   
                                                               0.080            
                                                               train_kappa:     
                                                               0.977 train_acc: 
                                                               0.979            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.4046666622161865     â”‚
â”‚    test_ce_loss_epoch     â”‚    2.2972652912139893     â”‚
â”‚     test_kappa_epoch      â”‚    0.1691380739212036     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 12.84it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–…â–ƒâ–ƒâ–„â–…â–…â–‡â–ƒâ–„â–†â–…â–…â–‚â–ƒâ–„â–â–‚â–†â–„â–„â–†â–„â–…â–ƒâ–„â–‡â–†â–†â–„â–‚â–„â–ƒâ–‚â–…â–…â–…â–…â–ˆâ–‡â–…
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–ƒâ–…â–ƒâ–‡â–„â–„â–ƒâ–„â–…â–„â–ƒâ–„â–ˆâ–‡â–ƒâ–‡â–ˆâ–ƒâ–ƒâ–„â–„â–„â–‚â–ƒâ–ƒâ–‚â–„â–ƒâ–ƒâ–†â–„â–‡â–ˆâ–„â–‚â–…â–ƒâ–‚â–â–…
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–…â–…â–…â–„â–ƒâ–…â–ˆâ–„â–…â–‡â–…â–„â–„â–‚â–„â–‚â–‚â–†â–…â–†â–‡â–„â–ƒâ–„â–…â–†â–…â–†â–†â–â–„â–„â–ƒâ–†â–…â–†â–‡â–ˆâ–†â–†
wandb:                      train_acc â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–‡â–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–‚â–â–
wandb:                    train_kappa â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–‡â–ˆâ–ˆ
wandb:            trainer/global_step â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–‚â–‚â–„â–‚â–…â–…â–…â–ˆâ–…â–‡
wandb:                    val_ce_loss â–â–â–â–â–ƒâ–‚â–ƒâ–†â–…â–ˆâ–‡
wandb:                      val_kappa â–â–‚â–‚â–„â–‚â–…â–…â–…â–ˆâ–…â–‡
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.40467
wandb:                  test_acc_step 0.44643
wandb:             test_ce_loss_epoch 2.29727
wandb:              test_ce_loss_step 2.46435
wandb:               test_kappa_epoch 0.16914
wandb:                test_kappa_step 0.2797
wandb:                      train_acc 0.97922
wandb:                  train_ce_loss 0.08017
wandb:                    train_kappa 0.97698
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.41074
wandb:                    val_ce_loss 2.28393
wandb:                      val_kappa 0.17373
wandb: 
wandb: ğŸš€ View run cool-sweep-9 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/9839w8ke
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_201756-9839w8ke/logs
wandb: Agent Starting Run: 07m4oktc with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 5.2491659758535025e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 1.4933964902180926e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_202326-07m4oktc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-sweep-10
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/07m4oktc
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.105
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.105. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:24 â€¢       15.72it/s v_num: oktc      
                                     0:00:00                   val_ce_loss:     
                                                               4.154 val_kappa: 
                                                               0.067 val_acc:   
                                                               0.364            
                                                               train_ce_loss:   
                                                               0.210            
                                                               train_kappa:     
                                                               0.914 train_acc: 
                                                               0.926            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.35066667199134827    â”‚
â”‚    test_ce_loss_epoch     â”‚     4.268784999847412     â”‚
â”‚     test_kappa_epoch      â”‚    0.06504064798355103    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 13.17it/s 
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–‡â–…â–…â–…â–„â–ƒâ–â–…â–…â–…â–„â–‡â–ƒâ–ƒâ–ƒâ–†â–„â–…â–ƒâ–ƒâ–â–ƒâ–â–ƒâ–‡â–ƒâ–…â–‡â–ƒâ–…â–ƒâ–†â–‡â–ˆâ–â–ƒâ–â–†â–ƒâ–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–…â–ƒâ–â–…â–ƒâ–„â–‡â–„â–‚â–„â–„â–…â–„â–„â–†â–â–„â–ƒâ–…â–ƒâ–…â–ˆâ–‡â–‚â–‚â–…â–„â–ƒâ–„â–„â–†â–„â–„â–ƒâ–†â–ƒâ–…â–…â–†â–…
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–ˆâ–„â–„â–ƒâ–â–†â–‚â–‚â–‚â–„â–„â–…â–…â–„â–†â–ƒâ–†â–ƒâ–…â–„â–‚â–„â–ƒâ–â–‡â–ƒâ–„â–‡â–„â–ˆâ–ƒâ–†â–†â–‡â–ƒâ–„â–ƒâ–‡â–…â–ƒ
wandb:                      train_acc â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–‡â–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–‚â–
wandb:                    train_kappa â–â–‚â–‚â–‚â–ƒâ–„â–…â–†â–‡â–‡â–ˆ
wandb:            trainer/global_step â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–‚â–â–‚â–â–â–‚â–ˆâ–ƒâ–‡â–‡â–ƒ
wandb:                    val_ce_loss â–â–â–â–â–‚â–‚â–â–ƒâ–‚â–ƒâ–ˆ
wandb:                      val_kappa â–‚â–â–‚â–â–â–ƒâ–ˆâ–ƒâ–‡â–†â–ƒ
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.35067
wandb:                  test_acc_step 0.33929
wandb:             test_ce_loss_epoch 4.26878
wandb:              test_ce_loss_step 4.44619
wandb:               test_kappa_epoch 0.06504
wandb:                test_kappa_step -0.03016
wandb:                      train_acc 0.92617
wandb:                  train_ce_loss 0.20985
wandb:                    train_kappa 0.91386
wandb:            trainer/global_step 2090
wandb:                        val_acc 0.3637
wandb:                    val_ce_loss 4.15372
wandb:                      val_kappa 0.06733
wandb: 
wandb: ğŸš€ View run bright-sweep-10 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/07m4oktc
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_202326-07m4oktc/logs
wandb: Agent Starting Run: svaeqm5v with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 6.951872902027624e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.951918703072749e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_202857-svaeqm5v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-11
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/svaeqm5v
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.102
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.102. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:24 â€¢       16.00it/s v_num: qm5v      
                                     0:00:00                   val_ce_loss:     
                                                               4.358 val_kappa: 
                                                               0.063 val_acc:   
                                                               0.361            
                                                               train_ce_loss:   
                                                               0.189            
                                                               train_kappa:     
                                                               0.924 train_acc: 
                                                               0.935            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.3610000014305115     â”‚
â”‚    test_ce_loss_epoch     â”‚     4.337547302246094     â”‚
â”‚     test_kappa_epoch      â”‚    0.06591182947158813    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 15.13it/s 
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–‡â–„â–‚â–…â–„â–†â–…â–„â–‚â–‡â–…â–…â–ƒâ–„â–…â–â–‚â–„â–ˆâ–„â–…â–‡â–‡â–„â–„â–†â–ƒâ–…â–„â–„â–‡â–„â–ƒâ–„â–†â–„â–†â–‡â–†â–…
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–ƒâ–„â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‡â–ƒâ–‚â–„â–ˆâ–„â–„â–…â–†â–„â–â–…â–‚â–‚â–ƒâ–†â–„â–‚â–„â–ƒâ–ƒâ–…â–â–„â–…â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–ˆâ–ƒâ–„â–†â–‚â–ƒâ–‚â–ƒâ–ƒâ–†â–„â–‚â–â–‚â–ƒâ–‚â–‚â–ƒâ–†â–â–‚â–ƒâ–ƒâ–‚â–„â–…â–â–„â–‚â–ƒâ–ƒâ–ƒâ–â–â–‚â–„â–ƒâ–„â–‚â–…
wandb:                      train_acc â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–‡â–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–ƒâ–‚â–
wandb:                    train_kappa â–â–‚â–‚â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–â–â–‚â–‚â–â–ˆâ–‚â–…â–†â–‚
wandb:                    val_ce_loss â–â–â–â–â–â–â–â–…â–‚â–ƒâ–ˆ
wandb:                      val_kappa â–‚â–â–‚â–‚â–â–â–ˆâ–‚â–‡â–ˆâ–‚
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.361
wandb:                  test_acc_step 0.35714
wandb:             test_ce_loss_epoch 4.33755
wandb:              test_ce_loss_step 3.99636
wandb:               test_kappa_epoch 0.06591
wandb:                test_kappa_step 0.13895
wandb:                      train_acc 0.93457
wandb:                  train_ce_loss 0.18855
wandb:                    train_kappa 0.92362
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.36074
wandb:                    val_ce_loss 4.35759
wandb:                      val_kappa 0.0635
wandb: 
wandb: ğŸš€ View run astral-sweep-11 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/svaeqm5v
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_202857-svaeqm5v/logs
wandb: Agent Starting Run: kt5sqvtq with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.00012258599315823844
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.550883524753298e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_203426-kt5sqvtq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-12
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/kt5sqvtq
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.107
Metric val_ce_loss improved by 0.004 >= min_delta = 0.0001. New best score: 1.103
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.103. Signaling Trainer to stop.
Epoch 11/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:25 â€¢       15.05it/s v_num: qvtq      
                                     0:00:00                   val_ce_loss:     
                                                               1.281 val_kappa: 
                                                               0.264 val_acc:   
                                                               0.442            
                                                               train_ce_loss:   
                                                               0.652            
                                                               train_kappa:     
                                                               0.668 train_acc: 
                                                               0.715            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.45366665720939636    â”‚
â”‚    test_ce_loss_epoch     â”‚     1.254696249961853     â”‚
â”‚     test_kappa_epoch      â”‚    0.2751889228820801     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 14.18it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–†â–…â–„â–„â–‡â–‚â–â–‚â–…â–…â–…â–„â–‚â–…â–…â–†â–…â–„â–ƒâ–ƒâ–ƒâ–…â–ƒâ–ƒâ–†â–„â–ƒâ–„â–â–‚â–â–†â–â–‡â–…â–„â–ƒâ–ˆâ–…â–…
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–‚â–„â–…â–„â–„â–…â–†â–‡â–„â–ƒâ–†â–†â–…â–„â–„â–ƒâ–ƒâ–…â–…â–…â–ˆâ–†â–„â–†â–ƒâ–ƒâ–†â–ƒâ–‡â–‡â–ˆâ–„â–†â–‚â–…â–ƒâ–ƒâ–â–…â–ƒ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–‡â–†â–‚â–„â–ƒâ–…â–ƒâ–„â–â–†â–ƒâ–„â–…â–„â–‡â–ƒâ–„â–…â–…â–†â–â–…â–…â–â–…â–†â–ƒâ–‚â–‚â–ƒâ–„â–†â–ƒâ–‡â–ƒâ–†â–‡â–ˆâ–…â–ˆ
wandb:                      train_acc â–â–â–‚â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–„â–‚â–
wandb:                    train_kappa â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–‡â–‡â–ˆ
wandb:            trainer/global_step â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–‚â–‚â–‚â–â–‚â–â–…â–â–…â–â–‚â–ˆ
wandb:                    val_ce_loss â–â–â–â–â–â–ƒâ–â–„â–â–ˆâ–ˆâ–
wandb:                      val_kappa â–â–‚â–‚â–â–‚â–â–…â–â–„â–â–‚â–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 0.00011
wandb:                 test_acc_epoch 0.45367
wandb:                  test_acc_step 0.5
wandb:             test_ce_loss_epoch 1.2547
wandb:              test_ce_loss_step 1.07876
wandb:               test_kappa_epoch 0.27519
wandb:                test_kappa_step 0.46617
wandb:                      train_acc 0.71494
wandb:                  train_ce_loss 0.65232
wandb:                    train_kappa 0.66785
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.44222
wandb:                    val_ce_loss 1.28132
wandb:                      val_kappa 0.26407
wandb: 
wandb: ğŸš€ View run bumbling-sweep-12 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/kt5sqvtq
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_203426-kt5sqvtq/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: lxcdeqzi with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 6.840510722374592e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.2768165372926652e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_204026-lxcdeqzi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-13
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/lxcdeqzi
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.114
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.114. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:21 â€¢       17.73it/s v_num: eqzi      
                                     0:00:00                   val_ce_loss:     
                                                               2.343 val_kappa: 
                                                               0.136 val_acc:   
                                                               0.395            
                                                               train_ce_loss:   
                                                               0.151            
                                                               train_kappa:     
                                                               0.945 train_acc: 
                                                               0.952            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.39100000262260437    â”‚
â”‚    test_ce_loss_epoch     â”‚     2.290358543395996     â”‚
â”‚     test_kappa_epoch      â”‚    0.1344902515411377     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:02 â€¢ 0:00:00 18.61it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–ˆâ–‚â–„â–‚â–…â–†â–ƒâ–„â–‚â–„â–ƒâ–ƒâ–ƒâ–‚â–…â–‚â–…â–„â–‡â–„â–…â–‚â–…â–ƒâ–…â–‡â–‚â–†â–â–ƒâ–„â–„â–ƒâ–„â–„â–„â–„â–…â–„â–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–‚â–ƒâ–…â–„â–„â–ƒâ–†â–ƒâ–„â–ƒâ–†â–„â–†â–…â–…â–†â–…â–ƒâ–â–„â–ƒâ–ˆâ–…â–†â–…â–‚â–„â–‚â–ˆâ–„â–…â–„â–…â–ƒâ–ƒâ–…â–ƒâ–‚â–‚â–„
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–ˆâ–…â–…â–ƒâ–†â–†â–„â–†â–ƒâ–…â–‚â–…â–…â–‚â–†â–„â–‡â–…â–‡â–‡â–…â–ƒâ–ƒâ–ƒâ–†â–ˆâ–ƒâ–„â–â–„â–ƒâ–‡â–ƒâ–ƒâ–‡â–‡â–ˆâ–…â–ƒâ–…
wandb:                      train_acc â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–‡â–‡â–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–ƒâ–‚â–
wandb:                    train_kappa â–â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–‡â–ˆ
wandb:            trainer/global_step â–â–‚â–‚â–‚â–ƒâ–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–‚â–‚â–ƒâ–ƒâ–‡â–â–„â–ƒâ–‡â–ˆ
wandb:                    val_ce_loss â–â–â–â–â–â–‚â–‡â–„â–ˆâ–…â–†
wandb:                      val_kappa â–â–â–‚â–ƒâ–ƒâ–ˆâ–‚â–„â–„â–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.391
wandb:                  test_acc_step 0.41071
wandb:             test_ce_loss_epoch 2.29036
wandb:              test_ce_loss_step 2.21826
wandb:               test_kappa_epoch 0.13449
wandb:                test_kappa_step 0.15663
wandb:                      train_acc 0.95173
wandb:                  train_ce_loss 0.15053
wandb:                    train_kappa 0.94479
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.39481
wandb:                    val_ce_loss 2.34257
wandb:                      val_kappa 0.1364
wandb: 
wandb: ğŸš€ View run charmed-sweep-13 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/lxcdeqzi
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_204026-lxcdeqzi/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ml8h6hho with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.00013492993012448017
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.477192938128564e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_204553-ml8h6hho
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-14
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ml8h6hho
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.098
Metric val_ce_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.097
Metric val_ce_loss improved by 0.119 >= min_delta = 0.0001. New best score: 0.977
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 0.977. Signaling Trainer to stop.
Epoch 19/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:24 â€¢       15.59it/s v_num: 6hho      
                                     0:00:00                   val_ce_loss:     
                                                               3.494 val_kappa: 
                                                               0.170 val_acc:   
                                                               0.412            
                                                               train_ce_loss:   
                                                               0.115            
                                                               train_kappa:     
                                                               0.952 train_acc: 
                                                               0.957            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚     0.398333340883255     â”‚
â”‚    test_ce_loss_epoch     â”‚    3.4774017333984375     â”‚
â”‚     test_kappa_epoch      â”‚    0.12695038318634033    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 14.50it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–ˆâ–…â–ƒâ–†â–„â–„â–„â–â–†â–…â–…â–ƒâ–…â–‡â–‚â–ˆâ–ƒâ–…â–„â–„â–‚â–â–‚â–‡â–ˆâ–†â–…â–†â–ƒâ–„â–ƒâ–‚â–„â–„â–ƒâ–‡â–‚â–…â–…â–†
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–â–„â–†â–„â–…â–…â–‡â–†â–‚â–„â–†â–‡â–…â–„â–…â–ƒâ–…â–…â–…â–†â–‡â–ˆâ–‡â–„â–‚â–…â–…â–ƒâ–…â–†â–…â–„â–…â–„â–…â–ƒâ–†â–‚â–„â–ƒ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–‡â–ƒâ–„â–…â–â–†â–†â–â–‚â–…â–‚â–…â–…â–ƒâ–…â–ƒâ–„â–†â–‡â–„â–ƒâ–„â–‚â–â–ˆâ–ˆâ–‡â–ƒâ–…â–†â–‚â–â–…â–ƒâ–‚â–†â–„â–„â–‡â–†
wandb:                      train_acc â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–â–â–
wandb:                    train_kappa â–â–â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–â–â–‚â–‚â–â–„â–„â–â–‡â–‚â–‚â–‚â–ƒâ–ˆâ–ƒâ–„â–ƒâ–‚â–ƒ
wandb:                    val_ce_loss â–â–â–â–â–â–‚â–â–â–ˆâ–â–…â–ƒâ–„â–ƒâ–â–ƒâ–ƒâ–‡â–‡â–„
wandb:                      val_kappa â–â–â–â–â–‚â–â–„â–„â–â–‡â–‚â–‚â–‚â–‚â–ˆâ–ƒâ–„â–ƒâ–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:                          epoch 20
wandb: exponential_decay_lr_scheduler 0.00011
wandb:                 test_acc_epoch 0.39833
wandb:                  test_acc_step 0.46429
wandb:             test_ce_loss_epoch 3.4774
wandb:              test_ce_loss_step 2.93847
wandb:               test_kappa_epoch 0.12695
wandb:                test_kappa_step 0.23438
wandb:                      train_acc 0.95671
wandb:                  train_ce_loss 0.11549
wandb:                    train_kappa 0.95222
wandb:            trainer/global_step 1900
wandb:                        val_acc 0.41222
wandb:                    val_ce_loss 3.49411
wandb:                      val_kappa 0.1695
wandb: 
wandb: ğŸš€ View run whole-sweep-14 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ml8h6hho
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_204553-ml8h6hho/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: e6etpsra with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.512168846921846e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 6.281122366182823e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_205559-e6etpsra
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-15
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/e6etpsra
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.114
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.114. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:21 â€¢       18.62it/s v_num: psra      
                                     0:00:00                   val_ce_loss:     
                                                               2.136 val_kappa: 
                                                               0.190 val_acc:   
                                                               0.410            
                                                               train_ce_loss:   
                                                               0.083            
                                                               train_kappa:     
                                                               0.974 train_acc: 
                                                               0.978            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.3973333239555359     â”‚
â”‚    test_ce_loss_epoch     â”‚    2.1523447036743164     â”‚
â”‚     test_kappa_epoch      â”‚    0.16555404663085938    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 15.14it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–†â–…â–…â–…â–…â–†â–…â–â–‚â–ˆâ–„â–ƒâ–â–†â–…â–ƒâ–„â–ˆâ–†â–…â–ƒâ–ƒâ–†â–†â–…â–†â–†â–„â–†â–…â–‡â–ƒâ–„â–…â–†â–„â–ˆâ–…â–â–ƒ
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–„â–‡â–„â–†â–…â–‡â–†â–‡â–…â–ƒâ–†â–‡â–‡â–‡â–„â–„â–‡â–â–â–ƒâ–‡â–‡â–‚â–„â–…â–ƒâ–ˆâ–ƒâ–„â–…â–…â–‡â–…â–†â–…â–†â–ƒâ–ƒâ–†â–ˆ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–†â–„â–ƒâ–„â–ƒâ–‡â–„â–â–ƒâ–‡â–„â–ƒâ–ƒâ–…â–„â–ƒâ–…â–ˆâ–‡â–†â–„â–ƒâ–…â–†â–„â–…â–†â–…â–ƒâ–…â–‡â–„â–„â–„â–†â–…â–‡â–…â–‚â–„
wandb:                      train_acc â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–‡â–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–‚â–‚â–
wandb:                    train_kappa â–â–‚â–‚â–ƒâ–„â–…â–†â–†â–‡â–ˆâ–ˆ
wandb:            trainer/global_step â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–ƒâ–â–ƒâ–„â–…â–…â–‡â–ˆâ–ˆâ–†â–ˆ
wandb:                    val_ce_loss â–â–â–â–â–â–‚â–‚â–ƒâ–…â–ˆâ–…
wandb:                      val_kappa â–‚â–â–ƒâ–ƒâ–…â–…â–†â–‡â–ˆâ–…â–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.39733
wandb:                  test_acc_step 0.32143
wandb:             test_ce_loss_epoch 2.15234
wandb:              test_ce_loss_step 2.65158
wandb:               test_kappa_epoch 0.16555
wandb:                test_kappa_step 0.11066
wandb:                      train_acc 0.97802
wandb:                  train_ce_loss 0.08316
wandb:                    train_kappa 0.9739
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.40963
wandb:                    val_ce_loss 2.13601
wandb:                      val_kappa 0.18959
wandb: 
wandb: ğŸš€ View run worldly-sweep-15 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/e6etpsra
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_205559-e6etpsra/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: kitmgk1o with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 6.247947040549693e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.4684561490620074e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_210135-kitmgk1o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-16
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/kitmgk1o
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.104
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.104. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:27 â€¢       13.97it/s v_num: gk1o      
                                     0:00:00                   val_ce_loss:     
                                                               2.203 val_kappa: 
                                                               0.181 val_acc:   
                                                               0.403            
                                                               train_ce_loss:   
                                                               0.082            
                                                               train_kappa:     
                                                               0.971 train_acc: 
                                                               0.978            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.39800000190734863    â”‚
â”‚    test_ce_loss_epoch     â”‚    2.2290618419647217     â”‚
â”‚     test_kappa_epoch      â”‚    0.16580551862716675    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 13.89it/s 
wandb: uploading history steps 102-102, summary; updating run config
wandb: uploading output.log; uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–â–„â–…â–…â–„â–ƒâ–‡â–â–‚â–„â–ƒâ–„â–ƒâ–ƒâ–â–ˆâ–„â–†â–ˆâ–‡â–†â–„â–ƒâ–ƒâ–†â–‡â–ƒâ–…â–ˆâ–„â–†â–…â–†â–‚â–…â–†â–†â–ƒâ–‚â–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–…â–…â–ƒâ–†â–†â–†â–„â–†â–†â–†â–…â–†â–…â–ˆâ–ˆâ–…â–„â–ƒâ–â–ƒâ–…â–†â–„â–…â–‚â–ƒâ–ˆâ–„â–â–…â–‡â–ƒâ–„â–„â–…â–ƒâ–„â–„â–ˆâ–…
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–„â–†â–†â–‡â–…â–…â–†â–â–†â–…â–†â–…â–ƒâ–‚â–‚â–†â–„â–…â–‡â–†â–…â–„â–„â–„â–‡â–‡â–‚â–‡â–ˆâ–ˆâ–†â–ˆâ–†â–â–†â–ˆâ–„â–„â–ƒâ–…
wandb:                      train_acc â–â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–‚â–‚â–
wandb:                    train_kappa â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–‡â–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–‚â–â–‚â–ƒâ–…â–„â–…â–â–ˆâ–†
wandb:                    val_ce_loss â–â–â–â–â–â–‚â–‚â–„â–ˆâ–ƒâ–„
wandb:                      val_kappa â–â–â–â–‚â–„â–ƒâ–…â–„â–â–ˆâ–†
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.398
wandb:                  test_acc_step 0.41071
wandb:             test_ce_loss_epoch 2.22906
wandb:              test_ce_loss_step 2.25371
wandb:               test_kappa_epoch 0.16581
wandb:                test_kappa_step 0.17027
wandb:                      train_acc 0.9777
wandb:                  train_ce_loss 0.08204
wandb:                    train_kappa 0.97135
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.40259
wandb:                    val_ce_loss 2.20253
wandb:                      val_kappa 0.18085
wandb: 
wandb: ğŸš€ View run gentle-sweep-16 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/kitmgk1o
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_210135-kitmgk1o/logs
wandb: Agent Starting Run: 8ekmhdek with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 0.0001042018583892548
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.722811200356592e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_210802-8ekmhdek
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-17
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/8ekmhdek
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.103
Metric val_ce_loss improved by 0.066 >= min_delta = 0.0001. New best score: 1.037
Metric val_ce_loss improved by 0.064 >= min_delta = 0.0001. New best score: 0.973
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 0.973. Signaling Trainer to stop.
Epoch 21/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:24 â€¢       15.89it/s v_num: hdek      
                                     0:00:00                   val_ce_loss:     
                                                               2.041 val_kappa: 
                                                               0.595 val_acc:   
                                                               0.619            
                                                               train_ce_loss:   
                                                               0.074            
                                                               train_kappa:     
                                                               0.971 train_acc: 
                                                               0.973            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚     0.609333336353302     â”‚
â”‚    test_ce_loss_epoch     â”‚    1.9919058084487915     â”‚
â”‚     test_kappa_epoch      â”‚    0.5691580772399902     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:04 â€¢ 0:00:00 10.45it/s 
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–‡â–…â–…â–…â–…â–…â–ˆâ–ƒâ–ƒâ–…â–ƒâ–ƒâ–…â–‚â–‡â–â–†â–…â–†â–‡â–†â–‡â–‡â–ƒâ–„â–…â–ƒâ–„â–‚â–‚â–…â–…â–…â–†â–‡â–ƒâ–‡â–‡â–…â–ˆ
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–‚â–†â–‚â–„â–„â–‚â–‚â–†â–†â–†â–†â–…â–ƒâ–†â–ƒâ–ˆâ–ƒâ–„â–‚â–â–‚â–‚â–‚â–…â–…â–‚â–†â–„â–…â–…â–ƒâ–†â–…â–ƒâ–ƒâ–‡â–‚â–‚â–ƒâ–‚
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–ˆâ–„â–ƒâ–†â–…â–„â–‡â–ƒâ–†â–‡â–…â–„â–…â–ƒâ–†â–‚â–„â–…â–†â–‡â–†â–ˆâ–ƒâ–ƒâ–…â–†â–ƒâ–†â–ƒâ–â–…â–…â–…â–†â–…â–„â–†â–†â–…â–ˆ
wandb:                      train_acc â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–
wandb:                    train_kappa â–â–â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–
wandb:                        val_acc â–â–â–â–‚â–‚â–â–â–„â–ƒâ–…â–â–‡â–‚â–ƒâ–ƒâ–†â–ˆâ–…â–ˆâ–‚â–†â–ˆ
wandb:                    val_ce_loss â–â–â–â–â–â–‚â–ƒâ–â–‚â–â–ˆâ–â–‚â–‚â–„â–‚â–â–‚â–â–‡â–‚â–‚
wandb:                      val_kappa â–â–â–â–‚â–‚â–â–â–„â–‚â–†â–â–‡â–‚â–ƒâ–ƒâ–‡â–ˆâ–…â–ˆâ–‚â–†â–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 22
wandb: exponential_decay_lr_scheduler 8e-05
wandb:                 test_acc_epoch 0.60933
wandb:                  test_acc_step 0.69643
wandb:             test_ce_loss_epoch 1.99191
wandb:              test_ce_loss_step 1.64027
wandb:               test_kappa_epoch 0.56916
wandb:                test_kappa_step 0.7615
wandb:                      train_acc 0.97329
wandb:                  train_ce_loss 0.07374
wandb:                    train_kappa 0.9711
wandb:            trainer/global_step 4180
wandb:                        val_acc 0.61852
wandb:                    val_ce_loss 2.04077
wandb:                      val_kappa 0.59515
wandb: 
wandb: ğŸš€ View run royal-sweep-17 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/8ekmhdek
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_210802-8ekmhdek/logs
wandb: Agent Starting Run: oo8yf2w3 with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 6.970853779207268e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.6212850414965646e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_211901-oo8yf2w3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-18
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/oo8yf2w3
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.110
Metric val_ce_loss improved by 0.010 >= min_delta = 0.0001. New best score: 1.100
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.100. Signaling Trainer to stop.
Epoch 11/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:23 â€¢       16.40it/s v_num: f2w3      
                                     0:00:00                   val_ce_loss:     
                                                               2.235 val_kappa: 
                                                               0.166 val_acc:   
                                                               0.411            
                                                               train_ce_loss:   
                                                               0.100            
                                                               train_kappa:     
                                                               0.965 train_acc: 
                                                               0.970            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.4090000092983246     â”‚
â”‚    test_ce_loss_epoch     â”‚    2.2609431743621826     â”‚
â”‚     test_kappa_epoch      â”‚    0.16526609659194946    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 15.38it/s 
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–…â–‡â–…â–…â–…â–†â–…â–…â–…â–†â–‚â–†â–„â–†â–ƒâ–ƒâ–…â–†â–†â–…â–ˆâ–ƒâ–‡â–ƒâ–†â–…â–ƒâ–†â–†â–…â–…â–ƒâ–â–†â–„â–ƒâ–…â–ˆâ–ˆâ–…
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–ƒâ–‚â–…â–â–„â–‚â–‚â–„â–‚â–„â–…â–‚â–†â–ƒâ–…â–ˆâ–„â–‚â–ƒâ–ƒâ–‚â–„â–â–„â–‚â–‚â–…â–ƒâ–ƒâ–„â–„â–†â–…â–„â–ƒâ–…â–„â–‚â–‚â–ƒ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–ˆâ–†â–†â–‡â–‡â–‡â–…â–…â–…â–†â–â–…â–…â–†â–‚â–…â–„â–…â–ˆâ–‡â–‡â–‚â–ˆâ–ƒâ–…â–…â–ƒâ–…â–†â–ƒâ–…â–ƒâ–‚â–…â–â–†â–…â–†â–‡â–†
wandb:                      train_acc â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–‡â–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–‚â–â–
wandb:                    train_kappa â–â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–ˆ
wandb:                        val_acc â–â–â–ƒâ–ƒâ–‚â–…â–‚â–†â–…â–†â–†â–ˆ
wandb:                    val_ce_loss â–â–â–â–â–‚â–â–‚â–„â–„â–†â–ˆâ–†
wandb:                      val_kappa â–â–‚â–‚â–ƒâ–‚â–…â–ƒâ–‡â–†â–…â–†â–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.409
wandb:                  test_acc_step 0.42857
wandb:             test_ce_loss_epoch 2.26094
wandb:              test_ce_loss_step 2.18406
wandb:               test_kappa_epoch 0.16527
wandb:                test_kappa_step 0.24904
wandb:                      train_acc 0.97041
wandb:                  train_ce_loss 0.10007
wandb:                    train_kappa 0.96497
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.41111
wandb:                    val_ce_loss 2.23526
wandb:                      val_kappa 0.16636
wandb: 
wandb: ğŸš€ View run misunderstood-sweep-18 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/oo8yf2w3
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_211901-oo8yf2w3/logs
wandb: Agent Starting Run: 4o3hucgw with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.631844029815053e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.714973528107466e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_212453-4o3hucgw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-19
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/4o3hucgw
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.113
Metric val_ce_loss improved by 0.011 >= min_delta = 0.0001. New best score: 1.102
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.102. Signaling Trainer to stop.
Epoch 11/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:23 â€¢       16.56it/s v_num: ucgw      
                                     0:00:00                   val_ce_loss:     
                                                               2.334 val_kappa: 
                                                               0.189 val_acc:   
                                                               0.401            
                                                               train_ce_loss:   
                                                               0.028            
                                                               train_kappa:     
                                                               0.996 train_acc: 
                                                               0.996            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.41200000047683716    â”‚
â”‚    test_ce_loss_epoch     â”‚    2.2844250202178955     â”‚
â”‚     test_kappa_epoch      â”‚    0.18939930200576782    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 13.54it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–‡â–‡â–…â–„â–„â–†â–„â–…â–â–ƒâ–ƒâ–„â–„â–‚â–…â–â–ƒâ–‚â–ˆâ–„â–„â–ƒâ–ƒâ–ƒâ–‡â–‡â–ƒâ–‡â–„â–ƒâ–…â–ƒâ–â–‚â–…â–†â–‚â–‡â–„â–…
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–ƒâ–‚â–„â–„â–„â–‚â–‡â–†â–†â–†â–†â–†â–†â–‡â–…â–ˆâ–ˆâ–†â–â–†â–…â–†â–…â–‡â–…â–„â–‡â–ƒâ–…â–‡â–ƒâ–†â–†â–‡â–†â–…â–…â–ƒâ–…â–ƒ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–‡â–‡â–†â–„â–ƒâ–‡â–ƒâ–†â–ƒâ–†â–†â–ƒâ–ƒâ–â–†â–ƒâ–…â–ƒâ–‡â–‚â–…â–„â–â–‚â–ˆâ–†â–‚â–†â–†â–‚â–…â–„â–â–â–†â–„â–â–‡â–„â–†
wandb:                      train_acc â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–ƒâ–‚â–‚â–â–
wandb:                    train_kappa â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–ˆ
wandb:                        val_acc â–â–ƒâ–ƒâ–ƒâ–‡â–†â–†â–ƒâ–†â–‡â–„â–ˆ
wandb:                    val_ce_loss â–â–â–â–‚â–â–ƒâ–ƒâ–ˆâ–…â–ˆâ–ˆâ–†
wandb:                      val_kappa â–â–‚â–ƒâ–ƒâ–†â–…â–‡â–„â–‡â–†â–†â–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.412
wandb:                  test_acc_step 0.44643
wandb:             test_ce_loss_epoch 2.28443
wandb:              test_ce_loss_step 1.73704
wandb:               test_kappa_epoch 0.1894
wandb:                test_kappa_step 0.29032
wandb:                      train_acc 0.99572
wandb:                  train_ce_loss 0.02773
wandb:                    train_kappa 0.99559
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.40148
wandb:                    val_ce_loss 2.33386
wandb:                      val_kappa 0.18909
wandb: 
wandb: ğŸš€ View run absurd-sweep-19 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/4o3hucgw
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_212453-4o3hucgw/logs
wandb: Agent Starting Run: he52a58p with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 7.284453261477594e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 3.938446311272622e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_213047-he52a58p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-20
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/he52a58p
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.129
Metric val_ce_loss improved by 0.030 >= min_delta = 0.0001. New best score: 1.098
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.098. Signaling Trainer to stop.
Epoch 11/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:22 â€¢       17.10it/s v_num: a58p      
                                     0:00:00                   val_ce_loss:     
                                                               2.395 val_kappa: 
                                                               0.076 val_acc:   
                                                               0.392            
                                                               train_ce_loss:   
                                                               0.454            
                                                               train_kappa:     
                                                               0.793 train_acc: 
                                                               0.814            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.3889999985694885     â”‚
â”‚    test_ce_loss_epoch     â”‚    2.5066771507263184     â”‚
â”‚     test_kappa_epoch      â”‚    0.08062863349914551    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:02 â€¢ 0:00:00 17.85it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–†â–†â–…â–‡â–†â–„â–â–‚â–†â–„â–ƒâ–‚â–ƒâ–ˆâ–†â–†â–…â–†â–„â–„â–â–‚â–ƒâ–†â–ˆâ–â–ƒâ–…â–„â–‚â–‚â–†â–ƒâ–ˆâ–ƒâ–‡â–â–‚â–ƒâ–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–„â–‚â–ƒâ–ƒâ–ƒâ–…â–ˆâ–…â–ƒâ–…â–…â–…â–†â–…â–…â–‚â–„â–ƒâ–†â–„â–†â–‡â–…â–ƒâ–â–„â–†â–ƒâ–†â–†â–†â–‚â–„â–ƒâ–†â–‚â–†â–„â–†â–…
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–†â–…â–…â–†â–‚â–‡â–ƒâ–…â–„â–…â–…â–„â–‚â–‡â–ˆâ–†â–‡â–ˆâ–‡â–‡â–„â–‚â–ƒâ–ƒâ–ˆâ–„â–â–‚â–†â–‡â–‚â–„â–ƒâ–‡â–†â–†â–†â–‚â–„â–ƒ
wandb:                      train_acc â–â–â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–ƒâ–‚â–
wandb:                    train_kappa â–â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb:            trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–‚â–‚â–â–â–â–‚â–‚â–ˆâ–†â–…â–ƒ
wandb:                    val_ce_loss â–â–â–â–‚â–„â–ˆâ–ƒâ–…â–â–ƒâ–ƒâ–†
wandb:                      val_kappa â–â–‚â–‚â–â–â–‚â–ƒâ–‚â–ˆâ–…â–†â–ƒ
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 7e-05
wandb:                 test_acc_epoch 0.389
wandb:                  test_acc_step 0.375
wandb:             test_ce_loss_epoch 2.50668
wandb:              test_ce_loss_step 2.63223
wandb:               test_kappa_epoch 0.08063
wandb:                test_kappa_step -0.06233
wandb:                      train_acc 0.81436
wandb:                  train_ce_loss 0.45398
wandb:                    train_kappa 0.79305
wandb:            trainer/global_step 2280
wandb:                        val_acc 0.39185
wandb:                    val_ce_loss 2.39454
wandb:                      val_kappa 0.076
wandb: 
wandb: ğŸš€ View run earnest-sweep-20 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/he52a58p
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_213047-he52a58p/logs
wandb: Agent Starting Run: ay1a8cgp with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 8.381574010550002e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.7094304355853094e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_213641-ay1a8cgp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-21
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ay1a8cgp
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.112
Metric val_ce_loss improved by 0.005 >= min_delta = 0.0001. New best score: 1.107
Metric val_ce_loss improved by 0.005 >= min_delta = 0.0001. New best score: 1.102
Metric val_ce_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.101
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.101. Signaling Trainer to stop.
Epoch 13/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:24 â€¢       15.58it/s v_num: 8cgp      
                                     0:00:00                   val_ce_loss:     
                                                               1.830 val_kappa: 
                                                               0.259 val_acc:   
                                                               0.456            
                                                               train_ce_loss:   
                                                               0.303            
                                                               train_kappa:     
                                                               0.873 train_acc: 
                                                               0.882            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.45133334398269653    â”‚
â”‚    test_ce_loss_epoch     â”‚    1.8351728916168213     â”‚
â”‚     test_kappa_epoch      â”‚    0.23670828342437744    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 13.25it/s 
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–…â–†â–…â–…â–â–…â–…â–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–ƒâ–ˆâ–„â–ƒâ–…â–‡â–„â–ƒâ–…â–…â–„â–…â–†â–…â–…â–…â–‡â–„â–…â–„â–„â–†â–„â–„â–‡â–ƒâ–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–ƒâ–‚â–‚â–ƒâ–‡â–ƒâ–â–„â–„â–†â–…â–…â–ƒâ–…â–‚â–ƒâ–„â–ƒâ–â–„â–…â–†â–…â–ƒâ–ƒâ–â–„â–‚â–…â–„â–†â–ƒâ–„â–ƒâ–‚â–‚â–„â–‚â–ˆâ–„
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–„â–…â–…â–„â–â–†â–…â–ƒâ–…â–†â–…â–…â–…â–ƒâ–ˆâ–…â–‚â–…â–‡â–…â–ƒâ–„â–„â–„â–‡â–ˆâ–ƒâ–†â–ƒâ–‡â–…â–…â–ƒâ–„â–…â–†â–…â–‡â–„â–…
wandb:                      train_acc â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–†â–‡â–‡â–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–„â–ƒâ–‚â–
wandb:                    train_kappa â–â–â–â–‚â–‚â–ƒâ–„â–„â–…â–†â–†â–‡â–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–ˆ
wandb:                        val_acc â–‚â–â–‚â–ƒâ–â–â–‚â–â–…â–†â–ˆâ–„â–ƒâ–†
wandb:                    val_ce_loss â–â–â–â–â–‚â–„â–†â–ˆâ–‚â–â–‚â–†â–†â–ƒ
wandb:                      val_kappa â–‚â–‚â–‚â–ƒâ–â–â–â–â–„â–‡â–ˆâ–„â–ƒâ–‡
wandb: 
wandb: Run summary:
wandb:                          epoch 14
wandb: exponential_decay_lr_scheduler 7e-05
wandb:                 test_acc_epoch 0.45133
wandb:                  test_acc_step 0.42857
wandb:             test_ce_loss_epoch 1.83517
wandb:              test_ce_loss_step 1.94321
wandb:               test_kappa_epoch 0.23671
wandb:                test_kappa_step 0.24841
wandb:                      train_acc 0.88152
wandb:                  train_ce_loss 0.30322
wandb:                    train_kappa 0.87305
wandb:            trainer/global_step 2660
wandb:                        val_acc 0.45593
wandb:                    val_ce_loss 1.83033
wandb:                      val_kappa 0.25942
wandb: 
wandb: ğŸš€ View run flowing-sweep-21 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ay1a8cgp
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_213641-ay1a8cgp/logs
wandb: Agent Starting Run: iydwkqt5 with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 0.00019803304466861824
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 4.611888825225099e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_214349-iydwkqt5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-22
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/iydwkqt5
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.104
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.104. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:25 â€¢       15.40it/s v_num: kqt5      
                                     0:00:00                   val_ce_loss:     
                                                               1.150 val_kappa: 
                                                               0.373 val_acc:   
                                                               0.511            
                                                               train_ce_loss:   
                                                               0.735            
                                                               train_kappa:     
                                                               0.593 train_acc: 
                                                               0.662            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.5120000243186951     â”‚
â”‚    test_ce_loss_epoch     â”‚     1.129611611366272     â”‚
â”‚     test_kappa_epoch      â”‚    0.37357258796691895    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 13.83it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–†â–‚â–„â–‡â–„â–…â–ˆâ–„â–‚â–…â–„â–…â–ƒâ–…â–†â–‚â–â–…â–‡â–‡â–…â–‡â–…â–ƒâ–„â–‡â–„â–ƒâ–…â–…â–ˆâ–…â–„â–…â–†â–ƒâ–ˆâ–…â–†â–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–‚â–†â–…â–…â–…â–„â–‚â–…â–‡â–ƒâ–ƒâ–„â–†â–†â–‚â–…â–ˆâ–…â–‚â–ƒâ–„â–‚â–†â–…â–„â–â–…â–…â–…â–…â–‚â–„â–…â–„â–„â–…â–â–ƒâ–‚â–ƒ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–†â–ƒâ–ƒâ–ˆâ–…â–„â–†â–ƒâ–„â–„â–„â–„â–‚â–…â–…â–ƒâ–â–…â–…â–†â–„â–…â–ƒâ–„â–…â–†â–„â–„â–„â–„â–†â–…â–„â–„â–„â–…â–†â–„â–†â–„
wandb:                      train_acc â–â–â–â–‚â–‚â–„â–…â–†â–‡â–‡â–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–…â–„â–ƒâ–‚â–
wandb:                    train_kappa â–â–â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–‚â–â–â–‚â–â–„â–ƒâ–‚â–ƒâ–ˆ
wandb:                    val_ce_loss â–â–â–â–â–â–†â–ƒâ–„â–ˆâ–…â–
wandb:                      val_kappa â–â–‚â–‚â–â–‚â–â–‚â–ƒâ–‚â–‚â–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 0.00018
wandb:                 test_acc_epoch 0.512
wandb:                  test_acc_step 0.48214
wandb:             test_ce_loss_epoch 1.12961
wandb:              test_ce_loss_step 1.0229
wandb:               test_kappa_epoch 0.37357
wandb:                test_kappa_step 0.35012
wandb:                      train_acc 0.66243
wandb:                  train_ce_loss 0.73475
wandb:                    train_kappa 0.59284
wandb:            trainer/global_step 2090
wandb:                        val_acc 0.51148
wandb:                    val_ce_loss 1.15026
wandb:                      val_kappa 0.37262
wandb: 
wandb: ğŸš€ View run golden-sweep-22 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/iydwkqt5
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_214349-iydwkqt5/logs
wandb: Agent Starting Run: h6dwz4j2 with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.221695281640076e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.530389989451133e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_214920-h6dwz4j2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-23
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/h6dwz4j2
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.117
Metric val_ce_loss improved by 0.014 >= min_delta = 0.0001. New best score: 1.102
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.102. Signaling Trainer to stop.
Epoch 11/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:24 â€¢       16.02it/s v_num: z4j2      
                                     0:00:00                   val_ce_loss:     
                                                               2.361 val_kappa: 
                                                               0.132 val_acc:   
                                                               0.385            
                                                               train_ce_loss:   
                                                               0.013            
                                                               train_kappa:     
                                                               0.999 train_acc: 
                                                               1.000            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.39633333683013916    â”‚
â”‚    test_ce_loss_epoch     â”‚     2.310173511505127     â”‚
â”‚     test_kappa_epoch      â”‚    0.15408724546432495    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 12.95it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–†â–„â–ƒâ–…â–…â–†â–â–†â–ƒâ–ˆâ–‡â–‚â–ƒâ–‡â–‚â–„â–„â–‡â–ƒâ–…â–„â–‚â–†â–…â–„â–†â–…â–…â–ƒâ–ƒâ–„â–„â–‚â–„â–…â–„â–…â–ˆâ–…â–…
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–ƒâ–…â–†â–„â–‚â–ƒâ–ˆâ–ƒâ–†â–â–ƒâ–‡â–†â–…â–…â–†â–†â–â–„â–„â–…â–‡â–ƒâ–…â–„â–‚â–…â–‚â–ˆâ–†â–…â–ƒâ–ˆâ–†â–„â–‡â–„â–‚â–„â–…
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–‡â–„â–…â–…â–ƒâ–…â–â–†â–„â–ˆâ–…â–ƒâ–‚â–‡â–„â–„â–ƒâ–…â–„â–†â–‚â–„â–…â–…â–…â–…â–…â–‡â–„â–‚â–†â–„â–‚â–ƒâ–…â–ƒâ–†â–…â–†â–„
wandb:                      train_acc â–â–‚â–‚â–ƒâ–ƒâ–„â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–„â–ƒâ–‚â–â–â–
wandb:                    train_kappa â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–ƒâ–‚â–…â–†â–ˆâ–„â–…â–ˆâ–ˆâ–†â–‡
wandb:                    val_ce_loss â–â–â–â–â–‚â–‚â–†â–†â–ˆâ–‡â–‡â–‡
wandb:                      val_kappa â–â–ƒâ–â–„â–…â–‡â–„â–…â–‡â–ˆâ–†â–‡
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.39633
wandb:                  test_acc_step 0.42857
wandb:             test_ce_loss_epoch 2.31017
wandb:              test_ce_loss_step 2.38791
wandb:               test_kappa_epoch 0.15409
wandb:                test_kappa_step 0.13869
wandb:                      train_acc 0.99963
wandb:                  train_ce_loss 0.0135
wandb:                    train_kappa 0.99944
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.38481
wandb:                    val_ce_loss 2.36123
wandb:                      val_kappa 0.13204
wandb: 
wandb: ğŸš€ View run comic-sweep-23 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/h6dwz4j2
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_214920-h6dwz4j2/logs
wandb: Agent Starting Run: u8kc1hjg with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 6.77754411691963e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.5269983339321772e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_215526-u8kc1hjg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-24
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/u8kc1hjg
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.112
Metric val_ce_loss improved by 0.007 >= min_delta = 0.0001. New best score: 1.105
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.105. Signaling Trainer to stop.
Epoch 11/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:26 â€¢       14.49it/s v_num: 1hjg      
                                     0:00:00                   val_ce_loss:     
                                                               2.239 val_kappa: 
                                                               0.180 val_acc:   
                                                               0.424            
                                                               train_ce_loss:   
                                                               0.088            
                                                               train_kappa:     
                                                               0.969 train_acc: 
                                                               0.973            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.4216666519641876     â”‚
â”‚    test_ce_loss_epoch     â”‚    2.2660534381866455     â”‚
â”‚     test_kappa_epoch      â”‚    0.20585542917251587    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:02 â€¢ 0:00:00 18.45it/s 
wandb: uploading config.yaml
wandb: uploading history steps 110-159, summary, console lines 42-62
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–ƒâ–„â–…â–ƒâ–ƒâ–‡â–…â–„â–„â–…â–‚â–ƒâ–…â–„â–„â–†â–„â–â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–„â–„â–‚â–„â–‚â–ƒâ–‚â–…â–„â–â–…â–ƒâ–ˆâ–‡â–„â–‚
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–†â–‚â–…â–†â–…â–â–‚â–„â–„â–„â–ˆâ–…â–ƒâ–„â–†â–„â–†â–„â–ƒâ–†â–ˆâ–‡â–„â–†â–ƒâ–„â–„â–„â–‡â–‡â–…â–ƒâ–‡â–†â–„â–…â–â–‚â–‚â–…
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–ƒâ–…â–„â–„â–„â–†â–…â–…â–…â–„â–„â–…â–„â–„â–…â–…â–„â–‚â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–…â–…â–„â–…â–‚â–„â–ƒâ–„â–„â–â–ƒâ–„â–ˆâ–†â–ƒâ–‚
wandb:                      train_acc â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–‡â–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–ƒâ–‚â–â–
wandb:                    train_kappa â–â–‚â–‚â–‚â–ƒâ–„â–…â–†â–‡â–‡â–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:                        val_acc â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–…â–„â–‡â–ˆ
wandb:                    val_ce_loss â–â–â–â–â–â–„â–„â–‡â–ƒâ–ˆâ–…â–†
wandb:                      val_kappa â–â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–†â–…â–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:                          epoch 12
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.42167
wandb:                  test_acc_step 0.375
wandb:             test_ce_loss_epoch 2.26605
wandb:              test_ce_loss_step 2.44606
wandb:               test_kappa_epoch 0.20586
wandb:                test_kappa_step 0.01145
wandb:                      train_acc 0.97276
wandb:                  train_ce_loss 0.08789
wandb:                    train_kappa 0.96882
wandb:            trainer/global_step 1140
wandb:                        val_acc 0.42407
wandb:                    val_ce_loss 2.23886
wandb:                      val_kappa 0.17956
wandb: 
wandb: ğŸš€ View run easy-sweep-24 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/u8kc1hjg
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_215526-u8kc1hjg/logs
wandb: Agent Starting Run: cq60h2w5 with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 7.012535728784849e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.623693491129549e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_220133-cq60h2w5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-25
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/cq60h2w5
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.103
Metric val_ce_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.102
Metric val_ce_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.101
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.101. Signaling Trainer to stop.
Epoch 18/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:25 â€¢       15.09it/s v_num: h2w5      
                                     0:00:00                   val_ce_loss:     
                                                               4.361 val_kappa: 
                                                               0.152 val_acc:   
                                                               0.417            
                                                               train_ce_loss:   
                                                               0.047            
                                                               train_kappa:     
                                                               0.984 train_acc: 
                                                               0.985            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.42233332991600037    â”‚
â”‚    test_ce_loss_epoch     â”‚     4.306259632110596     â”‚
â”‚     test_kappa_epoch      â”‚    0.15314191579818726    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:02 â€¢ 0:00:00 15.68it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–†â–„â–ƒâ–…â–†â–†â–‡â–…â–ƒâ–‡â–„â–…â–„â–ƒâ–…â–â–‚â–†â–ˆâ–…â–…â–ˆâ–…â–â–‡â–‡â–„â–…â–„â–…â–ˆâ–†â–…â–†â–ˆâ–„â–‡â–†â–†â–…
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–ƒâ–†â–…â–„â–„â–‚â–„â–‚â–†â–„â–„â–…â–†â–„â–ƒâ–ˆâ–‡â–ƒâ–‚â–„â–ƒâ–‚â–…â–ˆâ–…â–ƒâ–„â–ƒâ–ƒâ–…â–‚â–…â–…â–„â–â–…â–‚â–„â–‚â–‚
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–…â–„â–„â–ˆâ–†â–†â–†â–„â–„â–‡â–„â–„â–„â–â–„â–‚â–‚â–‡â–…â–†â–„â–‡â–â–â–ˆâ–‡â–…â–„â–ƒâ–„â–ˆâ–†â–„â–„â–„â–…â–‡â–†â–„â–‡
wandb:                      train_acc â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–„â–ƒâ–‚â–‚â–â–â–â–
wandb:                    train_kappa â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–ˆ
wandb:                        val_acc â–â–‚â–â–‚â–â–â–„â–â–‡â–†â–‚â–‚â–„â–ˆâ–…â–ˆâ–‚â–†â–…
wandb:                    val_ce_loss â–â–â–â–â–â–‚â–â–‚â–â–‚â–…â–„â–„â–‚â–ƒâ–‚â–ˆâ–ƒâ–‡
wandb:                      val_kappa â–â–‚â–‚â–‚â–‚â–â–ƒâ–â–‡â–…â–‚â–‚â–„â–‡â–…â–ˆâ–ƒâ–†â–„
wandb: 
wandb: Run summary:
wandb:                          epoch 19
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.42233
wandb:                  test_acc_step 0.42857
wandb:             test_ce_loss_epoch 4.30626
wandb:              test_ce_loss_step 3.48945
wandb:               test_kappa_epoch 0.15314
wandb:                test_kappa_step 0.24684
wandb:                      train_acc 0.98535
wandb:                  train_ce_loss 0.04722
wandb:                    train_kappa 0.98364
wandb:            trainer/global_step 3610
wandb:                        val_acc 0.41704
wandb:                    val_ce_loss 4.36127
wandb:                      val_kappa 0.1525
wandb: 
wandb: ğŸš€ View run decent-sweep-25 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/cq60h2w5
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_220133-cq60h2w5/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: dkedlgey with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.0001432595983100201
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 4.094795798177227e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_221123-dkedlgey
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-26
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/dkedlgey
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.136
Metric val_ce_loss improved by 0.035 >= min_delta = 0.0001. New best score: 1.101
Metric val_ce_loss improved by 0.003 >= min_delta = 0.0001. New best score: 1.099
Metric val_ce_loss improved by 0.058 >= min_delta = 0.0001. New best score: 1.041
Metric val_ce_loss improved by 0.070 >= min_delta = 0.0001. New best score: 0.971
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 0.971. Signaling Trainer to stop.
Epoch 21/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:23 â€¢       16.88it/s v_num: lgey      
                                     0:00:00                   val_ce_loss:     
                                                               1.756 val_kappa: 
                                                               0.525 val_acc:   
                                                               0.603            
                                                               train_ce_loss:   
                                                               0.079            
                                                               train_kappa:     
                                                               0.970 train_acc: 
                                                               0.972            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.5996666550636292     â”‚
â”‚    test_ce_loss_epoch     â”‚     1.757861614227295     â”‚
â”‚     test_kappa_epoch      â”‚    0.5197272300720215     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 14.03it/s 
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–…â–‚â–‚â–†â–…â–…â–‚â–â–„â–„â–„â–‚â–‚â–‚â–„â–â–â–„â–ˆâ–…â–„â–…â–ƒâ–…â–…â–ƒâ–‚â–„â–„â–„â–„â–…â–ƒâ–…â–„â–„â–„â–„â–…â–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–„â–ˆâ–†â–‚â–…â–„â–‡â–‡â–„â–…â–„â–‡â–†â–‡â–„â–‡â–ˆâ–„â–â–ƒâ–„â–ƒâ–†â–„â–…â–…â–†â–…â–…â–…â–…â–„â–†â–ƒâ–„â–ƒâ–…â–„â–ƒâ–ƒ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–…â–â–ƒâ–‡â–„â–„â–ƒâ–ƒâ–…â–†â–…â–„â–ƒâ–â–…â–â–‚â–„â–ˆâ–‡â–ƒâ–‡â–â–ƒâ–…â–„â–ƒâ–†â–„â–„â–„â–…â–‚â–…â–…â–„â–„â–„â–…â–…
wandb:                      train_acc â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–
wandb:                    train_kappa â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–
wandb:                        val_acc â–â–â–â–‚â–‚â–â–‚â–…â–ƒâ–â–…â–‡â–ƒâ–ƒâ–‡â–‡â–„â–„â–ƒâ–ƒâ–ˆâ–ˆ
wandb:                    val_ce_loss â–â–â–â–â–â–‚â–â–â–â–ˆâ–‚â–â–ƒâ–ƒâ–‚â–‚â–…â–„â–…â–‡â–ƒâ–ƒ
wandb:                      val_kappa â–â–â–â–‚â–‚â–â–‚â–…â–„â–â–†â–‡â–ƒâ–„â–ˆâ–‡â–„â–„â–ƒâ–‚â–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 22
wandb: exponential_decay_lr_scheduler 0.00012
wandb:                 test_acc_epoch 0.59967
wandb:                  test_acc_step 0.60714
wandb:             test_ce_loss_epoch 1.75786
wandb:              test_ce_loss_step 1.26203
wandb:               test_kappa_epoch 0.51973
wandb:                test_kappa_step 0.59018
wandb:                      train_acc 0.97235
wandb:                  train_ce_loss 0.079
wandb:                    train_kappa 0.97049
wandb:            trainer/global_step 2090
wandb:                        val_acc 0.60259
wandb:                    val_ce_loss 1.75577
wandb:                      val_kappa 0.52517
wandb: 
wandb: ğŸš€ View run stellar-sweep-26 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/dkedlgey
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_221123-dkedlgey/logs
wandb: Agent Starting Run: l68b17hi with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.0001295646275903409
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 6.753310321054957e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_222206-l68b17hi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-27
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/l68b17hi
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.112
Metric val_ce_loss improved by 0.010 >= min_delta = 0.0001. New best score: 1.102
Metric val_ce_loss improved by 0.008 >= min_delta = 0.0001. New best score: 1.094
Metric val_ce_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.093
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.093. Signaling Trainer to stop.
Epoch 20/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:23 â€¢       16.45it/s v_num: 17hi      
                                     0:00:00                   val_ce_loss:     
                                                               2.711 val_kappa: 
                                                               0.407 val_acc:   
                                                               0.529            
                                                               train_ce_loss:   
                                                               0.046            
                                                               train_kappa:     
                                                               0.982 train_acc: 
                                                               0.985            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.5323333144187927     â”‚
â”‚    test_ce_loss_epoch     â”‚    2.6432738304138184     â”‚
â”‚     test_kappa_epoch      â”‚    0.41815441846847534    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 13.29it/s 
wandb: uploading output.log; uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–†â–„â–†â–„â–…â–†â–ˆâ–†â–‚â–…â–†â–…â–…â–â–†â–…â–ƒâ–ƒâ–ˆâ–†â–‡â–†â–…â–ƒâ–†â–†â–ƒâ–…â–ƒâ–…â–ˆâ–…â–†â–†â–‡â–ƒâ–‡â–ˆâ–„â–…
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–„â–…â–„â–…â–†â–ƒâ–ƒâ–„â–ˆâ–„â–„â–ƒâ–…â–†â–ƒâ–„â–†â–…â–‚â–ƒâ–…â–„â–„â–‡â–…â–â–†â–ƒâ–†â–ƒâ–„â–…â–„â–ƒâ–‚â–†â–â–ƒâ–„â–„
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–‡â–…â–†â–‡â–†â–‡â–‡â–‡â–…â–‡â–‡â–†â–…â–â–†â–†â–…â–…â–ˆâ–‡â–…â–‡â–„â–…â–‡â–‡â–…â–ˆâ–„â–…â–ˆâ–‡â–ˆâ–‡â–‡â–…â–‡â–ˆâ–„â–ˆ
wandb:                      train_acc â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–„â–ƒâ–‚â–‚â–‚â–â–â–â–
wandb:                    train_kappa â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–
wandb:                        val_acc â–â–â–‚â–â–‚â–‚â–‚â–â–ƒâ–ƒâ–†â–‚â–ƒâ–„â–ƒâ–‡â–‡â–…â–ˆâ–„â–‡
wandb:                    val_ce_loss â–â–â–â–â–â–â–‚â–ˆâ–‚â–‚â–â–…â–ƒâ–…â–†â–„â–ƒâ–‡â–ƒâ–†â–†
wandb:                      val_kappa â–â–‚â–‚â–â–‚â–â–‚â–â–ƒâ–„â–†â–‚â–ƒâ–„â–ƒâ–†â–‡â–†â–ˆâ–„â–‡
wandb: 
wandb: Run summary:
wandb:                          epoch 21
wandb: exponential_decay_lr_scheduler 0.00011
wandb:                 test_acc_epoch 0.53233
wandb:                  test_acc_step 0.53571
wandb:             test_ce_loss_epoch 2.64327
wandb:              test_ce_loss_step 2.51616
wandb:               test_kappa_epoch 0.41815
wandb:                test_kappa_step 0.55068
wandb:                      train_acc 0.98461
wandb:                  train_ce_loss 0.04593
wandb:                    train_kappa 0.98226
wandb:            trainer/global_step 1995
wandb:                        val_acc 0.52852
wandb:                    val_ce_loss 2.71132
wandb:                      val_kappa 0.40708
wandb: 
wandb: ğŸš€ View run usual-sweep-27 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/l68b17hi
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_222206-l68b17hi/logs
wandb: Agent Starting Run: v0lb0ezs with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 9.014620927120994e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 1.2112530011813313e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_223213-v0lb0ezs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-28
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/v0lb0ezs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.109
Metric val_ce_loss improved by 0.104 >= min_delta = 0.0001. New best score: 1.005
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.005. Signaling Trainer to stop.
Epoch 16/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:26 â€¢       14.38it/s v_num: 0ezs      
                                     0:00:00                   val_ce_loss:     
                                                               7.600 val_kappa: 
                                                               0.022 val_acc:   
                                                               0.344            
                                                               train_ce_loss:   
                                                               0.134            
                                                               train_kappa:     
                                                               0.948 train_acc: 
                                                               0.950            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.3466666638851166     â”‚
â”‚    test_ce_loss_epoch     â”‚     7.626334190368652     â”‚
â”‚     test_kappa_epoch      â”‚   0.028235316276550293    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:02 â€¢ 0:00:00 16.25it/s 
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–†â–…â–…â–‡â–‡â–ƒâ–ƒâ–…â–ˆâ–…â–†â–„â–„â–†â–ƒâ–†â–„â–…â–ƒâ–…â–„â–ƒâ–„â–‡â–†â–„â–…â–…â–„â–„â–ƒâ–…â–…â–…â–„â–‡â–â–„â–„â–…
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–„â–„â–‚â–â–ƒâ–†â–‡â–„â–‚â–…â–‚â–…â–†â–‚â–†â–â–…â–ƒâ–†â–†â–†â–ˆâ–†â–‚â–â–„â–…â–ƒâ–†â–…â–‡â–ƒâ–„â–ƒâ–ˆâ–â–ˆâ–„â–…â–ƒ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–†â–…â–†â–ˆâ–ƒâ–‡â–„â–„â–„â–…â–†â–‡â–ƒâ–†â–…â–ƒâ–‡â–…â–ˆâ–†â–ˆâ–ƒâ–‡â–„â–†â–…â–„â–ƒâ–…â–…â–†â–‡â–ˆâ–ƒâ–†â–†â–†â–â–‚â–†
wandb:                      train_acc â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–‚â–‚â–â–
wandb:                    train_kappa â–â–â–â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–‚â–â–â–‚â–ƒâ–â–†â–‚â–‚â–‚â–…â–ˆâ–ƒâ–‚â–ˆâ–„â–
wandb:                    val_ce_loss â–â–â–â–â–â–ƒâ–â–‚â–ƒâ–ƒâ–â–â–ƒâ–„â–‚â–„â–ˆ
wandb:                      val_kappa â–‚â–â–‚â–‚â–‚â–â–†â–‚â–‚â–ƒâ–†â–ˆâ–ƒâ–ƒâ–‡â–…â–‚
wandb: 
wandb: Run summary:
wandb:                          epoch 17
wandb: exponential_decay_lr_scheduler 8e-05
wandb:                 test_acc_epoch 0.34667
wandb:                  test_acc_step 0.35714
wandb:             test_ce_loss_epoch 7.62633
wandb:              test_ce_loss_step 6.90854
wandb:               test_kappa_epoch 0.02824
wandb:                test_kappa_step 0.04906
wandb:                      train_acc 0.95045
wandb:                  train_ce_loss 0.13437
wandb:                    train_kappa 0.94843
wandb:            trainer/global_step 3230
wandb:                        val_acc 0.3437
wandb:                    val_ce_loss 7.60019
wandb:                      val_kappa 0.02185
wandb: 
wandb: ğŸš€ View run vibrant-sweep-28 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/v0lb0ezs
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_223213-v0lb0ezs/logs
wandb: Agent Starting Run: tdy1z6po with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.00012455695319139057
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.5111629887345276e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_224034-tdy1z6po
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-29
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/tdy1z6po
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.106
Metric val_ce_loss improved by 0.020 >= min_delta = 0.0001. New best score: 1.086
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.086. Signaling Trainer to stop.
Epoch 18/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:28 â€¢       13.53it/s v_num: z6po      
                                     0:00:00                   val_ce_loss:     
                                                               1.953 val_kappa: 
                                                               0.369 val_acc:   
                                                               0.499            
                                                               train_ce_loss:   
                                                               0.111            
                                                               train_kappa:     
                                                               0.957 train_acc: 
                                                               0.961            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚     0.515333354473114     â”‚
â”‚    test_ce_loss_epoch     â”‚     1.848555326461792     â”‚
â”‚     test_kappa_epoch      â”‚    0.3839162588119507     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 13.80it/s 
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml; uploading history steps 176-229, summary, console lines 42-62
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–‡â–ƒâ–…â–…â–ƒâ–„â–…â–â–…â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–‡â–…â–ƒâ–ƒâ–„â–„â–„â–…â–…â–ƒâ–…â–„â–â–‚â–…â–â–ˆâ–ƒâ–…â–ƒâ–†â–…â–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–â–„â–ƒâ–‚â–†â–„â–„â–†â–â–†â–†â–ˆâ–ƒâ–‡â–„â–„â–…â–â–…â–„â–‡â–…â–„â–†â–ƒâ–ƒâ–†â–‚â–†â–ˆâ–ˆâ–‚â–‡â–â–…â–‚â–ƒâ–ƒâ–ƒâ–…
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–ˆâ–†â–†â–†â–†â–…â–†â–â–†â–„â–ƒâ–†â–‡â–ƒâ–…â–…â–„â–‡â–‡â–„â–…â–†â–„â–…â–‡â–‡â–…â–†â–„â–‚â–„â–‡â–ƒâ–‡â–…â–†â–†â–‡â–†â–†
wandb:                      train_acc â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–„â–„â–ƒâ–‚â–‚â–‚â–
wandb:                    train_kappa â–â–â–â–‚â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:                        val_acc â–‚â–â–‚â–‚â–‚â–â–ƒâ–ƒâ–‡â–ƒâ–‚â–ƒâ–†â–â–„â–‚â–†â–„â–ˆ
wandb:                    val_ce_loss â–â–â–â–â–â–â–â–â–â–‚â–ƒâ–„â–â–ˆâ–ƒâ–†â–‚â–…â–‚
wandb:                      val_kappa â–‚â–â–‚â–â–â–â–„â–ƒâ–‡â–ƒâ–‚â–‚â–‡â–â–„â–‚â–‡â–ƒâ–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 19
wandb: exponential_decay_lr_scheduler 0.0001
wandb:                 test_acc_epoch 0.51533
wandb:                  test_acc_step 0.53571
wandb:             test_ce_loss_epoch 1.84856
wandb:              test_ce_loss_step 1.90097
wandb:               test_kappa_epoch 0.38392
wandb:                test_kappa_step 0.45495
wandb:                      train_acc 0.96062
wandb:                  train_ce_loss 0.1108
wandb:                    train_kappa 0.95744
wandb:            trainer/global_step 1805
wandb:                        val_acc 0.49889
wandb:                    val_ce_loss 1.95294
wandb:                      val_kappa 0.3688
wandb: 
wandb: ğŸš€ View run revived-sweep-29 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/tdy1z6po
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_224034-tdy1z6po/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: omgu68q9 with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 9.371429241367074e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.651165348950609e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_225004-omgu68q9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-30
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/omgu68q9
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.111
Metric val_ce_loss improved by 0.008 >= min_delta = 0.0001. New best score: 1.103
Metric val_ce_loss improved by 0.002 >= min_delta = 0.0001. New best score: 1.101
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.101. Signaling Trainer to stop.
Epoch 13/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:26 â€¢       14.71it/s v_num: 68q9      
                                     0:00:00                   val_ce_loss:     
                                                               2.165 val_kappa: 
                                                               0.289 val_acc:   
                                                               0.474            
                                                               train_ce_loss:   
                                                               0.108            
                                                               train_kappa:     
                                                               0.960 train_acc: 
                                                               0.964            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.46566668152809143    â”‚
â”‚    test_ce_loss_epoch     â”‚     2.214128017425537     â”‚
â”‚     test_kappa_epoch      â”‚    0.27945882081985474    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:02 â€¢ 0:00:00 16.78it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–‡â–ƒâ–â–ƒâ–…â–‡â–†â–‚â–…â–ˆâ–‚â–…â–‚â–…â–â–‚â–„â–ƒâ–†â–…â–…â–…â–ˆâ–ƒâ–…â–‡â–‚â–…â–…â–ƒâ–‡â–ƒâ–ƒâ–…â–†â–„â–‚â–‡â–†â–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–â–†â–ˆâ–„â–†â–‚â–„â–‡â–„â–ƒâ–…â–†â–ˆâ–…â–ˆâ–†â–†â–ƒâ–ƒâ–†â–„â–„â–ƒâ–‡â–ƒâ–‚â–†â–„â–„â–…â–„â–†â–†â–ƒâ–„â–„â–‡â–â–ƒâ–…
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–‡â–…â–â–â–…â–‡â–†â–…â–ƒâ–ˆâ–„â–„â–„â–„â–‚â–ƒâ–ƒâ–„â–…â–…â–„â–†â–‡â–„â–…â–…â–…â–†â–„â–„â–‡â–…â–„â–„â–…â–„â–…â–†â–…â–…
wandb:                      train_acc â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–ƒâ–‚â–â–
wandb:                    train_kappa â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–â–‚â–„â–„â–‚â–‚â–‚â–‡â–ˆâ–ƒâ–‡â–‡â–ˆ
wandb:                    val_ce_loss â–â–â–â–â–â–‚â–ƒâ–…â–‚â–‚â–ˆâ–„â–…â–…
wandb:                      val_kappa â–â–â–‚â–„â–…â–â–‚â–‚â–‡â–ˆâ–‚â–‡â–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 14
wandb: exponential_decay_lr_scheduler 8e-05
wandb:                 test_acc_epoch 0.46567
wandb:                  test_acc_step 0.42857
wandb:             test_ce_loss_epoch 2.21413
wandb:              test_ce_loss_step 2.32682
wandb:               test_kappa_epoch 0.27946
wandb:                test_kappa_step 0.27407
wandb:                      train_acc 0.96383
wandb:                  train_ce_loss 0.10832
wandb:                    train_kappa 0.95998
wandb:            trainer/global_step 1330
wandb:                        val_acc 0.47444
wandb:                    val_ce_loss 2.16507
wandb:                      val_kappa 0.28863
wandb: 
wandb: ğŸš€ View run curious-sweep-30 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/omgu68q9
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_225004-omgu68q9/logs
wandb: Agent Starting Run: ancajt8k with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 0.00017261862351525368
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 6.692975125166259e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_225700-ancajt8k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-31
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ancajt8k
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.102
Metric val_ce_loss improved by 0.004 >= min_delta = 0.0001. New best score: 1.099
Metric val_ce_loss improved by 0.026 >= min_delta = 0.0001. New best score: 1.073
Metric val_ce_loss improved by 0.209 >= min_delta = 0.0001. New best score: 0.864
Metric val_ce_loss improved by 0.095 >= min_delta = 0.0001. New best score: 0.768
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 0.768. Signaling Trainer to stop.
Epoch 27/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:25 â€¢       14.97it/s v_num: jt8k      
                                     0:00:00                   val_ce_loss:     
                                                               3.705 val_kappa: 
                                                               0.358 val_acc:   
                                                               0.546            
                                                               train_ce_loss:   
                                                               0.120            
                                                               train_kappa:     
                                                               0.953 train_acc: 
                                                               0.954            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.5556666851043701     â”‚
â”‚    test_ce_loss_epoch     â”‚    3.5261361598968506     â”‚
â”‚     test_kappa_epoch      â”‚    0.3704336881637573     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 14.81it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–‡â–‚â–‚â–‡â–„â–‡â–‡â–ƒâ–‚â–†â–…â–…â–â–…â–…â–ƒâ–ƒâ–…â–ˆâ–…â–†â–‡â–ƒâ–†â–‡â–…â–ƒâ–‡â–ƒâ–…â–‡â–…â–†â–…â–ˆâ–„â–ˆâ–†â–‡â–‡
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–‚â–ˆâ–ƒâ–…â–ƒâ–„â–‚â–…â–…â–ƒâ–„â–…â–‡â–„â–…â–†â–…â–„â–â–‚â–ƒâ–‚â–…â–ƒâ–ƒâ–‚â–†â–‚â–…â–„â–‚â–„â–‚â–ƒâ–â–…â–‚â–ƒâ–â–‚
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–†â–ƒâ–…â–‡â–…â–†â–…â–ƒâ–…â–†â–…â–„â–‚â–ƒâ–„â–„â–ƒâ–†â–„â–…â–„â–†â–â–…â–ˆâ–…â–ƒâ–‡â–„â–…â–…â–…â–…â–…â–…â–„â–…â–„â–…â–ˆ
wandb:                      train_acc â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–
wandb:                    train_kappa â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–
wandb:                        val_acc â–â–â–â–â–â–‚â–â–â–â–â–â–†â–†â–â–…â–ƒâ–‚â–‡â–…â–…â–†â–…â–…â–…â–„â–ˆâ–†â–…
wandb:                    val_ce_loss â–â–â–â–‚â–â–â–‚â–ˆâ–‚â–„â–†â–â–â–†â–‚â–‚â–…â–â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–…â–‚â–‚â–…
wandb:                      val_kappa â–â–â–â–â–â–ƒâ–â–â–â–â–â–†â–‡â–â–…â–ƒâ–‚â–‡â–…â–…â–‡â–…â–†â–„â–„â–ˆâ–…â–…
wandb: 
wandb: Run summary:
wandb:                          epoch 28
wandb: exponential_decay_lr_scheduler 0.00013
wandb:                 test_acc_epoch 0.55567
wandb:                  test_acc_step 0.625
wandb:             test_ce_loss_epoch 3.52614
wandb:              test_ce_loss_step 2.95937
wandb:               test_kappa_epoch 0.37043
wandb:                test_kappa_step 0.61787
wandb:                      train_acc 0.9542
wandb:                  train_ce_loss 0.11977
wandb:                    train_kappa 0.95347
wandb:            trainer/global_step 5320
wandb:                        val_acc 0.5463
wandb:                    val_ce_loss 3.70516
wandb:                      val_kappa 0.35792
wandb: 
wandb: ğŸš€ View run dainty-sweep-31 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ancajt8k
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_225700-ancajt8k/logs
wandb: Agent Starting Run: 8jo3udht with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 8.138570702092895e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 4.007040775365062e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_231046-8jo3udht
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-32
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/8jo3udht
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.120
Metric val_ce_loss improved by 0.010 >= min_delta = 0.0001. New best score: 1.110
Metric val_ce_loss improved by 0.001 >= min_delta = 0.0001. New best score: 1.109
Metric val_ce_loss improved by 0.004 >= min_delta = 0.0001. New best score: 1.105
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.105. Signaling Trainer to stop.
Epoch 13/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:28 â€¢       13.85it/s v_num: udht      
                                     0:00:00                   val_ce_loss:     
                                                               2.050 val_kappa: 
                                                               0.369 val_acc:   
                                                               0.490            
                                                               train_ce_loss:   
                                                               0.369            
                                                               train_kappa:     
                                                               0.840 train_acc: 
                                                               0.853            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.5016666650772095     â”‚
â”‚    test_ce_loss_epoch     â”‚    1.9842861890792847     â”‚
â”‚     test_kappa_epoch      â”‚    0.3948584198951721     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 12.87it/s 
wandb: uploading output.log; uploading config.yaml
wandb: uploading history steps 132-181, summary, console lines 44-64
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–‡â–…â–‚â–„â–‚â–…â–‡â–ƒâ–‚â–†â–â–‡â–…â–â–‡â–ƒâ–„â–ƒâ–†â–…â–†â–†â–„â–‚â–…â–ˆâ–‚â–ƒâ–ƒâ–…â–†â–…â–ƒâ–ƒâ–‡â–â–‡â–ƒâ–‚â–‡
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–„â–…â–…â–…â–‡â–„â–ƒâ–‡â–ˆâ–…â–‡â–ƒâ–„â–‡â–ƒâ–‡â–…â–…â–‚â–…â–…â–ƒâ–„â–†â–‡â–„â–†â–†â–„â–„â–„â–…â–…â–…â–ƒâ–ˆâ–â–…â–…â–ƒ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–‡â–…â–â–…â–„â–„â–…â–„â–…â–‡â–ƒâ–†â–„â–‚â–†â–„â–ƒâ–„â–„â–…â–…â–„â–‚â–„â–†â–ˆâ–‚â–…â–‚â–„â–…â–…â–ƒâ–ƒâ–†â–ƒâ–„â–‚â–‚â–‡
wandb:                      train_acc â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–ƒâ–‚â–
wandb:                    train_kappa â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb:            trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–‚â–‚â–â–‚â–â–â–ƒâ–ƒâ–„â–‚â–ƒâ–ƒâ–ˆ
wandb:                    val_ce_loss â–â–â–â–â–â–‚â–„â–â–„â–ƒâ–…â–ˆâ–†â–ƒ
wandb:                      val_kappa â–â–‚â–â–â–‚â–â–‚â–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 14
wandb: exponential_decay_lr_scheduler 7e-05
wandb:                 test_acc_epoch 0.50167
wandb:                  test_acc_step 0.58929
wandb:             test_ce_loss_epoch 1.98429
wandb:              test_ce_loss_step 1.67481
wandb:               test_kappa_epoch 0.39486
wandb:                test_kappa_step 0.61326
wandb:                      train_acc 0.85342
wandb:                  train_ce_loss 0.36887
wandb:                    train_kappa 0.83976
wandb:            trainer/global_step 2660
wandb:                        val_acc 0.49037
wandb:                    val_ce_loss 2.05014
wandb:                      val_kappa 0.36928
wandb: 
wandb: ğŸš€ View run decent-sweep-32 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/8jo3udht
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_231046-8jo3udht/logs
wandb: Agent Starting Run: ynigfmpd with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.00017186391081894798
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.941812724686396e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_231750-ynigfmpd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-33
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ynigfmpd
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.110
Metric val_ce_loss improved by 0.002 >= min_delta = 0.0001. New best score: 1.108
Metric val_ce_loss improved by 0.045 >= min_delta = 0.0001. New best score: 1.063
Metric val_ce_loss improved by 0.012 >= min_delta = 0.0001. New best score: 1.051
Metric val_ce_loss improved by 0.177 >= min_delta = 0.0001. New best score: 0.874
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 0.874. Signaling Trainer to stop.
Epoch 22/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:23 â€¢       16.84it/s v_num: fmpd      
                                     0:00:00                   val_ce_loss:     
                                                               2.188 val_kappa: 
                                                               0.459 val_acc:   
                                                               0.561            
                                                               train_ce_loss:   
                                                               0.162            
                                                               train_kappa:     
                                                               0.931 train_acc: 
                                                               0.938            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.5596666932106018     â”‚
â”‚    test_ce_loss_epoch     â”‚    2.1479318141937256     â”‚
â”‚     test_kappa_epoch      â”‚    0.4551404118537903     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 15.35it/s 
wandb: uploading output.log; uploading config.yaml
wandb: uploading output.log
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–†â–‚â–ƒâ–…â–â–‡â–‡â–ƒâ–ƒâ–…â–„â–…â–‚â–‚â–…â–â–‚â–…â–„â–„â–…â–‡â–‚â–…â–ƒâ–…â–‚â–„â–„â–„â–…â–„â–ƒâ–‡â–…â–‚â–ˆâ–†â–„â–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–ƒâ–†â–„â–ƒâ–ˆâ–â–ƒâ–ˆâ–†â–„â–„â–…â–‡â–ˆâ–ƒâ–‡â–ˆâ–†â–‚â–…â–„â–‚â–ˆâ–…â–…â–‚â–†â–„â–†â–†â–„â–ƒâ–…â–â–…â–†â–‚â–ƒâ–„â–ƒ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–‡â–ƒâ–ƒâ–…â–ƒâ–‡â–†â–„â–…â–†â–…â–…â–â–‚â–…â–ƒâ–â–…â–„â–„â–„â–ˆâ–â–„â–…â–…â–‚â–…â–„â–„â–„â–…â–ƒâ–…â–„â–…â–†â–…â–‚â–…
wandb:                      train_acc â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–â–
wandb:                    train_kappa â–â–â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–â–â–â–‚â–„â–â–„â–…â–‚â–ƒâ–‚â–ˆâ–ˆâ–‚â–â–‚â–†â–„â–…â–â–†â–‡
wandb:                    val_ce_loss â–â–â–â–â–â–â–‚â–â–â–‚â–â–ƒâ–â–â–„â–‡â–„â–â–‚â–‚â–ˆâ–â–‚
wandb:                      val_kappa â–â–â–â–â–â–„â–â–„â–…â–‚â–„â–‚â–ˆâ–ˆâ–‚â–â–‚â–‡â–„â–„â–â–†â–‡
wandb: 
wandb: Run summary:
wandb:                          epoch 23
wandb: exponential_decay_lr_scheduler 0.00014
wandb:                 test_acc_epoch 0.55967
wandb:                  test_acc_step 0.55357
wandb:             test_ce_loss_epoch 2.14793
wandb:              test_ce_loss_step 1.74514
wandb:               test_kappa_epoch 0.45514
wandb:                test_kappa_step 0.49708
wandb:                      train_acc 0.93815
wandb:                  train_ce_loss 0.16229
wandb:                    train_kappa 0.93071
wandb:            trainer/global_step 2185
wandb:                        val_acc 0.56111
wandb:                    val_ce_loss 2.18787
wandb:                      val_kappa 0.4589
wandb: 
wandb: ğŸš€ View run olive-sweep-33 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/ynigfmpd
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_231750-ynigfmpd/logs
wandb: Agent Starting Run: m61m7zjr with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.00013276022787327656
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.551124074386431e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_232849-m61m7zjr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-34
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/m61m7zjr
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.122
Metric val_ce_loss improved by 0.011 >= min_delta = 0.0001. New best score: 1.112
Metric val_ce_loss improved by 0.006 >= min_delta = 0.0001. New best score: 1.105
Metric val_ce_loss improved by 0.009 >= min_delta = 0.0001. New best score: 1.096
Metric val_ce_loss improved by 0.023 >= min_delta = 0.0001. New best score: 1.073
Metric val_ce_loss improved by 0.051 >= min_delta = 0.0001. New best score: 1.022
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.022. Signaling Trainer to stop.
Epoch 21/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:24 â€¢       16.22it/s v_num: 7zjr      
                                     0:00:00                   val_ce_loss:     
                                                               4.503 val_kappa: 
                                                               0.138 val_acc:   
                                                               0.390            
                                                               train_ce_loss:   
                                                               0.088            
                                                               train_kappa:     
                                                               0.964 train_acc: 
                                                               0.968            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.40166667103767395    â”‚
â”‚    test_ce_loss_epoch     â”‚     4.426739692687988     â”‚
â”‚     test_kappa_epoch      â”‚    0.1524486541748047     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 15.15it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–†â–„â–…â–‡â–†â–ƒâ–ƒâ–…â–ˆâ–…â–†â–…â–ƒâ–…â–ƒâ–‡â–…â–†â–‚â–†â–…â–â–†â–‡â–‡â–„â–…â–‡â–„â–ƒâ–ƒâ–…â–†â–…â–ƒâ–‡â–â–…â–…â–‡
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–„â–ƒâ–‚â–ƒâ–„â–„â–†â–„â–â–„â–„â–„â–…â–…â–„â–â–ƒâ–‚â–†â–…â–…â–ˆâ–…â–ƒâ–â–„â–„â–‚â–ƒâ–…â–†â–„â–ƒâ–‚â–…â–‚â–ˆâ–‚â–„â–‚
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–…â–ƒâ–â–‡â–‚â–…â–„â–‚â–ƒâ–…â–ƒâ–†â–„â–‚â–…â–‡â–†â–†â–„â–†â–„â–ƒâ–†â–„â–…â–…â–„â–†â–ƒâ–„â–„â–…â–†â–…â–…â–„â–…â–„â–…â–ˆ
wandb:                      train_acc â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–…â–…â–„â–„â–ƒâ–‚â–‚â–‚â–â–â–â–
wandb:                    train_kappa â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–†â–‡â–‡â–‡â–ƒâ–‚â–ˆâ–ƒâ–ˆâ–†â–‡â–ƒ
wandb:                    val_ce_loss â–â–â–â–â–â–â–â–‚â–‚â–„â–â–â–â–â–„â–ˆâ–‚â–†â–‚â–„â–„â–‡
wandb:                      val_kappa â–â–â–â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–†â–‡â–‡â–‡â–‚â–‚â–ˆâ–ƒâ–ˆâ–…â–†â–ƒ
wandb: 
wandb: Run summary:
wandb:                          epoch 22
wandb: exponential_decay_lr_scheduler 0.00011
wandb:                 test_acc_epoch 0.40167
wandb:                  test_acc_step 0.48214
wandb:             test_ce_loss_epoch 4.42674
wandb:              test_ce_loss_step 3.76557
wandb:               test_kappa_epoch 0.15245
wandb:                test_kappa_step 0.37442
wandb:                      train_acc 0.96794
wandb:                  train_ce_loss 0.08841
wandb:                    train_kappa 0.96368
wandb:            trainer/global_step 2090
wandb:                        val_acc 0.39037
wandb:                    val_ce_loss 4.50342
wandb:                      val_kappa 0.13823
wandb: 
wandb: ğŸš€ View run glowing-sweep-34 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/m61m7zjr
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_232849-m61m7zjr/logs
wandb: Agent Starting Run: o8zm1mbb with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.00014768231449264118
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.909963570042533e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_233944-o8zm1mbb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-35
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/o8zm1mbb
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.105
Metric val_ce_loss improved by 0.002 >= min_delta = 0.0001. New best score: 1.103
Metric val_ce_loss improved by 0.006 >= min_delta = 0.0001. New best score: 1.097
Metric val_ce_loss improved by 0.028 >= min_delta = 0.0001. New best score: 1.069
Metric val_ce_loss improved by 0.050 >= min_delta = 0.0001. New best score: 1.018
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.018. Signaling Trainer to stop.
Epoch 23/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:24 â€¢       16.10it/s v_num: 1mbb      
                                     0:00:00                   val_ce_loss:     
                                                               3.343 val_kappa: 
                                                               0.268 val_acc:   
                                                               0.437            
                                                               train_ce_loss:   
                                                               0.100            
                                                               train_kappa:     
                                                               0.961 train_acc: 
                                                               0.964            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.44066667556762695    â”‚
â”‚    test_ce_loss_epoch     â”‚    3.3470253944396973     â”‚
â”‚     test_kappa_epoch      â”‚    0.25914859771728516    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 15.15it/s 
wandb: uploading history steps 226-275, summary, console lines 45-65; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–ˆâ–†â–…â–†â–†â–‚â–ƒâ–ƒâ–†â–„â–†â–ƒâ–ƒâ–†â–ƒâ–†â–ƒâ–…â–„â–ƒâ–‚â–ƒâ–„â–ƒâ–ˆâ–ƒâ–„â–‡â–â–„â–ƒâ–‚â–‚â–…â–…â–†â–â–†â–†â–…
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–‚â–†â–…â–‚â–ƒâ–„â–ˆâ–‡â–‚â–ƒâ–ƒâ–†â–‡â–„â–„â–‚â–‡â–†â–„â–†â–†â–†â–†â–†â–â–„â–‡â–‚â–‡â–†â–†â–„â–…â–„â–ƒâ–„â–†â–‚â–ƒâ–…
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–†â–†â–†â–…â–â–ƒâ–†â–‚â–â–…â–…â–„â–„â–ƒâ–†â–…â–„â–…â–…â–ƒâ–„â–…â–ƒâ–ƒâ–ˆâ–ƒâ–†â–‡â–â–…â–ƒâ–ƒâ–ƒâ–„â–‡â–…â–†â–â–…â–…
wandb:                      train_acc â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–
wandb:                    train_kappa â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–â–â–â–‚â–‚â–ƒâ–â–â–â–„â–‚â–ƒâ–‚â–†â–â–„â–ƒâ–‚â–ƒâ–†â–†â–‚â–ˆâ–ƒ
wandb:                    val_ce_loss â–â–â–â–â–â–â–…â–‚â–‚â–â–ƒâ–ƒâ–…â–â–ˆâ–‚â–ƒâ–ˆâ–ƒâ–‚â–‚â–…â–â–ƒ
wandb:                      val_kappa â–â–â–â–‚â–ƒâ–ƒâ–â–â–â–„â–â–â–‚â–†â–â–„â–ƒâ–‚â–„â–‡â–†â–‚â–ˆâ–„
wandb: 
wandb: Run summary:
wandb:                          epoch 24
wandb: exponential_decay_lr_scheduler 0.00012
wandb:                 test_acc_epoch 0.44067
wandb:                  test_acc_step 0.44643
wandb:             test_ce_loss_epoch 3.34703
wandb:              test_ce_loss_step 3.58185
wandb:               test_kappa_epoch 0.25915
wandb:                test_kappa_step 0.28313
wandb:                      train_acc 0.96416
wandb:                  train_ce_loss 0.09979
wandb:                    train_kappa 0.96122
wandb:            trainer/global_step 2280
wandb:                        val_acc 0.43667
wandb:                    val_ce_loss 3.34322
wandb:                      val_kappa 0.26808
wandb: 
wandb: ğŸš€ View run flowing-sweep-35 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/o8zm1mbb
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_233944-o8zm1mbb/logs
wandb: Agent Starting Run: 6eqdocvh with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 8.624288825036369e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 2.4550277851495488e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_235137-6eqdocvh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-36
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/6eqdocvh
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.109
Metric val_ce_loss improved by 0.007 >= min_delta = 0.0001. New best score: 1.102
Metric val_ce_loss improved by 0.004 >= min_delta = 0.0001. New best score: 1.098
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.098. Signaling Trainer to stop.
Epoch 14/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:25 â€¢       15.25it/s v_num: ocvh      
                                     0:00:00                   val_ce_loss:     
                                                               2.260 val_kappa: 
                                                               0.242 val_acc:   
                                                               0.443            
                                                               train_ce_loss:   
                                                               0.040            
                                                               train_kappa:     
                                                               0.987 train_acc: 
                                                               0.990            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.4416666626930237     â”‚
â”‚    test_ce_loss_epoch     â”‚     2.252209186553955     â”‚
â”‚     test_kappa_epoch      â”‚    0.23715978860855103    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 14.37it/s 
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–…â–„â–…â–‚â–ƒâ–ˆâ–‡â–†â–ƒâ–„â–‚â–â–‚â–ƒâ–„â–„â–ˆâ–†â–…â–‚â–…â–„â–ƒâ–…â–†â–‡â–â–†â–…â–‚â–‚â–†â–…â–…â–‡â–‚â–ˆâ–…â–…â–
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–â–„â–…â–ƒâ–ƒâ–â–ƒâ–…â–„â–„â–†â–†â–‡â–…â–†â–…â–„â–‚â–‚â–†â–‚â–…â–…â–‡â–ƒâ–ƒâ–‡â–‚â–â–…â–ˆâ–ƒâ–ƒâ–‚â–„â–…â–‚â–‚â–ƒâ–ˆ
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–†â–…â–†â–ƒâ–ƒâ–‡â–‡â–„â–„â–†â–‚â–‚â–â–„â–…â–‚â–†â–†â–†â–ƒâ–ƒâ–†â–„â–ƒâ–ˆâ–‡â–‚â–…â–…â–‚â–„â–†â–…â–ƒâ–†â–„â–‡â–†â–„â–„
wandb:                      train_acc â–â–â–‚â–‚â–‚â–ƒâ–„â–„â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–„â–ƒâ–‚â–‚â–â–
wandb:                    train_kappa â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–ˆ
wandb:                        val_acc â–‚â–â–ƒâ–ƒâ–„â–‚â–‚â–ƒâ–…â–‚â–ƒâ–†â–‚â–ˆâ–ˆ
wandb:                    val_ce_loss â–â–â–â–â–â–‚â–‚â–„â–‚â–ƒâ–†â–ƒâ–ˆâ–ƒâ–ƒ
wandb:                      val_kappa â–â–â–ƒâ–ƒâ–…â–ƒâ–ƒâ–ƒâ–…â–„â–ƒâ–†â–ƒâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 15
wandb: exponential_decay_lr_scheduler 7e-05
wandb:                 test_acc_epoch 0.44167
wandb:                  test_acc_step 0.33929
wandb:             test_ce_loss_epoch 2.25221
wandb:              test_ce_loss_step 3.06761
wandb:               test_kappa_epoch 0.23716
wandb:                test_kappa_step 0.22814
wandb:                      train_acc 0.98951
wandb:                  train_ce_loss 0.04025
wandb:                    train_kappa 0.98704
wandb:            trainer/global_step 1425
wandb:                        val_acc 0.44259
wandb:                    val_ce_loss 2.26047
wandb:                      val_kappa 0.24168
wandb: 
wandb: ğŸš€ View run olive-sweep-36 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/6eqdocvh
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_235137-6eqdocvh/logs
wandb: Agent Starting Run: mgj9hvlr with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 2
wandb: 	lr: 7.067110079456035e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 6.464561264195618e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250325_235904-mgj9hvlr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-37
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/mgj9hvlr
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.109
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.109. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:23 â€¢       16.52it/s v_num: hvlr      
                                     0:00:00                   val_ce_loss:     
                                                               2.305 val_kappa: 
                                                               0.121 val_acc:   
                                                               0.390            
                                                               train_ce_loss:   
                                                               0.589            
                                                               train_kappa:     
                                                               0.705 train_acc: 
                                                               0.750            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.3736666738986969     â”‚
â”‚    test_ce_loss_epoch     â”‚    2.3307645320892334     â”‚
â”‚     test_kappa_epoch      â”‚    0.11992263793945312    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 15.17it/s 
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–…â–„â–…â–…â–…â–„â–â–ƒâ–ˆâ–‡â–†â–…â–‚â–…â–ƒâ–†â–…â–„â–„â–…â–ƒâ–â–ƒâ–ˆâ–‡â–ƒâ–„â–…â–„â–„â–…â–„â–…â–ƒâ–‚â–†â–‚â–…â–ƒâ–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–„â–ƒâ–‚â–‚â–…â–„â–ˆâ–…â–‚â–„â–„â–†â–ƒâ–ƒâ–…â–â–ƒâ–ƒâ–„â–„â–…â–‡â–‡â–‚â–‚â–„â–†â–ƒâ–…â–ƒâ–…â–…â–„â–„â–†â–â–†â–‚â–†â–„
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–‡â–ƒâ–ƒâ–…â–ƒâ–…â–â–ƒâ–ƒâ–‡â–…â–†â–‚â–…â–…â–‚â–ˆâ–ƒâ–‡â–…â–„â–„â–‚â–…â–‡â–ƒâ–ƒâ–†â–…â–„â–‡â–ƒâ–„â–ƒâ–ƒâ–‡â–‡â–„â–‚â–†
wandb:                      train_acc â–â–â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–…â–„â–ƒâ–
wandb:                    train_kappa â–â–â–‚â–‚â–‚â–ƒâ–„â–…â–†â–‡â–ˆ
wandb:            trainer/global_step â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–
wandb:                        val_acc â–‚â–â–â–‚â–â–…â–‚â–„â–â–ˆâ–„
wandb:                    val_ce_loss â–â–â–â–â–â–â–‡â–‚â–ˆâ–‚â–†
wandb:                      val_kappa â–‚â–â–â–â–â–…â–‚â–ƒâ–â–ˆâ–„
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 6e-05
wandb:                 test_acc_epoch 0.37367
wandb:                  test_acc_step 0.375
wandb:             test_ce_loss_epoch 2.33076
wandb:              test_ce_loss_step 2.30493
wandb:               test_kappa_epoch 0.11992
wandb:                test_kappa_step 0.17883
wandb:                      train_acc 0.75016
wandb:                  train_ce_loss 0.58923
wandb:                    train_kappa 0.70531
wandb:            trainer/global_step 2090
wandb:                        val_acc 0.39037
wandb:                    val_ce_loss 2.30476
wandb:                      val_kappa 0.1207
wandb: 
wandb: ğŸš€ View run fine-sweep-37 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/mgj9hvlr
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_235904-mgj9hvlr/logs
wandb: Agent Starting Run: rqygzpaq with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 5.1851080740208175e-05
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 5.524819387086987e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250326_000440-rqygzpaq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-38
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/rqygzpaq
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.109
Monitored metric val_ce_loss did not improve in the last 10 records. Best score: 1.109. Signaling Trainer to stop.
Epoch 10/59 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380/380 0:00:26 â€¢       14.53it/s v_num: zpaq      
                                     0:00:00                   val_ce_loss:     
                                                               2.298 val_kappa: 
                                                               0.167 val_acc:   
                                                               0.394            
                                                               train_ce_loss:   
                                                               0.025            
                                                               train_kappa:     
                                                               0.997 train_acc: 
                                                               0.997            
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚      test_acc_epoch       â”‚    0.38199999928474426    â”‚
â”‚    test_ce_loss_epoch     â”‚     2.348148822784424     â”‚
â”‚     test_kappa_epoch      â”‚     0.145008385181427     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47/47 0:00:03 â€¢ 0:00:00 14.06it/s 
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: exponential_decay_lr_scheduler â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                 test_acc_epoch â–
wandb:                  test_acc_step â–†â–ƒâ–…â–†â–†â–ƒâ–…â–…â–ˆâ–…â–…â–…â–ƒâ–…â–†â–†â–â–†â–†â–ƒâ–†â–„â–†â–…â–„â–†â–‡â–ˆâ–…â–„â–…â–…â–…â–„â–„â–†â–ˆâ–…â–ƒâ–„
wandb:             test_ce_loss_epoch â–
wandb:              test_ce_loss_step â–†â–†â–…â–ƒâ–ƒâ–†â–ƒâ–…â–ƒâ–ƒâ–„â–‡â–ˆâ–†â–ƒâ–„â–ˆâ–ƒâ–ƒâ–‡â–…â–ˆâ–ƒâ–‡â–„â–…â–ƒâ–‚â–…â–…â–…â–…â–†â–‡â–†â–â–„â–†â–ƒâ–„
wandb:               test_kappa_epoch â–
wandb:                test_kappa_step â–†â–„â–‚â–„â–ƒâ–ƒâ–ƒâ–ƒâ–†â–†â–†â–ƒâ–ƒâ–…â–†â–ƒâ–â–…â–†â–…â–…â–ƒâ–ƒâ–ƒâ–…â–„â–†â–†â–…â–…â–…â–†â–„â–ƒâ–‚â–„â–ˆâ–ƒâ–„â–„
wandb:                      train_acc â–â–‚â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–ˆâ–ˆ
wandb:                  train_ce_loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–…â–„â–ƒâ–‚â–â–
wandb:                    train_kappa â–â–‚â–‚â–ƒâ–„â–…â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            trainer/global_step â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆ
wandb:                        val_acc â–â–ƒâ–‚â–ƒâ–…â–…â–„â–‡â–ˆâ–‡â–ˆ
wandb:                    val_ce_loss â–â–â–‚â–â–‚â–ƒâ–ˆâ–†â–†â–‡â–‡
wandb:                      val_kappa â–â–ƒâ–â–„â–„â–†â–ƒâ–†â–‡â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:                          epoch 11
wandb: exponential_decay_lr_scheduler 5e-05
wandb:                 test_acc_epoch 0.382
wandb:                  test_acc_step 0.35714
wandb:             test_ce_loss_epoch 2.34815
wandb:              test_ce_loss_step 2.14118
wandb:               test_kappa_epoch 0.14501
wandb:                test_kappa_step 0.07692
wandb:                      train_acc 0.99716
wandb:                  train_ce_loss 0.02537
wandb:                    train_kappa 0.99685
wandb:            trainer/global_step 1045
wandb:                        val_acc 0.39407
wandb:                    val_ce_loss 2.29794
wandb:                      val_kappa 0.16667
wandb: 
wandb: ğŸš€ View run effortless-sweep-38 at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/rqygzpaq
wandb: â­ï¸ View project at: https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250326_000440-rqygzpaq/logs
wandb: Agent Starting Run: qtqa8m3i with config:
wandb: 	BATCH_SIZE: 64
wandb: 	ImagenetModels_submodel: resnet18
wandb: 	accumulate_grad_batches: 4
wandb: 	lr: 0.0001178793039621521
wandb: 	should_finetune_model: False
wandb: 	weight_decay: 1.3314027877165238e-05
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /scratch/s_porwal_me.iitr/DeepLenseSubmissionProposal-2025/wandb/run-20250326_001014-qtqa8m3i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-39
wandb: â­ï¸ View project at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/sweeps/5e1jb5oe
wandb: ğŸš€ View run at https://wandb.ai/shri_krishna/DeepLenseClassificationSweep/runs/qtqa8m3i
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/s_porwal_me.iitr/miniconda3/envs/deeplense/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                    â”ƒ Type                      â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model_obj               â”‚ ImagenetModels            â”‚ 11.2 M â”‚
â”‚ 1  â”‚ model_obj.model         â”‚ ResNet                    â”‚ 11.2 M â”‚
â”‚ 2  â”‚ model_obj.model.conv1   â”‚ Conv2d                    â”‚  9.4 K â”‚
â”‚ 3  â”‚ model_obj.model.bn1     â”‚ BatchNorm2d               â”‚    128 â”‚
â”‚ 4  â”‚ model_obj.model.relu    â”‚ ReLU                      â”‚      0 â”‚
â”‚ 5  â”‚ model_obj.model.maxpool â”‚ MaxPool2d                 â”‚      0 â”‚
â”‚ 6  â”‚ model_obj.model.layer1  â”‚ Sequential                â”‚  147 K â”‚
â”‚ 7  â”‚ model_obj.model.layer2  â”‚ Sequential                â”‚  525 K â”‚
â”‚ 8  â”‚ model_obj.model.layer3  â”‚ Sequential                â”‚  2.1 M â”‚
â”‚ 9  â”‚ model_obj.model.layer4  â”‚ Sequential                â”‚  8.4 M â”‚
â”‚ 10 â”‚ model_obj.model.avgpool â”‚ AdaptiveAvgPool2d         â”‚      0 â”‚
â”‚ 11 â”‚ model_obj.model.fc      â”‚ Linear                    â”‚  1.5 K â”‚
â”‚ 12 â”‚ tr_kappa                â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 13 â”‚ val_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 14 â”‚ tst_kappa               â”‚ MulticlassCohenKappa      â”‚      0 â”‚
â”‚ 15 â”‚ tr_accuracy             â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 16 â”‚ val_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 17 â”‚ tst_accuracy            â”‚ MulticlassAccuracy        â”‚      0 â”‚
â”‚ 18 â”‚ val_conf_mat            â”‚ MulticlassConfusionMatrix â”‚      0 â”‚
â”‚ 19 â”‚ criterion               â”‚ CrossEntropyLoss          â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.2 M                                                        
Non-trainable params: 0                                                         
Total params: 11.2 M                                                            
Total estimated model params size (MB): 44                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Metric val_ce_loss improved. New best score: 1.108
Metric val_ce_loss improved by 0.007 >= min_delta = 0.0001. New best score: 1.102
Metric val_ce_loss improved by 0.031 >= min_delta = 0.0001. New best score: 1.071
Metric val_ce_loss improved by 0.028 >= min_delta = 0.0001. New best score: 1.042
