ViT_params:
  image_size: 64
  patch_size: 4
  num_classes: 3 
  dim: 96
  depth: 3
  heads: 4
  mlp_dim_ratio: 1.2
  pool: 'cls' 
  channels: 1
  dim_head: 24
  dropout: 0.15

MAE_params:
  decoder_dim : 96
  masking_ratio: 0.75
  decoder_depth: 1
  decoder_heads: 4
  decoder_dim_head: 24
  mlp_dim_ratio: 2
  dropout: 0.1


train_config:
  model_name: "MAE_ViT"
  dataset_objective : "pretraining"
  BATCH_SIZE: 64
  weight_decay : 0.0001
  lr : 3.0e-4
  ckpt_file_name : '{epoch} | {val_MAE_loss:.3f}'
  dir : "results/MAE"
  accumulate_grad_batches : 8
  accelerator : gpu
  MAX_EPOCHS : 100
  num_workers: 8

data_config:
  tr_path: "data/dataframes/foundation_models_pretraining_dataset/train_df.csv"
  val_path : "data/dataframes/foundation_models_pretraining_dataset/val_df.csv"
  tst_path : "data/dataframes/foundation_models_pretraining_dataset/test_df.csv"
  batch_size : ${train_config.BATCH_SIZE}
  num_workers : ${train_config.num_workers}

scheduler_params:
  scheduler_name: 'cosine_decay_lr_scheduler'

  cosine_decay_lr_scheduler_params:
  T_max: 250
  eta_min: 1.0e-07

  exponential_decay_lr_scheduler_params:
    gamma: 0.99

callbacks_config:
  ModelCheckpoint:
    monitor: 'val_MAE_loss'
    mode: 'min'
    save_top_k: 1
    save_last: True
    verbose : True
  EarlyStopping:
    monitor: 'val_MAE_Loss'
    mode: 'min'
    min_delta : 0.00005
    patience : 8
    verbose : True