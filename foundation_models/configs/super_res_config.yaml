ViT_params:
  image_size: 76
  patch_size: 4
  num_classes: 3
  dim: 256
  depth: 6
  heads: 6
  mlp_dim_ratio: 4
  pool: 'cls'
  channels: 1
  dim_head: 128
  dropout: 0.1

SuperRes_params:
  patch_height: ${ViT_params.patch_size}
  patch_width: ${ViT_params.patch_size}
  high_res_height: 152
  high_res_width: 152
  low_res_height : ${ViT_params.image_size}
  low_res_width : ${ViT_params.image_size}
  out_channels: 1
  shift: 0.02
  scale: 1.8

train_config:
  model_name: 'Task-4B_SuperRes'
  dataset_objective : "finetuned_super_res"
  BATCH_SIZE: 64
  weight_decay : 0.000001
  lr : 1.0e-4
  ckpt_file_name : '{epoch} | {val_MSE_loss:.3f}'   
  dir : "results/MAE"
  accumulate_grad_batches : 3
  accelerator : 'gpu'
  MAX_EPOCHS : 200
  num_workers: 8
  pretrained_mae_ckpt_path: 'results/MAE/MAE_ViT_pretraining/ckpts/epoch=45 | val_MSE_loss=0.064.ckpt'
  find_unused_parameters : True
  should_log_images : True
  log_images_every_n_epochs : 10

data_config:
  tr_path: "data/dataframes/foundation_models_superres_dataset/train_df.csv"
  val_path : "data/dataframes/foundation_models_superres_dataset/val_df.csv"
  tst_path : "data/dataframes/foundation_models_superres_dataset/test_df.csv"
  batch_size : ${train_config.BATCH_SIZE}
  num_workers : ${train_config.num_workers}
  objective : ${train_config.dataset_objective}

scheduler_params:
  scheduler_name: 'cosine_decay_lr_scheduler'

  cosine_decay_lr_scheduler_params:
    T_max: 150
    eta_min: 1.0e-07

  exponential_decay_lr_scheduler_params:
    gamma: 0.99

callbacks_config:
  ModelCheckpoint:
    monitor: 'val_MSE_loss'   
    mode: 'min'
    save_top_k: 1
    save_last: True
    verbose : True
  EarlyStopping:
    monitor: 'val_MSE_loss'  
    mode: 'min'
    min_delta : 0.00005
    patience : 8
    verbose : True
