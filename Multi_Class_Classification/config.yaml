train_config:
  model_name: "ImagenetModels"
  BATCH_SIZE: 64
  weight_decay : 0.0000005
  lr : 2.0e-4
  ckpt_file_name : '{epoch} | {val_ce_loss:.3f} | {val_kappa:.3f} | {val_acc:.3f}'
  dir : "results/Classification"
  accumulate_grad_batches : 4
  accelerator : gpu
  MAX_EPOCHS : 150
  num_workers: 8
  num_classes : 3

data_config:
  dataset_type: "imagenet_3channel"
  train_path : "data/dataframes/common_classification_dataset/train_df.csv"
  val_path : "data/dataframes/common_classification_dataset/val_df.csv"
  test_path : "data/dataframes/common_classification_dataset/test_df.csv"
  batch_size : ${train_config.BATCH_SIZE}
  num_workers : ${train_config.num_workers}
  
  
callbacks_config:
  ModelCheckpoint:
    monitor: 'val_ce_loss'
    mode: 'min'
    save_top_k: 1
    save_last: True
    verbose : True
  EarlyStopping:
    monitor: 'val_ce_loss'
    mode: 'min'
    min_delta : 0.00005
    patience : 10
    verbose : True

lr_scheduler_params :
  lr_scheduler_name: 'exponential_decay_lr_scheduler'
  exponential_decay_lr_scheduler_params:
    gamma: 0.98

  cosine_annealing_lr_scheduler_params:
    T_max: 150
    eta_min: 1.0e-07

ImagenetModels_params : 
  model_name : "resnet18"
  should_finetune : True
  num_classes : ${train_config.num_classes}
  lr : ${train_config.lr}
